{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import yaml\n",
    "import argparse\n",
    "from dotmap import DotMap\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "import wandb\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"./src\")  # make sure Python can find src/\n",
    "import data\n",
    "from model_linear import GPTLinear\n",
    "from model_softmax import GPTSoftmax\n",
    "from multi_task_train import train_step\n",
    "\n",
    "\n",
    "def set_seed(seed: int):\n",
    "    \"\"\"Set all random seeds for reproducibility.\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    ## Not sure if below would work if I dont have gpu\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    print(f\"Seed set to {seed}\")\n",
    "\n",
    "\n",
    "def load_config(config_path: str):\n",
    "    \"\"\"Load YAML config and convert to DotMap.\"\"\"\n",
    "    with open(config_path, \"r\") as f:\n",
    "        cfg = yaml.safe_load(f)\n",
    "    cfg = DotMap(cfg)\n",
    "    return cfg\n",
    "\n",
    "\n",
    "def prepare_data_samplers(config):\n",
    "    \"\"\"Create a dict of data samplers for each task.\"\"\"\n",
    "    num_task = len(config.data.tasks)\n",
    "    data_samplers = {}\n",
    "    for i in range(num_task):\n",
    "        task = config.data.tasks[i]\n",
    "        task_class = getattr(data, task.name)\n",
    "        data_samplers[task.name] = task_class(\n",
    "            min_num=config.data.min_num,\n",
    "            max_num=config.data.max_num,\n",
    "            k=config.data.k if hasattr(config.data, 'k') else None,\n",
    "            p=config.data.p if hasattr(config.data, 'p') else None,\n",
    "            sep=task.sep,\n",
    "        )\n",
    "    return data_samplers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.functional import cosine_similarity\n",
    "import matplotlib.pyplot as plt\n",
    "import wandb  # only if config.train.wandb = True\n",
    "\n",
    "# Optional, depending on your setup\n",
    "import numpy as np\n",
    "\n",
    "def train_step(\n",
    "    model,\n",
    "    optim,\n",
    "    data_samplers,\n",
    "    step,\n",
    "    config,\n",
    "    device,\n",
    "):\n",
    "    n_train, n_test, num_tokens = (\n",
    "        config.data.n_train,\n",
    "        config.data.n_test,\n",
    "        config.data.num_tokens,\n",
    "    )\n",
    "\n",
    "    # --- MIXED BATCH SAMPLING ---\n",
    "    # task_names = list(data_samplers.keys())\n",
    "    n_tasks = len(data_samplers)\n",
    "\n",
    "    n_train_each = n_train // n_tasks\n",
    "    n_test_each = n_test // n_tasks\n",
    "\n",
    "    mixed_train = {}\n",
    "    mixed_test = {}\n",
    "\n",
    "    for name, sampler in data_samplers.items():\n",
    "        data = sampler.sample(\n",
    "            num_samples=n_train_each + n_test_each,\n",
    "            num_tokens=num_tokens,\n",
    "        )\n",
    "        train_part = data[:n_train_each, :]\n",
    "        test_part = data[n_train_each:, :]\n",
    "        mixed_train[name] = train_part\n",
    "        mixed_test[name] = test_part\n",
    "\n",
    "    train_data = torch.cat(list(mixed_train.values()), dim=0)\n",
    "    test_data = torch.cat(list(mixed_test.values()), dim=0)\n",
    "\n",
    "    if config.data.mix == 'random':\n",
    "        # optionally shuffle to fully mix across tasks\n",
    "        perm = torch.randperm(train_data.size(0))\n",
    "        train_data = train_data[perm]\n",
    "        perm = torch.randperm(test_data.size(0))\n",
    "        test_data = test_data[perm]\n",
    "\n",
    "    prompt_len = num_tokens + 1\n",
    "    gen_len = num_tokens\n",
    "    acc_start = num_tokens + 1\n",
    "\n",
    "    model.train()\n",
    "    optim.zero_grad(set_to_none=True)\n",
    "\n",
    "    _, _, _, loss = model(\n",
    "        train_data[:, :-1], \n",
    "        targets=train_data[:, 1:], \n",
    "        prompt_len =prompt_len, \n",
    "        mask_input=config.train.mask_input,\n",
    "    )\n",
    "    loss.backward()\n",
    "\n",
    "    if config.train.grad_clip > 0:\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), config.train.grad_clip)\n",
    "\n",
    "    optim.step()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        ### Per task metrics\n",
    "        if n_tasks > 1:\n",
    "            examples_seen_per_task = n_train_each * (step + 1)\n",
    "            per_task_metrics = {}\n",
    "\n",
    "            for task_name in mixed_train.keys():\n",
    "                t_train = mixed_train[task_name]\n",
    "                t_test = mixed_test[task_name]\n",
    "\n",
    "                # Compute loss on this task\n",
    "                _, _, _, task_train_loss = model(\n",
    "                    t_train[:, :-1],\n",
    "                    targets=t_train[:, 1:],\n",
    "                    prompt_len=prompt_len,\n",
    "                    mask_input=config.train.mask_input,\n",
    "                )\n",
    "\n",
    "                # Predictions\n",
    "                t_train_pred = model.generate(\n",
    "                    idx=t_train[:, :prompt_len],\n",
    "                    max_new_tokens=gen_len,\n",
    "                    prompt_len=prompt_len,\n",
    "                )\n",
    "                t_test_pred = model.generate(\n",
    "                    idx=t_test[:, :prompt_len],\n",
    "                    max_new_tokens=gen_len,\n",
    "                    prompt_len=prompt_len,\n",
    "                )\n",
    "\n",
    "                # Accuracy for this task\n",
    "                t_train_acc = torch.mean(\n",
    "                    (t_train_pred[:, acc_start:] == t_train[:, acc_start:]).float()\n",
    "                ).item()\n",
    "                t_test_acc = torch.mean(\n",
    "                    (t_test_pred[:, acc_start:] == t_test[:, acc_start:]).float()\n",
    "                ).item()\n",
    "\n",
    "                # Store\n",
    "                per_task_metrics[task_name] = {\n",
    "                    \"train_loss\": task_train_loss.item(),\n",
    "                    \"train_acc\": t_train_acc,\n",
    "                    \"test_acc\": t_test_acc,\n",
    "                    # \"num_train_samples\": t_train.shape[0],\n",
    "                    # \"num_test_samples\": t_test.shape[0],\n",
    "                }\n",
    "            \n",
    "            if config.train.wandb:\n",
    "                for task_name, vals in per_task_metrics.items():\n",
    "                    wandb.log({\n",
    "                        f\"{task_name}/train_loss\": vals[\"train_loss\"],\n",
    "                        f\"{task_name}/train_acc\": vals[\"train_acc\"],\n",
    "                        f\"{task_name}/test_acc\": vals[\"test_acc\"],\n",
    "                        # f\"{task_name}/num_train_samples\": vals[\"num_train_samples\"],\n",
    "                        f\"{task_name}/examples_seen\": examples_seen_per_task,\n",
    "                    }, step=step)\n",
    "\n",
    "        # Log train loss, train / test acc, repetition frequency\n",
    "        attn_map, pre_lm_h, _, train_loss = model(\n",
    "            train_data[:, :-1], \n",
    "            targets=train_data[:, 1:], \n",
    "            prompt_len =prompt_len, \n",
    "            mask_input=config.train.mask_input,\n",
    "            )\n",
    "\n",
    "        train_pred = model.generate(\n",
    "            idx=train_data[:, :prompt_len],\n",
    "            max_new_tokens=gen_len,\n",
    "            prompt_len =prompt_len,\n",
    "        )\n",
    "        test_pred = model.generate(\n",
    "            idx=test_data[:, :prompt_len],\n",
    "            max_new_tokens=gen_len,\n",
    "            prompt_len =prompt_len,\n",
    "        )\n",
    "\n",
    "        train_acc = torch.mean(\n",
    "            (train_pred[:, acc_start:] == train_data[:, acc_start:]).to(float)\n",
    "        ).item()\n",
    "        test_acc = torch.mean(\n",
    "            (test_pred[:, acc_start:] == test_data[:, acc_start:]).to(float)\n",
    "        ).item()\n",
    "\n",
    "        data_repeat_frac = torch.mean((test_data[:, acc_start:-1] == test_data[:, acc_start+1:]).to(float))\n",
    "        model_repeat_frac = torch.mean((test_pred[:, acc_start:-1] == test_pred[:, acc_start+1:]).to(float))\n",
    "\n",
    "        # Log attention progress measure\n",
    "        attn_map_output_seq = attn_map[:, :, acc_start-1:]\n",
    "        att_mask = torch.zeros_like(attn_map_output_seq).to(device)\n",
    "\n",
    "        att_mask[:, :, 0, 0] = 1\n",
    "        for i in range(num_tokens - 1):\n",
    "            att_mask[:, :, i + 1, i : i + 2] = 1\n",
    "\n",
    "        att_prog_measure = torch.mean(\n",
    "            torch.sum(torch.abs(attn_map_output_seq) * att_mask, dim=(-3, -2, -1)) /\n",
    "            torch.sum(torch.abs(attn_map_output_seq), dim=(-3, -2, -1)),\n",
    "            dim=0\n",
    "        )\n",
    "\n",
    "        # Log pair-wise cosine similarity between hidden states\n",
    "        embed_start = acc_start - 1\n",
    "        embed_len = gen_len\n",
    "\n",
    "        logit_cs = torch.zeros((embed_len, embed_len))\n",
    "\n",
    "        for i_1 in range(embed_start, embed_start + embed_len):\n",
    "            for i_2 in range(embed_start, i_1):\n",
    "                logit_cs[i_1 - embed_start, i_2 - embed_start] = torch.mean(\n",
    "                    (\n",
    "                        cosine_similarity(\n",
    "                            pre_lm_h[:, i_1, :], pre_lm_h[:, i_2, :], dim=-1\n",
    "                        )\n",
    "                    ), dim=0\n",
    "                )\n",
    "\n",
    "        # --- Cosine similarity figure ---\n",
    "        logit_fig, ax_cs = plt.subplots(figsize=(10,10))\n",
    "        im_cs = ax_cs.imshow(logit_cs)\n",
    "        ax_cs.set_title(\"avg pre_lm_h cosine sim\")\n",
    "        logit_fig.colorbar(im_cs, ax=ax_cs, shrink=0.9)\n",
    "        ax_cs.set_xticks(range(embed_len))\n",
    "        ax_cs.set_yticks(range(embed_len))\n",
    "        ax_cs.set_xlabel(\"Token index\")\n",
    "        ax_cs.set_ylabel(\"Token index\")\n",
    "        plt.tight_layout()\n",
    "        # plt.show()\n",
    "\n",
    "        # Optional: overlay numbers\n",
    "        for i1 in range(embed_len):\n",
    "            for i2 in range(embed_len):\n",
    "                ax_cs.text(i2, i1, f\"{logit_cs[i1, i2].item():.2f}\",\n",
    "                        ha=\"center\", va=\"center\", color=\"w\" if logit_cs[i1, i2] < logit_cs.max()/2 else \"k\")\n",
    "    \n",
    "\n",
    "        if config.train.wandb:\n",
    "            wandb.log({\"pre_lm_h_cosine_sim\": logit_fig}, step=step)\n",
    "        plt.close(logit_fig)\n",
    "\n",
    "\n",
    "        # --- Attention maps per head (averaged over batch) ---\n",
    "        avg_attn_per_head = attn_map.mean(dim=0).detach().cpu().numpy()  # shape: (n_head, T, T)\n",
    "\n",
    "        for h in range(config.model.n_head):\n",
    "            fig_head, ax_head = plt.subplots(figsize=(10,10))\n",
    "            im_head = ax_head.imshow(avg_attn_per_head[h])\n",
    "            ax_head.set_title(f\"Head {h} avg attention\")\n",
    "            fig_head.colorbar(im_head, ax=ax_head, shrink=0.9)\n",
    "            ax_head.set_xticks(range(avg_attn_per_head[h].shape[-1]))\n",
    "            ax_head.set_yticks(range(avg_attn_per_head[h].shape[-2]))\n",
    "            \n",
    "            if config.train.wandb:\n",
    "                wandb.log({f\"Head_{h}_avg_attention\": fig_head}, step=step)\n",
    "            plt.close(fig_head)\n",
    "\n",
    "\n",
    "        print(\n",
    "            f\"Step {step} -- Train loss: {train_loss}, Train Acc: {train_acc} Test Acc: {test_acc}\"\n",
    "        )\n",
    "        # print(f\"input: {test_data[0]} \\n predicted:{test_pred[0]}\")\n",
    "        if config.train.wandb:\n",
    "            \n",
    "            log_data = {\n",
    "                'examples_seen_per_task': examples_seen_per_task,\n",
    "                \"train_loss\": train_loss,\n",
    "                \"train_acc\": train_acc,\n",
    "                \"test_acc\": test_acc,\n",
    "                \"data_repeat_frac\": data_repeat_frac,\n",
    "                \"model_repeat_frac\": model_repeat_frac,\n",
    "                \"att_prog_measure\": att_prog_measure,\n",
    "                # \"pre_lm_h_cosine_sim\": logit_fig,\n",
    "                \"mean_cosine_sim\": torch.sum(logit_cs[:, 1:]) / (0.5 * (gen_len-1) * (gen_len-2))\n",
    "            }\n",
    "\n",
    "            for output_pos in range(gen_len):\n",
    "                log_data.update(\n",
    "                    {\n",
    "                        f\"idx{output_pos}_check\": torch.mean(\n",
    "                            (train_pred[:, acc_start + output_pos] == train_data[:, acc_start + output_pos]).to(float)\n",
    "                        ).item()\n",
    "                    }\n",
    "                )\n",
    "\n",
    "                if output_pos < gen_len-1:\n",
    "                    log_data.update(\n",
    "                        {\n",
    "                            f\"mean_cosine_sim_{output_pos}\": torch.sum(logit_cs[:, output_pos]) / (gen_len-1-output_pos)\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "            wandb.log(log_data, step=step)\n",
    "\n",
    "        plt.close()\n",
    "        del (\n",
    "            logit_fig,\n",
    "            ax_cs,\n",
    "            logit_cs,\n",
    "            ax_head,\n",
    "            fig_head\n",
    "\n",
    "        )\n",
    "\n",
    "        if config.train.save_ckpt:\n",
    "            if (step == 0) or ((step + 1) % config.train.ckpt_freq == 0):\n",
    "                model.train()\n",
    "                torch.save(\n",
    "                    {\n",
    "                        \"epoch\": step,\n",
    "                        \"model\": model.state_dict(),\n",
    "                        \"optim\": optim.state_dict(),\n",
    "                        \"train_loss\": train_loss,\n",
    "                        \"test_acc\": test_acc,\n",
    "                    },\n",
    "                    \"./mws_k2_l1_h1_a16_n16.tar\",\n",
    "                )\n",
    "                print(f\"saved state at epoch {step} to {f'./mws_k2_l1_h1_a16_n16.tar'}\")\n",
    "\n",
    "                if config.train.wandb:\n",
    "                    model_wandb = wandb.Artifact(\n",
    "                        f\"model_step{step}\", type=\"model\"\n",
    "                    )\n",
    "                    model_wandb.add_file(f\"./mws_k2_l1_h1_a16_n16.tar\")\n",
    "                    wandb.log_artifact(model_wandb)\n",
    "                    print(\"model uploaded to wandb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed set to 67\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:jfe1tbm9) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>MovingWindowProduct/examples_seen</td><td>▁▁▂▂▂▃▃▃▄▄▅▅▅▆▆▆▇▇▇██</td></tr><tr><td>MovingWindowProduct/test_acc</td><td>▅▅▂▄▁▃█▂▃▅▄▄▅▅▅▅▆▆▇▃▇</td></tr><tr><td>MovingWindowProduct/train_acc</td><td>▄█▃▄▅▆▁▅▃▃▄▁▅▇▅█▇▇▇▆▅</td></tr><tr><td>MovingWindowProduct/train_loss</td><td>██▆▅▄▃▄▃▃▂▂▂▂▁▂▂▁▁▂▁▁</td></tr><tr><td>MovingWindowSum/examples_seen</td><td>▁▁▂▂▂▃▃▃▄▄▅▅▅▆▆▆▇▇▇██</td></tr><tr><td>MovingWindowSum/test_acc</td><td>▂▂▅▁▆▃▃▆▅▄▂▃▄▄▆▂▇▄▅▂█</td></tr><tr><td>MovingWindowSum/train_acc</td><td>▂▄▁▃▁▃▃▂▃▁▄▃▁▄▅▄█▅▇▆█</td></tr><tr><td>MovingWindowSum/train_loss</td><td>█▇▆▇▇▅▄▆▆▆▄▆▄▄▃▂▄▁▂▁▁</td></tr><tr><td>att_prog_measure</td><td>▁▂▂▁▂▃▃▃▄▅▄▅▅▅▇▇▇███</td></tr><tr><td>data_repeat_frac</td><td>▁▅▁▄▇▂▅▄▅█▄▄▃▄▆▇▆█▆▇</td></tr><tr><td>examples_seen_per_task</td><td>▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██</td></tr><tr><td>idx0_check</td><td>▂▃▁▂▃▄▃▃▃▃▅▅▅▇▆▇██▇█</td></tr><tr><td>idx10_check</td><td>▄▁▄▇▁▆▁█▇▄▃█▅▄▅▅▆▆▄█</td></tr><tr><td>idx11_check</td><td>▄███▁▇▅▄▄▃▄▄▄▅▇▄▄▅▇▄</td></tr><tr><td>idx12_check</td><td>▃▃▅▁▅▇▄▅▅▃▁▁▃▅▅█▅▃▇▅</td></tr><tr><td>idx13_check</td><td>▃█▅▃▆▅▄▄▆▆▂▃▃▃▅▆▇▂▇▁</td></tr><tr><td>idx14_check</td><td>▆▇▃▄▄▄▅▅▄▁▅▂▆▅▅▃▇█▅▅</td></tr><tr><td>idx15_check</td><td>▇▇▃▄▅▄▅▇▅▃▁▄▂▅▄▅▅▅▄█</td></tr><tr><td>idx1_check</td><td>▃▅▅▄▃▄▅▃▁▄▁▁▅▄▅▄▄▂▃█</td></tr><tr><td>idx2_check</td><td>█▄▃▄▂▅▄▂▅▂▅▄▁▅▅▄▃▄▇▁</td></tr><tr><td>idx3_check</td><td>▂▅▃▇▇▆▂▃▅▆▅▂▄▃▄▅█▅▃▁</td></tr><tr><td>idx4_check</td><td>▄▄▃▆▄▇▅▆▁▅█▁▁▄█▄▇▄▄▄</td></tr><tr><td>idx5_check</td><td>▇▆▇▅█▁▅▃▄▁▅▃▄▅▃▅▇▇▅▃</td></tr><tr><td>idx6_check</td><td>▃▅▁▃▅▅▆▂▅▅▅▃▃▁▃▁▇▁█▅</td></tr><tr><td>idx7_check</td><td>▃▆▂▅▅▅▂▁▂▂█▆▄▄▂▁▃▃▂▁</td></tr><tr><td>idx8_check</td><td>▆█▅▂▃▁▁▄▅▁▃▂▃▅▃▆▃▆▂▂</td></tr><tr><td>idx9_check</td><td>▂▃▃▄▅▂▁▅▃█▄▂▄▆▄▅▁▃▆▅</td></tr><tr><td>mean_cosine_sim</td><td>▁▁▁▂▂▂▃▃▃▄▄▅▅▆▆▆▇▇██</td></tr><tr><td>mean_cosine_sim_0</td><td>▁▁▁▂▂▃▃▃▄▅▅▆▆▇▇▇████</td></tr><tr><td>mean_cosine_sim_1</td><td>▁▁▁▂▂▂▃▃▄▄▅▅▅▆▆▇▇▇██</td></tr><tr><td>mean_cosine_sim_10</td><td>▁▁▁▂▂▂▃▃▄▄▅▅▅▆▆▇▇▇██</td></tr><tr><td>mean_cosine_sim_11</td><td>▁▁▂▂▃▃▃▄▄▅▅▅▆▆▆▇▇▇██</td></tr><tr><td>mean_cosine_sim_12</td><td>▁▁▁▂▃▃▃▄▄▅▅▆▆▆▆▇▇▇██</td></tr><tr><td>mean_cosine_sim_13</td><td>▁▁▁▁▂▂▃▃▄▄▄▅▅▆▆▆▇▇██</td></tr><tr><td>mean_cosine_sim_14</td><td>▁▁▁▂▂▃▃▄▄▅▅▆▆▆▇▇▇███</td></tr><tr><td>mean_cosine_sim_2</td><td>▁▁▁▁▂▂▂▃▃▃▄▄▅▅▆▆▇▇██</td></tr><tr><td>mean_cosine_sim_3</td><td>▁▁▁▂▂▂▃▃▄▄▅▅▅▆▆▇▇▇██</td></tr><tr><td>mean_cosine_sim_4</td><td>▁▁▁▂▂▂▃▃▃▄▄▅▅▆▆▆▇▇██</td></tr><tr><td>mean_cosine_sim_5</td><td>▁▁▁▂▂▂▃▃▄▄▄▅▅▆▆▆▇▇██</td></tr><tr><td>mean_cosine_sim_6</td><td>▁▁▁▁▂▂▂▃▃▃▄▄▅▅▆▆▇▇██</td></tr><tr><td>mean_cosine_sim_7</td><td>▁▁▁▂▂▂▃▃▄▄▄▅▅▆▆▇▇▇██</td></tr><tr><td>mean_cosine_sim_8</td><td>▁▁▁▂▂▂▃▃▄▄▅▅▅▆▆▇▇▇██</td></tr><tr><td>mean_cosine_sim_9</td><td>▁▁▁▁▂▂▂▃▃▄▄▄▅▅▆▆▇▇██</td></tr><tr><td>model_repeat_frac</td><td>▂▂▁▂▂▂▃▃▁▁▂▄▅█▅▇▇▇▆▇</td></tr><tr><td>test_acc</td><td>▂▂▃▁▃▂▅▄▅▄▂▃▅▄▆▃█▅▆▁</td></tr><tr><td>train_acc</td><td>▃▆▁▃▃▄▂▃▃▁▄▁▂▆▅▆█▆▇▆</td></tr><tr><td>train_loss</td><td>██▆▆▅▄▄▄▄▃▂▃▂▂▂▁▂▁▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>MovingWindowProduct/examples_seen</td><td>2688</td></tr><tr><td>MovingWindowProduct/test_acc</td><td>0.08398</td></tr><tr><td>MovingWindowProduct/train_acc</td><td>0.06885</td></tr><tr><td>MovingWindowProduct/train_loss</td><td>2.79752</td></tr><tr><td>MovingWindowSum/examples_seen</td><td>2688</td></tr><tr><td>MovingWindowSum/test_acc</td><td>0.0957</td></tr><tr><td>MovingWindowSum/train_acc</td><td>0.08643</td></tr><tr><td>MovingWindowSum/train_loss</td><td>2.83567</td></tr><tr><td>att_prog_measure</td><td>0.08438</td></tr><tr><td>data_repeat_frac</td><td>0.06979</td></tr><tr><td>examples_seen_per_task</td><td>2560</td></tr><tr><td>idx0_check</td><td>0.26562</td></tr><tr><td>idx10_check</td><td>0.08203</td></tr><tr><td>idx11_check</td><td>0.05859</td></tr><tr><td>idx12_check</td><td>0.06641</td></tr><tr><td>idx13_check</td><td>0.03125</td></tr><tr><td>idx14_check</td><td>0.0625</td></tr><tr><td>idx15_check</td><td>0.09766</td></tr><tr><td>idx1_check</td><td>0.10156</td></tr><tr><td>idx2_check</td><td>0.04297</td></tr><tr><td>idx3_check</td><td>0.03125</td></tr><tr><td>idx4_check</td><td>0.05859</td></tr><tr><td>idx5_check</td><td>0.0625</td></tr><tr><td>idx6_check</td><td>0.07031</td></tr><tr><td>idx7_check</td><td>0.04688</td></tr><tr><td>idx8_check</td><td>0.05078</td></tr><tr><td>idx9_check</td><td>0.07422</td></tr><tr><td>mean_cosine_sim</td><td>0.59816</td></tr><tr><td>mean_cosine_sim_0</td><td>0.40081</td></tr><tr><td>mean_cosine_sim_1</td><td>0.61245</td></tr><tr><td>mean_cosine_sim_10</td><td>0.61608</td></tr><tr><td>mean_cosine_sim_11</td><td>0.67859</td></tr><tr><td>mean_cosine_sim_12</td><td>0.68464</td></tr><tr><td>mean_cosine_sim_13</td><td>0.64535</td></tr><tr><td>mean_cosine_sim_14</td><td>0.72805</td></tr><tr><td>mean_cosine_sim_2</td><td>0.52017</td></tr><tr><td>mean_cosine_sim_3</td><td>0.64443</td></tr><tr><td>mean_cosine_sim_4</td><td>0.58822</td></tr><tr><td>mean_cosine_sim_5</td><td>0.58875</td></tr><tr><td>mean_cosine_sim_6</td><td>0.56479</td></tr><tr><td>mean_cosine_sim_7</td><td>0.60764</td></tr><tr><td>mean_cosine_sim_8</td><td>0.59315</td></tr><tr><td>mean_cosine_sim_9</td><td>0.56925</td></tr><tr><td>model_repeat_frac</td><td>0.14062</td></tr><tr><td>test_acc</td><td>0.05469</td></tr><tr><td>train_acc</td><td>0.0752</td></tr><tr><td>train_loss</td><td>2.81639</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">mix_mws_mwp_linear_2head</strong> at: <a href='https://wandb.ai/wth_ucsd/loss_plateau_tf/runs/jfe1tbm9' target=\"_blank\">https://wandb.ai/wth_ucsd/loss_plateau_tf/runs/jfe1tbm9</a><br/>Synced 6 W&B file(s), 60 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251114_033151-jfe1tbm9/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:jfe1tbm9). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jyue/private/tf-loss-plateau/wandb/run-20251114_033251-2tv9xvfz</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/wth_ucsd/loss_plateau_tf/runs/2tv9xvfz' target=\"_blank\">mix_mws_mwp_linear_2head</a></strong> to <a href='https://wandb.ai/wth_ucsd/loss_plateau_tf' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/wth_ucsd/loss_plateau_tf' target=\"_blank\">https://wandb.ai/wth_ucsd/loss_plateau_tf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/wth_ucsd/loss_plateau_tf/runs/2tv9xvfz' target=\"_blank\">https://wandb.ai/wth_ucsd/loss_plateau_tf/runs/2tv9xvfz</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0 -- Train loss: 2.880411148071289, Train Acc: 0.063720703125 Test Acc: 0.060546875\n",
      "Step 1 -- Train loss: 2.876390218734741, Train Acc: 0.07373046875 Test Acc: 0.060546875\n",
      "Step 2 -- Train loss: 2.864926338195801, Train Acc: 0.058837890625 Test Acc: 0.0654296875\n",
      "Step 3 -- Train loss: 2.859476327896118, Train Acc: 0.06494140625 Test Acc: 0.0556640625\n",
      "Step 4 -- Train loss: 2.8546979427337646, Train Acc: 0.064453125 Test Acc: 0.064453125\n",
      "Step 5 -- Train loss: 2.8414862155914307, Train Acc: 0.0693359375 Test Acc: 0.0595703125\n",
      "Step 6 -- Train loss: 2.842879056930542, Train Acc: 0.061279296875 Test Acc: 0.07421875\n",
      "Step 7 -- Train loss: 2.841280460357666, Train Acc: 0.065185546875 Test Acc: 0.0673828125\n",
      "Step 8 -- Train loss: 2.8434133529663086, Train Acc: 0.064208984375 Test Acc: 0.0703125\n",
      "Step 9 -- Train loss: 2.8364195823669434, Train Acc: 0.060302734375 Test Acc: 0.068359375\n",
      "Step 10 -- Train loss: 2.8284096717834473, Train Acc: 0.067138671875 Test Acc: 0.05859375\n",
      "Step 11 -- Train loss: 2.837667942047119, Train Acc: 0.059814453125 Test Acc: 0.0634765625\n",
      "Step 12 -- Train loss: 2.828716278076172, Train Acc: 0.0634765625 Test Acc: 0.0712890625\n",
      "Step 13 -- Train loss: 2.8251547813415527, Train Acc: 0.073974609375 Test Acc: 0.0693359375\n",
      "Step 14 -- Train loss: 2.8289952278137207, Train Acc: 0.07177734375 Test Acc: 0.076171875\n",
      "Step 15 -- Train loss: 2.820772647857666, Train Acc: 0.0732421875 Test Acc: 0.0634765625\n",
      "Step 16 -- Train loss: 2.827331066131592, Train Acc: 0.0810546875 Test Acc: 0.0859375\n",
      "Step 17 -- Train loss: 2.8187832832336426, Train Acc: 0.076171875 Test Acc: 0.072265625\n",
      "Step 18 -- Train loss: 2.8210883140563965, Train Acc: 0.078125 Test Acc: 0.0771484375\n",
      "Step 19 -- Train loss: 2.8163888454437256, Train Acc: 0.0751953125 Test Acc: 0.0546875\n",
      "Step 20 -- Train loss: 2.816596508026123, Train Acc: 0.07763671875 Test Acc: 0.08984375\n",
      "Step 21 -- Train loss: 2.8164663314819336, Train Acc: 0.076904296875 Test Acc: 0.08203125\n",
      "Step 22 -- Train loss: 2.8104071617126465, Train Acc: 0.07958984375 Test Acc: 0.083984375\n",
      "Step 23 -- Train loss: 2.807431221008301, Train Acc: 0.08154296875 Test Acc: 0.0830078125\n",
      "Step 24 -- Train loss: 2.8057594299316406, Train Acc: 0.087646484375 Test Acc: 0.0751953125\n",
      "Step 25 -- Train loss: 2.8012754917144775, Train Acc: 0.093994140625 Test Acc: 0.09765625\n",
      "Step 26 -- Train loss: 2.800100088119507, Train Acc: 0.092529296875 Test Acc: 0.09375\n",
      "Step 27 -- Train loss: 2.7972073554992676, Train Acc: 0.10107421875 Test Acc: 0.099609375\n",
      "Step 28 -- Train loss: 2.7906322479248047, Train Acc: 0.107177734375 Test Acc: 0.11328125\n",
      "Step 29 -- Train loss: 2.7896604537963867, Train Acc: 0.105224609375 Test Acc: 0.1005859375\n",
      "Step 30 -- Train loss: 2.776975154876709, Train Acc: 0.11669921875 Test Acc: 0.115234375\n",
      "Step 31 -- Train loss: 2.7665200233459473, Train Acc: 0.11865234375 Test Acc: 0.1025390625\n",
      "Step 32 -- Train loss: 2.7627735137939453, Train Acc: 0.118408203125 Test Acc: 0.111328125\n",
      "Step 33 -- Train loss: 2.753610849380493, Train Acc: 0.114990234375 Test Acc: 0.1025390625\n",
      "Step 34 -- Train loss: 2.745265483856201, Train Acc: 0.115966796875 Test Acc: 0.123046875\n",
      "Step 35 -- Train loss: 2.735095739364624, Train Acc: 0.123779296875 Test Acc: 0.119140625\n",
      "Step 36 -- Train loss: 2.7250688076019287, Train Acc: 0.11865234375 Test Acc: 0.125\n",
      "Step 37 -- Train loss: 2.7215564250946045, Train Acc: 0.115234375 Test Acc: 0.10546875\n",
      "Step 38 -- Train loss: 2.710275173187256, Train Acc: 0.121337890625 Test Acc: 0.1064453125\n",
      "Step 39 -- Train loss: 2.7062888145446777, Train Acc: 0.1171875 Test Acc: 0.1103515625\n",
      "Step 40 -- Train loss: 2.7021594047546387, Train Acc: 0.1181640625 Test Acc: 0.1103515625\n",
      "Step 41 -- Train loss: 2.6955771446228027, Train Acc: 0.116455078125 Test Acc: 0.115234375\n",
      "Step 42 -- Train loss: 2.6870551109313965, Train Acc: 0.125244140625 Test Acc: 0.1064453125\n",
      "Step 43 -- Train loss: 2.68799090385437, Train Acc: 0.116943359375 Test Acc: 0.11328125\n",
      "Step 44 -- Train loss: 2.68536639213562, Train Acc: 0.119140625 Test Acc: 0.1123046875\n",
      "Step 45 -- Train loss: 2.6807096004486084, Train Acc: 0.1181640625 Test Acc: 0.1142578125\n",
      "Step 46 -- Train loss: 2.6762938499450684, Train Acc: 0.117919921875 Test Acc: 0.123046875\n",
      "Step 47 -- Train loss: 2.676241397857666, Train Acc: 0.116455078125 Test Acc: 0.11328125\n",
      "Step 48 -- Train loss: 2.673426866531372, Train Acc: 0.120361328125 Test Acc: 0.1103515625\n",
      "Step 49 -- Train loss: 2.6683096885681152, Train Acc: 0.118896484375 Test Acc: 0.1201171875\n",
      "Step 50 -- Train loss: 2.6661908626556396, Train Acc: 0.113525390625 Test Acc: 0.1142578125\n",
      "Step 51 -- Train loss: 2.6667566299438477, Train Acc: 0.118408203125 Test Acc: 0.1162109375\n",
      "Step 52 -- Train loss: 2.662745475769043, Train Acc: 0.116943359375 Test Acc: 0.123046875\n",
      "Step 53 -- Train loss: 2.660407543182373, Train Acc: 0.117919921875 Test Acc: 0.1064453125\n",
      "Step 54 -- Train loss: 2.662379741668701, Train Acc: 0.119140625 Test Acc: 0.123046875\n",
      "Step 55 -- Train loss: 2.65933895111084, Train Acc: 0.11865234375 Test Acc: 0.130859375\n",
      "Step 56 -- Train loss: 2.657330274581909, Train Acc: 0.1162109375 Test Acc: 0.12890625\n",
      "Step 57 -- Train loss: 2.6611578464508057, Train Acc: 0.12060546875 Test Acc: 0.1142578125\n",
      "Step 58 -- Train loss: 2.658289670944214, Train Acc: 0.12109375 Test Acc: 0.1083984375\n",
      "Step 59 -- Train loss: 2.657877206802368, Train Acc: 0.117431640625 Test Acc: 0.1328125\n",
      "Step 60 -- Train loss: 2.6558144092559814, Train Acc: 0.12158203125 Test Acc: 0.12890625\n",
      "Step 61 -- Train loss: 2.656353712081909, Train Acc: 0.11279296875 Test Acc: 0.123046875\n",
      "Step 62 -- Train loss: 2.6557791233062744, Train Acc: 0.119873046875 Test Acc: 0.138671875\n",
      "Step 63 -- Train loss: 2.651787519454956, Train Acc: 0.123779296875 Test Acc: 0.1259765625\n",
      "Step 64 -- Train loss: 2.6507885456085205, Train Acc: 0.117431640625 Test Acc: 0.1220703125\n",
      "Step 65 -- Train loss: 2.650754928588867, Train Acc: 0.120361328125 Test Acc: 0.10546875\n",
      "Step 66 -- Train loss: 2.6498935222625732, Train Acc: 0.1201171875 Test Acc: 0.1044921875\n",
      "Step 67 -- Train loss: 2.6503615379333496, Train Acc: 0.12255859375 Test Acc: 0.1123046875\n",
      "Step 68 -- Train loss: 2.6510019302368164, Train Acc: 0.119140625 Test Acc: 0.12109375\n",
      "Step 69 -- Train loss: 2.6481661796569824, Train Acc: 0.125244140625 Test Acc: 0.1240234375\n",
      "Step 70 -- Train loss: 2.647212505340576, Train Acc: 0.119873046875 Test Acc: 0.1181640625\n",
      "Step 71 -- Train loss: 2.6457810401916504, Train Acc: 0.129150390625 Test Acc: 0.1240234375\n",
      "Step 72 -- Train loss: 2.644303321838379, Train Acc: 0.12060546875 Test Acc: 0.1201171875\n",
      "Step 73 -- Train loss: 2.648895740509033, Train Acc: 0.126708984375 Test Acc: 0.1328125\n",
      "Step 74 -- Train loss: 2.6488630771636963, Train Acc: 0.120361328125 Test Acc: 0.1259765625\n",
      "Step 75 -- Train loss: 2.647524833679199, Train Acc: 0.123046875 Test Acc: 0.10546875\n",
      "Step 76 -- Train loss: 2.6456995010375977, Train Acc: 0.125732421875 Test Acc: 0.11328125\n",
      "Step 77 -- Train loss: 2.6464390754699707, Train Acc: 0.122802734375 Test Acc: 0.1123046875\n",
      "Step 78 -- Train loss: 2.6446216106414795, Train Acc: 0.1201171875 Test Acc: 0.1357421875\n",
      "Step 79 -- Train loss: 2.64119291305542, Train Acc: 0.126220703125 Test Acc: 0.1064453125\n",
      "Step 80 -- Train loss: 2.643510341644287, Train Acc: 0.119384765625 Test Acc: 0.1201171875\n",
      "Step 81 -- Train loss: 2.63858699798584, Train Acc: 0.125244140625 Test Acc: 0.1259765625\n",
      "Step 82 -- Train loss: 2.642077684402466, Train Acc: 0.1298828125 Test Acc: 0.111328125\n",
      "Step 83 -- Train loss: 2.640404462814331, Train Acc: 0.12744140625 Test Acc: 0.1259765625\n",
      "Step 84 -- Train loss: 2.641248941421509, Train Acc: 0.119384765625 Test Acc: 0.1240234375\n",
      "Step 85 -- Train loss: 2.6383118629455566, Train Acc: 0.117431640625 Test Acc: 0.1220703125\n",
      "Step 86 -- Train loss: 2.6364336013793945, Train Acc: 0.127685546875 Test Acc: 0.1220703125\n",
      "Step 87 -- Train loss: 2.6345646381378174, Train Acc: 0.1259765625 Test Acc: 0.130859375\n",
      "Step 88 -- Train loss: 2.637556314468384, Train Acc: 0.126708984375 Test Acc: 0.12890625\n",
      "Step 89 -- Train loss: 2.639355421066284, Train Acc: 0.1279296875 Test Acc: 0.12109375\n",
      "Step 90 -- Train loss: 2.634202241897583, Train Acc: 0.124267578125 Test Acc: 0.119140625\n",
      "Step 91 -- Train loss: 2.6379103660583496, Train Acc: 0.12353515625 Test Acc: 0.1376953125\n",
      "Step 92 -- Train loss: 2.6327011585235596, Train Acc: 0.12158203125 Test Acc: 0.1220703125\n",
      "Step 93 -- Train loss: 2.6344499588012695, Train Acc: 0.127685546875 Test Acc: 0.109375\n",
      "Step 94 -- Train loss: 2.6345810890197754, Train Acc: 0.125732421875 Test Acc: 0.125\n",
      "Step 95 -- Train loss: 2.6334757804870605, Train Acc: 0.12353515625 Test Acc: 0.130859375\n",
      "Step 96 -- Train loss: 2.6357626914978027, Train Acc: 0.123779296875 Test Acc: 0.1171875\n",
      "Step 97 -- Train loss: 2.631943702697754, Train Acc: 0.1201171875 Test Acc: 0.12890625\n",
      "Step 98 -- Train loss: 2.631936550140381, Train Acc: 0.129150390625 Test Acc: 0.126953125\n",
      "Step 99 -- Train loss: 2.6333060264587402, Train Acc: 0.118408203125 Test Acc: 0.1279296875\n",
      "Step 100 -- Train loss: 2.631693124771118, Train Acc: 0.128173828125 Test Acc: 0.1220703125\n",
      "Step 101 -- Train loss: 2.635481834411621, Train Acc: 0.12158203125 Test Acc: 0.1279296875\n",
      "Step 102 -- Train loss: 2.6327431201934814, Train Acc: 0.130859375 Test Acc: 0.12109375\n",
      "Step 103 -- Train loss: 2.6338918209075928, Train Acc: 0.124267578125 Test Acc: 0.115234375\n",
      "Step 104 -- Train loss: 2.6307239532470703, Train Acc: 0.12646484375 Test Acc: 0.1337890625\n",
      "Step 105 -- Train loss: 2.6281678676605225, Train Acc: 0.1298828125 Test Acc: 0.1357421875\n",
      "Step 106 -- Train loss: 2.630624532699585, Train Acc: 0.134033203125 Test Acc: 0.1181640625\n",
      "Step 107 -- Train loss: 2.633441209793091, Train Acc: 0.131591796875 Test Acc: 0.1142578125\n",
      "Step 108 -- Train loss: 2.630061388015747, Train Acc: 0.123779296875 Test Acc: 0.125\n",
      "Step 109 -- Train loss: 2.6281814575195312, Train Acc: 0.127197265625 Test Acc: 0.125\n",
      "Step 110 -- Train loss: 2.625767469406128, Train Acc: 0.117919921875 Test Acc: 0.12109375\n",
      "Step 111 -- Train loss: 2.6297342777252197, Train Acc: 0.125732421875 Test Acc: 0.1259765625\n",
      "Step 112 -- Train loss: 2.6299729347229004, Train Acc: 0.12646484375 Test Acc: 0.1259765625\n",
      "Step 113 -- Train loss: 2.62886381149292, Train Acc: 0.122802734375 Test Acc: 0.1162109375\n",
      "Step 114 -- Train loss: 2.629221200942993, Train Acc: 0.121826171875 Test Acc: 0.125\n",
      "Step 115 -- Train loss: 2.633892774581909, Train Acc: 0.1123046875 Test Acc: 0.1103515625\n",
      "Step 116 -- Train loss: 2.628838300704956, Train Acc: 0.122802734375 Test Acc: 0.1298828125\n",
      "Step 117 -- Train loss: 2.6220691204071045, Train Acc: 0.120849609375 Test Acc: 0.126953125\n",
      "Step 118 -- Train loss: 2.625950574874878, Train Acc: 0.129638671875 Test Acc: 0.119140625\n",
      "Step 119 -- Train loss: 2.624605655670166, Train Acc: 0.12548828125 Test Acc: 0.1201171875\n",
      "Step 120 -- Train loss: 2.6248960494995117, Train Acc: 0.12744140625 Test Acc: 0.1337890625\n",
      "Step 121 -- Train loss: 2.626512289047241, Train Acc: 0.127197265625 Test Acc: 0.1201171875\n",
      "Step 122 -- Train loss: 2.6258819103240967, Train Acc: 0.11474609375 Test Acc: 0.1298828125\n",
      "Step 123 -- Train loss: 2.626857042312622, Train Acc: 0.12548828125 Test Acc: 0.119140625\n",
      "Step 124 -- Train loss: 2.6277883052825928, Train Acc: 0.12353515625 Test Acc: 0.1240234375\n",
      "Step 125 -- Train loss: 2.6247718334198, Train Acc: 0.126220703125 Test Acc: 0.1240234375\n",
      "Step 126 -- Train loss: 2.6283321380615234, Train Acc: 0.1298828125 Test Acc: 0.111328125\n",
      "Step 127 -- Train loss: 2.6237246990203857, Train Acc: 0.119873046875 Test Acc: 0.134765625\n",
      "Step 128 -- Train loss: 2.6205482482910156, Train Acc: 0.134033203125 Test Acc: 0.12890625\n",
      "Step 129 -- Train loss: 2.6252670288085938, Train Acc: 0.128662109375 Test Acc: 0.1240234375\n",
      "Step 130 -- Train loss: 2.62568998336792, Train Acc: 0.1201171875 Test Acc: 0.125\n",
      "Step 131 -- Train loss: 2.618917226791382, Train Acc: 0.13037109375 Test Acc: 0.123046875\n",
      "Step 132 -- Train loss: 2.622281312942505, Train Acc: 0.130126953125 Test Acc: 0.1171875\n",
      "Step 133 -- Train loss: 2.6263346672058105, Train Acc: 0.123779296875 Test Acc: 0.126953125\n",
      "Step 134 -- Train loss: 2.6212716102600098, Train Acc: 0.12744140625 Test Acc: 0.1240234375\n",
      "Step 135 -- Train loss: 2.619774580001831, Train Acc: 0.125 Test Acc: 0.123046875\n",
      "Step 136 -- Train loss: 2.6155171394348145, Train Acc: 0.125 Test Acc: 0.125\n",
      "Step 137 -- Train loss: 2.6209394931793213, Train Acc: 0.119140625 Test Acc: 0.1220703125\n",
      "Step 138 -- Train loss: 2.6172564029693604, Train Acc: 0.124755859375 Test Acc: 0.125\n",
      "Step 139 -- Train loss: 2.618939161300659, Train Acc: 0.125244140625 Test Acc: 0.126953125\n",
      "Step 140 -- Train loss: 2.6145684719085693, Train Acc: 0.129638671875 Test Acc: 0.126953125\n",
      "Step 141 -- Train loss: 2.617917060852051, Train Acc: 0.127197265625 Test Acc: 0.1259765625\n",
      "Step 142 -- Train loss: 2.616004467010498, Train Acc: 0.126953125 Test Acc: 0.12890625\n",
      "Step 143 -- Train loss: 2.6182572841644287, Train Acc: 0.123779296875 Test Acc: 0.1123046875\n",
      "Step 144 -- Train loss: 2.6203396320343018, Train Acc: 0.12890625 Test Acc: 0.107421875\n",
      "Step 145 -- Train loss: 2.6137895584106445, Train Acc: 0.1279296875 Test Acc: 0.1298828125\n",
      "Step 146 -- Train loss: 2.614473581314087, Train Acc: 0.126953125 Test Acc: 0.125\n",
      "Step 147 -- Train loss: 2.613348960876465, Train Acc: 0.1201171875 Test Acc: 0.1171875\n",
      "Step 148 -- Train loss: 2.6155378818511963, Train Acc: 0.126708984375 Test Acc: 0.1337890625\n",
      "Step 149 -- Train loss: 2.615346908569336, Train Acc: 0.12548828125 Test Acc: 0.125\n",
      "Step 150 -- Train loss: 2.6129579544067383, Train Acc: 0.12451171875 Test Acc: 0.1318359375\n",
      "Step 151 -- Train loss: 2.6093063354492188, Train Acc: 0.1298828125 Test Acc: 0.1181640625\n",
      "Step 152 -- Train loss: 2.6102254390716553, Train Acc: 0.1318359375 Test Acc: 0.1240234375\n",
      "Step 153 -- Train loss: 2.6053504943847656, Train Acc: 0.12890625 Test Acc: 0.1298828125\n",
      "Step 154 -- Train loss: 2.606414794921875, Train Acc: 0.1337890625 Test Acc: 0.1318359375\n",
      "Step 155 -- Train loss: 2.6091344356536865, Train Acc: 0.130126953125 Test Acc: 0.126953125\n",
      "Step 156 -- Train loss: 2.5986621379852295, Train Acc: 0.13037109375 Test Acc: 0.1259765625\n",
      "Step 157 -- Train loss: 2.6038031578063965, Train Acc: 0.131103515625 Test Acc: 0.130859375\n",
      "Step 158 -- Train loss: 2.602909564971924, Train Acc: 0.135009765625 Test Acc: 0.1318359375\n",
      "Step 159 -- Train loss: 2.601022481918335, Train Acc: 0.131591796875 Test Acc: 0.1240234375\n",
      "Step 160 -- Train loss: 2.5989341735839844, Train Acc: 0.1337890625 Test Acc: 0.126953125\n",
      "Step 161 -- Train loss: 2.5965871810913086, Train Acc: 0.134521484375 Test Acc: 0.1357421875\n",
      "Step 162 -- Train loss: 2.603631019592285, Train Acc: 0.12841796875 Test Acc: 0.1416015625\n",
      "Step 163 -- Train loss: 2.5936191082000732, Train Acc: 0.135009765625 Test Acc: 0.134765625\n",
      "Step 164 -- Train loss: 2.5941247940063477, Train Acc: 0.14111328125 Test Acc: 0.12890625\n",
      "Step 165 -- Train loss: 2.5925981998443604, Train Acc: 0.142578125 Test Acc: 0.119140625\n",
      "Step 166 -- Train loss: 2.582435369491577, Train Acc: 0.137939453125 Test Acc: 0.126953125\n",
      "Step 167 -- Train loss: 2.594806671142578, Train Acc: 0.129150390625 Test Acc: 0.1318359375\n",
      "Step 168 -- Train loss: 2.585761785507202, Train Acc: 0.13623046875 Test Acc: 0.1376953125\n",
      "Step 169 -- Train loss: 2.579221248626709, Train Acc: 0.13818359375 Test Acc: 0.1396484375\n",
      "Step 170 -- Train loss: 2.582364559173584, Train Acc: 0.138427734375 Test Acc: 0.1279296875\n",
      "Step 171 -- Train loss: 2.574469804763794, Train Acc: 0.14453125 Test Acc: 0.1318359375\n",
      "Step 172 -- Train loss: 2.5750551223754883, Train Acc: 0.1474609375 Test Acc: 0.1376953125\n",
      "Step 173 -- Train loss: 2.569948196411133, Train Acc: 0.14501953125 Test Acc: 0.1533203125\n",
      "Step 174 -- Train loss: 2.567687749862671, Train Acc: 0.1513671875 Test Acc: 0.130859375\n",
      "Step 175 -- Train loss: 2.555579900741577, Train Acc: 0.159912109375 Test Acc: 0.1533203125\n",
      "Step 176 -- Train loss: 2.5493459701538086, Train Acc: 0.156982421875 Test Acc: 0.1552734375\n",
      "Step 177 -- Train loss: 2.556835412979126, Train Acc: 0.157470703125 Test Acc: 0.1416015625\n",
      "Step 178 -- Train loss: 2.546290159225464, Train Acc: 0.163818359375 Test Acc: 0.1630859375\n",
      "Step 179 -- Train loss: 2.537564277648926, Train Acc: 0.166259765625 Test Acc: 0.1640625\n",
      "Step 180 -- Train loss: 2.5188839435577393, Train Acc: 0.178955078125 Test Acc: 0.1689453125\n",
      "Step 181 -- Train loss: 2.513955593109131, Train Acc: 0.185546875 Test Acc: 0.1767578125\n",
      "Step 182 -- Train loss: 2.5136008262634277, Train Acc: 0.185546875 Test Acc: 0.197265625\n",
      "Step 183 -- Train loss: 2.5081489086151123, Train Acc: 0.187744140625 Test Acc: 0.1796875\n",
      "Step 184 -- Train loss: 2.486135482788086, Train Acc: 0.201904296875 Test Acc: 0.201171875\n",
      "Step 185 -- Train loss: 2.48451828956604, Train Acc: 0.208984375 Test Acc: 0.1826171875\n",
      "Step 186 -- Train loss: 2.4594225883483887, Train Acc: 0.2265625 Test Acc: 0.21484375\n",
      "Step 187 -- Train loss: 2.44262957572937, Train Acc: 0.249267578125 Test Acc: 0.232421875\n",
      "Step 188 -- Train loss: 2.4382007122039795, Train Acc: 0.264892578125 Test Acc: 0.275390625\n",
      "Step 189 -- Train loss: 2.419949531555176, Train Acc: 0.2841796875 Test Acc: 0.2783203125\n",
      "Step 190 -- Train loss: 2.400378465652466, Train Acc: 0.2939453125 Test Acc: 0.3056640625\n",
      "Step 191 -- Train loss: 2.3725621700286865, Train Acc: 0.31982421875 Test Acc: 0.2802734375\n",
      "Step 192 -- Train loss: 2.3523435592651367, Train Acc: 0.33349609375 Test Acc: 0.341796875\n",
      "Step 193 -- Train loss: 2.3366711139678955, Train Acc: 0.354248046875 Test Acc: 0.349609375\n",
      "Step 194 -- Train loss: 2.3020074367523193, Train Acc: 0.37841796875 Test Acc: 0.4033203125\n",
      "Step 195 -- Train loss: 2.2604475021362305, Train Acc: 0.418212890625 Test Acc: 0.3974609375\n",
      "Step 196 -- Train loss: 2.2216362953186035, Train Acc: 0.453125 Test Acc: 0.45703125\n",
      "Step 197 -- Train loss: 2.1904869079589844, Train Acc: 0.462890625 Test Acc: 0.4638671875\n",
      "Step 198 -- Train loss: 2.149693489074707, Train Acc: 0.48193359375 Test Acc: 0.478515625\n",
      "Step 199 -- Train loss: 2.1002817153930664, Train Acc: 0.496337890625 Test Acc: 0.4775390625\n",
      "Step 200 -- Train loss: 2.0528857707977295, Train Acc: 0.504150390625 Test Acc: 0.48828125\n",
      "Step 201 -- Train loss: 2.011147975921631, Train Acc: 0.504150390625 Test Acc: 0.5283203125\n",
      "Step 202 -- Train loss: 1.9507217407226562, Train Acc: 0.518798828125 Test Acc: 0.5283203125\n",
      "Step 203 -- Train loss: 1.9075907468795776, Train Acc: 0.52978515625 Test Acc: 0.5322265625\n",
      "Step 204 -- Train loss: 1.8333582878112793, Train Acc: 0.538330078125 Test Acc: 0.5224609375\n",
      "Step 205 -- Train loss: 1.7841434478759766, Train Acc: 0.537841796875 Test Acc: 0.5419921875\n",
      "Step 206 -- Train loss: 1.7571688890457153, Train Acc: 0.538330078125 Test Acc: 0.5263671875\n",
      "Step 207 -- Train loss: 1.6917724609375, Train Acc: 0.5517578125 Test Acc: 0.546875\n",
      "Step 208 -- Train loss: 1.6338473558425903, Train Acc: 0.5615234375 Test Acc: 0.552734375\n",
      "Step 209 -- Train loss: 1.5751327276229858, Train Acc: 0.56689453125 Test Acc: 0.560546875\n",
      "Step 210 -- Train loss: 1.5403555631637573, Train Acc: 0.57861328125 Test Acc: 0.5771484375\n",
      "Step 211 -- Train loss: 1.51108717918396, Train Acc: 0.569091796875 Test Acc: 0.572265625\n",
      "Step 212 -- Train loss: 1.469393253326416, Train Acc: 0.57275390625 Test Acc: 0.5751953125\n",
      "Step 213 -- Train loss: 1.4296870231628418, Train Acc: 0.57861328125 Test Acc: 0.568359375\n",
      "Step 214 -- Train loss: 1.3890551328659058, Train Acc: 0.582763671875 Test Acc: 0.583984375\n",
      "Step 215 -- Train loss: 1.3432058095932007, Train Acc: 0.583984375 Test Acc: 0.5732421875\n",
      "Step 216 -- Train loss: 1.3295562267303467, Train Acc: 0.5751953125 Test Acc: 0.5927734375\n",
      "Step 217 -- Train loss: 1.2596229314804077, Train Acc: 0.591796875 Test Acc: 0.5859375\n",
      "Step 218 -- Train loss: 1.247491478919983, Train Acc: 0.58251953125 Test Acc: 0.59375\n",
      "Step 219 -- Train loss: 1.1897797584533691, Train Acc: 0.59765625 Test Acc: 0.595703125\n",
      "Step 220 -- Train loss: 1.1610418558120728, Train Acc: 0.597412109375 Test Acc: 0.5859375\n",
      "Step 221 -- Train loss: 1.1375367641448975, Train Acc: 0.58935546875 Test Acc: 0.6015625\n",
      "Step 222 -- Train loss: 1.1009489297866821, Train Acc: 0.592529296875 Test Acc: 0.5947265625\n",
      "Step 223 -- Train loss: 1.0575958490371704, Train Acc: 0.594482421875 Test Acc: 0.5888671875\n",
      "Step 224 -- Train loss: 1.0157690048217773, Train Acc: 0.604248046875 Test Acc: 0.5927734375\n",
      "Step 225 -- Train loss: 0.9930917620658875, Train Acc: 0.6162109375 Test Acc: 0.607421875\n",
      "Step 226 -- Train loss: 0.9838618636131287, Train Acc: 0.607421875 Test Acc: 0.5849609375\n",
      "Step 227 -- Train loss: 0.9455779790878296, Train Acc: 0.6103515625 Test Acc: 0.58984375\n",
      "Step 228 -- Train loss: 0.9132483005523682, Train Acc: 0.61572265625 Test Acc: 0.5947265625\n",
      "Step 229 -- Train loss: 0.8904730677604675, Train Acc: 0.630126953125 Test Acc: 0.6171875\n",
      "Step 230 -- Train loss: 0.8819008469581604, Train Acc: 0.62939453125 Test Acc: 0.6298828125\n",
      "Step 231 -- Train loss: 0.8614844679832458, Train Acc: 0.63330078125 Test Acc: 0.6220703125\n",
      "Step 232 -- Train loss: 0.8410677313804626, Train Acc: 0.628173828125 Test Acc: 0.6162109375\n",
      "Step 233 -- Train loss: 0.8155419826507568, Train Acc: 0.654541015625 Test Acc: 0.630859375\n",
      "Step 234 -- Train loss: 0.8195649981498718, Train Acc: 0.6455078125 Test Acc: 0.625\n",
      "Step 235 -- Train loss: 0.8016957640647888, Train Acc: 0.647216796875 Test Acc: 0.623046875\n",
      "Step 236 -- Train loss: 0.8046921491622925, Train Acc: 0.640869140625 Test Acc: 0.62109375\n",
      "Step 237 -- Train loss: 0.7837126851081848, Train Acc: 0.64892578125 Test Acc: 0.626953125\n",
      "Step 238 -- Train loss: 0.7875829935073853, Train Acc: 0.62646484375 Test Acc: 0.62890625\n",
      "Step 239 -- Train loss: 0.763995885848999, Train Acc: 0.6435546875 Test Acc: 0.6376953125\n",
      "Step 240 -- Train loss: 0.7592734098434448, Train Acc: 0.64990234375 Test Acc: 0.64453125\n",
      "Step 241 -- Train loss: 0.7767332792282104, Train Acc: 0.6298828125 Test Acc: 0.626953125\n",
      "Step 242 -- Train loss: 0.7430749535560608, Train Acc: 0.669677734375 Test Acc: 0.64453125\n",
      "Step 243 -- Train loss: 0.7383373379707336, Train Acc: 0.659912109375 Test Acc: 0.6552734375\n",
      "Step 244 -- Train loss: 0.7311133146286011, Train Acc: 0.662841796875 Test Acc: 0.638671875\n",
      "Step 245 -- Train loss: 0.7241838574409485, Train Acc: 0.6630859375 Test Acc: 0.6455078125\n",
      "Step 246 -- Train loss: 0.718526303768158, Train Acc: 0.66796875 Test Acc: 0.62109375\n",
      "Step 247 -- Train loss: 0.7108602523803711, Train Acc: 0.678466796875 Test Acc: 0.634765625\n",
      "Step 248 -- Train loss: 0.7136496901512146, Train Acc: 0.68310546875 Test Acc: 0.6689453125\n",
      "Step 249 -- Train loss: 0.6993374824523926, Train Acc: 0.68603515625 Test Acc: 0.6650390625\n",
      "Step 250 -- Train loss: 0.6970964670181274, Train Acc: 0.6787109375 Test Acc: 0.6513671875\n",
      "Step 251 -- Train loss: 0.693424642086029, Train Acc: 0.692138671875 Test Acc: 0.689453125\n",
      "Step 252 -- Train loss: 0.6940243244171143, Train Acc: 0.7060546875 Test Acc: 0.69140625\n",
      "Step 253 -- Train loss: 0.6803792715072632, Train Acc: 0.70849609375 Test Acc: 0.69140625\n",
      "Step 254 -- Train loss: 0.672744870185852, Train Acc: 0.70458984375 Test Acc: 0.6865234375\n",
      "Step 255 -- Train loss: 0.6715946793556213, Train Acc: 0.69970703125 Test Acc: 0.6748046875\n",
      "Step 256 -- Train loss: 0.6745968461036682, Train Acc: 0.69189453125 Test Acc: 0.6787109375\n",
      "Step 257 -- Train loss: 0.6646310687065125, Train Acc: 0.703857421875 Test Acc: 0.6640625\n",
      "Step 258 -- Train loss: 0.6371591687202454, Train Acc: 0.74365234375 Test Acc: 0.7255859375\n",
      "Step 259 -- Train loss: 0.6538175344467163, Train Acc: 0.730712890625 Test Acc: 0.703125\n",
      "Step 260 -- Train loss: 0.6466835737228394, Train Acc: 0.729736328125 Test Acc: 0.7314453125\n",
      "Step 261 -- Train loss: 0.6346017122268677, Train Acc: 0.739990234375 Test Acc: 0.7314453125\n",
      "Step 262 -- Train loss: 0.6276357173919678, Train Acc: 0.745361328125 Test Acc: 0.6806640625\n",
      "Step 263 -- Train loss: 0.6307989358901978, Train Acc: 0.741455078125 Test Acc: 0.736328125\n",
      "Step 264 -- Train loss: 0.6271690726280212, Train Acc: 0.73046875 Test Acc: 0.72265625\n",
      "Step 265 -- Train loss: 0.6221339106559753, Train Acc: 0.74951171875 Test Acc: 0.7255859375\n",
      "Step 266 -- Train loss: 0.6173973083496094, Train Acc: 0.75634765625 Test Acc: 0.7353515625\n",
      "Step 267 -- Train loss: 0.6183182001113892, Train Acc: 0.74365234375 Test Acc: 0.74609375\n",
      "Step 268 -- Train loss: 0.6146471500396729, Train Acc: 0.763427734375 Test Acc: 0.7587890625\n",
      "Step 269 -- Train loss: 0.6061021685600281, Train Acc: 0.77587890625 Test Acc: 0.7607421875\n",
      "Step 270 -- Train loss: 0.5956961512565613, Train Acc: 0.778564453125 Test Acc: 0.7685546875\n",
      "Step 271 -- Train loss: 0.5932907462120056, Train Acc: 0.776611328125 Test Acc: 0.7744140625\n",
      "Step 272 -- Train loss: 0.5834049582481384, Train Acc: 0.788818359375 Test Acc: 0.75390625\n",
      "Step 273 -- Train loss: 0.5787835121154785, Train Acc: 0.80322265625 Test Acc: 0.8037109375\n",
      "Step 274 -- Train loss: 0.5943219065666199, Train Acc: 0.779296875 Test Acc: 0.7734375\n",
      "Step 275 -- Train loss: 0.5722463130950928, Train Acc: 0.7880859375 Test Acc: 0.7822265625\n",
      "Step 276 -- Train loss: 0.5677546262741089, Train Acc: 0.7919921875 Test Acc: 0.7705078125\n",
      "Step 277 -- Train loss: 0.5682171583175659, Train Acc: 0.792236328125 Test Acc: 0.7802734375\n",
      "Step 278 -- Train loss: 0.5742220282554626, Train Acc: 0.782958984375 Test Acc: 0.7822265625\n",
      "Step 279 -- Train loss: 0.5558961033821106, Train Acc: 0.802001953125 Test Acc: 0.7919921875\n",
      "Step 280 -- Train loss: 0.5661652088165283, Train Acc: 0.797119140625 Test Acc: 0.8017578125\n",
      "Step 281 -- Train loss: 0.5535178780555725, Train Acc: 0.805908203125 Test Acc: 0.8369140625\n",
      "Step 282 -- Train loss: 0.5523767471313477, Train Acc: 0.8193359375 Test Acc: 0.8076171875\n",
      "Step 283 -- Train loss: 0.5480489730834961, Train Acc: 0.829833984375 Test Acc: 0.814453125\n",
      "Step 284 -- Train loss: 0.5409455895423889, Train Acc: 0.818603515625 Test Acc: 0.82421875\n",
      "Step 285 -- Train loss: 0.5358744263648987, Train Acc: 0.8095703125 Test Acc: 0.78515625\n",
      "Step 286 -- Train loss: 0.5289900302886963, Train Acc: 0.831298828125 Test Acc: 0.810546875\n",
      "Step 287 -- Train loss: 0.5349327921867371, Train Acc: 0.820556640625 Test Acc: 0.8232421875\n",
      "Step 288 -- Train loss: 0.5195849537849426, Train Acc: 0.83056640625 Test Acc: 0.80859375\n",
      "Step 289 -- Train loss: 0.5233124494552612, Train Acc: 0.833251953125 Test Acc: 0.837890625\n",
      "Step 290 -- Train loss: 0.5130110383033752, Train Acc: 0.8408203125 Test Acc: 0.8388671875\n",
      "Step 291 -- Train loss: 0.5103154182434082, Train Acc: 0.850830078125 Test Acc: 0.8369140625\n",
      "Step 292 -- Train loss: 0.5132943987846375, Train Acc: 0.851806640625 Test Acc: 0.85546875\n",
      "Step 293 -- Train loss: 0.4983992278575897, Train Acc: 0.863037109375 Test Acc: 0.873046875\n",
      "Step 294 -- Train loss: 0.4948276877403259, Train Acc: 0.861572265625 Test Acc: 0.83984375\n",
      "Step 295 -- Train loss: 0.509550154209137, Train Acc: 0.85009765625 Test Acc: 0.837890625\n",
      "Step 296 -- Train loss: 0.4875469207763672, Train Acc: 0.86279296875 Test Acc: 0.833984375\n",
      "Step 297 -- Train loss: 0.48388782143592834, Train Acc: 0.880126953125 Test Acc: 0.88671875\n",
      "Step 298 -- Train loss: 0.480819970369339, Train Acc: 0.87939453125 Test Acc: 0.8818359375\n",
      "Step 299 -- Train loss: 0.47660353779792786, Train Acc: 0.876708984375 Test Acc: 0.873046875\n",
      "Step 300 -- Train loss: 0.4726615250110626, Train Acc: 0.874267578125 Test Acc: 0.837890625\n",
      "Step 301 -- Train loss: 0.47398266196250916, Train Acc: 0.87451171875 Test Acc: 0.875\n",
      "Step 302 -- Train loss: 0.4494192600250244, Train Acc: 0.888671875 Test Acc: 0.8671875\n",
      "Step 303 -- Train loss: 0.4549890160560608, Train Acc: 0.89404296875 Test Acc: 0.8779296875\n",
      "Step 304 -- Train loss: 0.4623429775238037, Train Acc: 0.89501953125 Test Acc: 0.890625\n",
      "Step 305 -- Train loss: 0.4611439108848572, Train Acc: 0.890625 Test Acc: 0.8935546875\n",
      "Step 306 -- Train loss: 0.4469083845615387, Train Acc: 0.895751953125 Test Acc: 0.88671875\n",
      "Step 307 -- Train loss: 0.43344855308532715, Train Acc: 0.904052734375 Test Acc: 0.8896484375\n",
      "Step 308 -- Train loss: 0.4345184564590454, Train Acc: 0.90625 Test Acc: 0.8974609375\n",
      "Step 309 -- Train loss: 0.42274418473243713, Train Acc: 0.91845703125 Test Acc: 0.9140625\n",
      "Step 310 -- Train loss: 0.41858386993408203, Train Acc: 0.919189453125 Test Acc: 0.9130859375\n",
      "Step 311 -- Train loss: 0.4250844419002533, Train Acc: 0.92626953125 Test Acc: 0.9248046875\n",
      "Step 312 -- Train loss: 0.41981634497642517, Train Acc: 0.926025390625 Test Acc: 0.9326171875\n",
      "Step 313 -- Train loss: 0.4091349244117737, Train Acc: 0.92822265625 Test Acc: 0.919921875\n",
      "Step 314 -- Train loss: 0.4044886529445648, Train Acc: 0.9248046875 Test Acc: 0.9296875\n",
      "Step 315 -- Train loss: 0.395161896944046, Train Acc: 0.939697265625 Test Acc: 0.935546875\n",
      "Step 316 -- Train loss: 0.3980373740196228, Train Acc: 0.944580078125 Test Acc: 0.9404296875\n",
      "Step 317 -- Train loss: 0.38954633474349976, Train Acc: 0.938720703125 Test Acc: 0.9326171875\n",
      "Step 318 -- Train loss: 0.38411498069763184, Train Acc: 0.936279296875 Test Acc: 0.943359375\n",
      "Step 319 -- Train loss: 0.37835967540740967, Train Acc: 0.93505859375 Test Acc: 0.94140625\n",
      "Step 320 -- Train loss: 0.37991058826446533, Train Acc: 0.93994140625 Test Acc: 0.93359375\n",
      "Step 321 -- Train loss: 0.36981138586997986, Train Acc: 0.94677734375 Test Acc: 0.943359375\n",
      "Step 322 -- Train loss: 0.36726075410842896, Train Acc: 0.9482421875 Test Acc: 0.951171875\n",
      "Step 323 -- Train loss: 0.35439053177833557, Train Acc: 0.956787109375 Test Acc: 0.9521484375\n",
      "Step 324 -- Train loss: 0.353010892868042, Train Acc: 0.957275390625 Test Acc: 0.9560546875\n",
      "Step 325 -- Train loss: 0.33885452151298523, Train Acc: 0.959716796875 Test Acc: 0.955078125\n",
      "Step 326 -- Train loss: 0.3386576473712921, Train Acc: 0.962158203125 Test Acc: 0.943359375\n",
      "Step 327 -- Train loss: 0.33370885252952576, Train Acc: 0.967529296875 Test Acc: 0.9697265625\n",
      "Step 328 -- Train loss: 0.3329246938228607, Train Acc: 0.97021484375 Test Acc: 0.9609375\n",
      "Step 329 -- Train loss: 0.32088765501976013, Train Acc: 0.972412109375 Test Acc: 0.9677734375\n",
      "Step 330 -- Train loss: 0.3147081136703491, Train Acc: 0.97509765625 Test Acc: 0.9716796875\n",
      "Step 331 -- Train loss: 0.31574758887290955, Train Acc: 0.972412109375 Test Acc: 0.970703125\n",
      "Step 332 -- Train loss: 0.30081361532211304, Train Acc: 0.97607421875 Test Acc: 0.98046875\n",
      "Step 333 -- Train loss: 0.2982894480228424, Train Acc: 0.97998046875 Test Acc: 0.970703125\n",
      "Step 334 -- Train loss: 0.2971682548522949, Train Acc: 0.9775390625 Test Acc: 0.9814453125\n",
      "Step 335 -- Train loss: 0.2799540162086487, Train Acc: 0.98291015625 Test Acc: 0.97265625\n",
      "Step 336 -- Train loss: 0.28843170404434204, Train Acc: 0.975341796875 Test Acc: 0.9775390625\n",
      "Step 337 -- Train loss: 0.2774721086025238, Train Acc: 0.981201171875 Test Acc: 0.982421875\n",
      "Step 338 -- Train loss: 0.2659987807273865, Train Acc: 0.983642578125 Test Acc: 0.984375\n",
      "Step 339 -- Train loss: 0.26843342185020447, Train Acc: 0.98046875 Test Acc: 0.9814453125\n",
      "Step 340 -- Train loss: 0.25911614298820496, Train Acc: 0.986328125 Test Acc: 0.98828125\n",
      "Step 341 -- Train loss: 0.25430792570114136, Train Acc: 0.988037109375 Test Acc: 0.982421875\n",
      "Step 342 -- Train loss: 0.2499406486749649, Train Acc: 0.987060546875 Test Acc: 0.982421875\n",
      "Step 343 -- Train loss: 0.24410846829414368, Train Acc: 0.988037109375 Test Acc: 0.9833984375\n",
      "Step 344 -- Train loss: 0.24052199721336365, Train Acc: 0.9892578125 Test Acc: 0.986328125\n",
      "Step 345 -- Train loss: 0.23097418248653412, Train Acc: 0.98876953125 Test Acc: 0.9912109375\n",
      "Step 346 -- Train loss: 0.22811418771743774, Train Acc: 0.99267578125 Test Acc: 0.9873046875\n",
      "Step 347 -- Train loss: 0.2222219705581665, Train Acc: 0.99267578125 Test Acc: 0.994140625\n",
      "Step 348 -- Train loss: 0.21839049458503723, Train Acc: 0.9931640625 Test Acc: 0.994140625\n",
      "Step 349 -- Train loss: 0.21449217200279236, Train Acc: 0.989990234375 Test Acc: 0.9931640625\n",
      "Step 350 -- Train loss: 0.20853440463542938, Train Acc: 0.990234375 Test Acc: 0.9921875\n",
      "Step 351 -- Train loss: 0.2021409124135971, Train Acc: 0.992431640625 Test Acc: 0.9931640625\n",
      "Step 352 -- Train loss: 0.19832737743854523, Train Acc: 0.98876953125 Test Acc: 0.998046875\n",
      "Step 353 -- Train loss: 0.18918883800506592, Train Acc: 0.993896484375 Test Acc: 0.9912109375\n",
      "Step 354 -- Train loss: 0.1864481270313263, Train Acc: 0.994140625 Test Acc: 0.994140625\n",
      "Step 355 -- Train loss: 0.17982247471809387, Train Acc: 0.996826171875 Test Acc: 0.998046875\n",
      "Step 356 -- Train loss: 0.17403553426265717, Train Acc: 0.996826171875 Test Acc: 0.9921875\n",
      "Step 357 -- Train loss: 0.17353829741477966, Train Acc: 0.9951171875 Test Acc: 0.9990234375\n",
      "Step 358 -- Train loss: 0.16561590135097504, Train Acc: 0.997802734375 Test Acc: 0.998046875\n",
      "Step 359 -- Train loss: 0.16117973625659943, Train Acc: 0.99609375 Test Acc: 0.994140625\n",
      "Step 360 -- Train loss: 0.15668557584285736, Train Acc: 0.998046875 Test Acc: 0.9990234375\n",
      "Step 361 -- Train loss: 0.1552627831697464, Train Acc: 0.99658203125 Test Acc: 0.9990234375\n",
      "Step 362 -- Train loss: 0.1459958553314209, Train Acc: 0.998779296875 Test Acc: 1.0\n",
      "Step 363 -- Train loss: 0.14506103098392487, Train Acc: 0.998779296875 Test Acc: 1.0\n",
      "Step 364 -- Train loss: 0.13905839622020721, Train Acc: 0.99951171875 Test Acc: 0.998046875\n",
      "Step 365 -- Train loss: 0.13641387224197388, Train Acc: 0.99853515625 Test Acc: 0.998046875\n",
      "Step 366 -- Train loss: 0.13196641206741333, Train Acc: 0.999267578125 Test Acc: 0.9951171875\n",
      "Step 367 -- Train loss: 0.1308426707983017, Train Acc: 0.998779296875 Test Acc: 0.9990234375\n",
      "Step 368 -- Train loss: 0.12715983390808105, Train Acc: 0.9990234375 Test Acc: 1.0\n",
      "Step 369 -- Train loss: 0.1198711022734642, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 370 -- Train loss: 0.11994360387325287, Train Acc: 0.99951171875 Test Acc: 1.0\n",
      "Step 371 -- Train loss: 0.11522316932678223, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 372 -- Train loss: 0.11231016367673874, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 373 -- Train loss: 0.10964562743902206, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 374 -- Train loss: 0.10769934952259064, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 375 -- Train loss: 0.1033240258693695, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 376 -- Train loss: 0.10076424479484558, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 377 -- Train loss: 0.09745874255895615, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 378 -- Train loss: 0.09528796374797821, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 379 -- Train loss: 0.09180605411529541, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 380 -- Train loss: 0.08940520137548447, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 381 -- Train loss: 0.08635502308607101, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 382 -- Train loss: 0.08618982881307602, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 383 -- Train loss: 0.08380410075187683, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 384 -- Train loss: 0.08191156387329102, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 385 -- Train loss: 0.07802000641822815, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 386 -- Train loss: 0.07755792886018753, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 387 -- Train loss: 0.07488322257995605, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 388 -- Train loss: 0.07433672249317169, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 389 -- Train loss: 0.07179050892591476, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 390 -- Train loss: 0.07065083086490631, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 391 -- Train loss: 0.06856943666934967, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 392 -- Train loss: 0.06772944331169128, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 393 -- Train loss: 0.06490771472454071, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 394 -- Train loss: 0.06386611610651016, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 395 -- Train loss: 0.06301812082529068, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 396 -- Train loss: 0.061914797872304916, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 397 -- Train loss: 0.060930270701646805, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 398 -- Train loss: 0.05776664614677429, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 399 -- Train loss: 0.05762543901801109, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 400 -- Train loss: 0.05523323267698288, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 401 -- Train loss: 0.0557173527777195, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 402 -- Train loss: 0.05366438627243042, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 403 -- Train loss: 0.05266472324728966, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 404 -- Train loss: 0.052188675850629807, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 405 -- Train loss: 0.05070595443248749, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 406 -- Train loss: 0.05017593130469322, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 407 -- Train loss: 0.04908519238233566, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 408 -- Train loss: 0.047226130962371826, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 409 -- Train loss: 0.04731952026486397, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 410 -- Train loss: 0.04626297205686569, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 411 -- Train loss: 0.04541093483567238, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 412 -- Train loss: 0.04463187977671623, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 413 -- Train loss: 0.043869175016880035, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 414 -- Train loss: 0.04359934851527214, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 415 -- Train loss: 0.04210186377167702, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 416 -- Train loss: 0.04226893186569214, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 417 -- Train loss: 0.04090817645192146, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 418 -- Train loss: 0.04053817316889763, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 419 -- Train loss: 0.03966758772730827, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 420 -- Train loss: 0.03965412452816963, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 421 -- Train loss: 0.03881169855594635, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 422 -- Train loss: 0.03833460807800293, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 423 -- Train loss: 0.03737930208444595, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 424 -- Train loss: 0.037124037742614746, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 425 -- Train loss: 0.036930520087480545, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 426 -- Train loss: 0.03591051697731018, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 427 -- Train loss: 0.03549836575984955, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 428 -- Train loss: 0.03471514582633972, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 429 -- Train loss: 0.03409943729639053, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 430 -- Train loss: 0.034446679055690765, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 431 -- Train loss: 0.0336449071764946, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 432 -- Train loss: 0.03264571353793144, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 433 -- Train loss: 0.03245468810200691, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 434 -- Train loss: 0.03234720230102539, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 435 -- Train loss: 0.03210680931806564, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 436 -- Train loss: 0.03145161271095276, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 437 -- Train loss: 0.03133860602974892, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 438 -- Train loss: 0.030757179483771324, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 439 -- Train loss: 0.030183805152773857, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 440 -- Train loss: 0.02977173402905464, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 441 -- Train loss: 0.029643315821886063, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 442 -- Train loss: 0.029169628396630287, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 443 -- Train loss: 0.028631161898374557, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 444 -- Train loss: 0.028694694861769676, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 445 -- Train loss: 0.027880297973752022, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 446 -- Train loss: 0.027880370616912842, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 447 -- Train loss: 0.02733137086033821, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 448 -- Train loss: 0.026688670739531517, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 449 -- Train loss: 0.026539642363786697, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 450 -- Train loss: 0.02643464505672455, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 451 -- Train loss: 0.0266260989010334, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 452 -- Train loss: 0.026032716035842896, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 453 -- Train loss: 0.02541436441242695, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 454 -- Train loss: 0.02537882700562477, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 455 -- Train loss: 0.0253372173756361, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 456 -- Train loss: 0.025286229327321053, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 457 -- Train loss: 0.024773454293608665, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 458 -- Train loss: 0.02409425377845764, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 459 -- Train loss: 0.024132870137691498, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 460 -- Train loss: 0.023832611739635468, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 461 -- Train loss: 0.023572251200675964, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 462 -- Train loss: 0.023310694843530655, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 463 -- Train loss: 0.02319101244211197, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 464 -- Train loss: 0.023272909224033356, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 465 -- Train loss: 0.022829683497548103, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 466 -- Train loss: 0.02271333523094654, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 467 -- Train loss: 0.022429615259170532, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 468 -- Train loss: 0.02230784110724926, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 469 -- Train loss: 0.021792257204651833, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 470 -- Train loss: 0.022000504657626152, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 471 -- Train loss: 0.021637653931975365, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 472 -- Train loss: 0.02136390283703804, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 473 -- Train loss: 0.020922159776091576, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 474 -- Train loss: 0.020887522026896477, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 475 -- Train loss: 0.020790472626686096, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 476 -- Train loss: 0.020648784935474396, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 477 -- Train loss: 0.02043749764561653, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 478 -- Train loss: 0.020323488861322403, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 479 -- Train loss: 0.0198212843388319, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 480 -- Train loss: 0.019836869090795517, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 481 -- Train loss: 0.019339503720402718, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 482 -- Train loss: 0.01944819651544094, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 483 -- Train loss: 0.01966581866145134, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 484 -- Train loss: 0.019133254885673523, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 485 -- Train loss: 0.01927119866013527, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 486 -- Train loss: 0.01904372125864029, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 487 -- Train loss: 0.018511533737182617, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 488 -- Train loss: 0.01842697523534298, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 489 -- Train loss: 0.018154865130782127, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 490 -- Train loss: 0.018063288182020187, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 491 -- Train loss: 0.018418312072753906, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 492 -- Train loss: 0.017904339358210564, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 493 -- Train loss: 0.017880337312817574, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 494 -- Train loss: 0.017864471301436424, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 495 -- Train loss: 0.01767796091735363, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 496 -- Train loss: 0.017592381685972214, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 497 -- Train loss: 0.017489153891801834, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 498 -- Train loss: 0.017550790682435036, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 499 -- Train loss: 0.017046932131052017, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 500 -- Train loss: 0.01684441976249218, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 501 -- Train loss: 0.016658207401633263, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 502 -- Train loss: 0.016556808724999428, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 503 -- Train loss: 0.01682974398136139, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 504 -- Train loss: 0.016320394352078438, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 505 -- Train loss: 0.016147887334227562, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 506 -- Train loss: 0.016238966956734657, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 507 -- Train loss: 0.016192227602005005, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 508 -- Train loss: 0.01589205116033554, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 509 -- Train loss: 0.01562763936817646, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 510 -- Train loss: 0.015728885307908058, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 511 -- Train loss: 0.015531247481703758, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 512 -- Train loss: 0.01573781482875347, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 513 -- Train loss: 0.015294073149561882, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 514 -- Train loss: 0.015455203130841255, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 515 -- Train loss: 0.015164416283369064, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 516 -- Train loss: 0.015476323664188385, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 517 -- Train loss: 0.015323999337852001, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 518 -- Train loss: 0.015097292140126228, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 519 -- Train loss: 0.014536161907017231, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 520 -- Train loss: 0.014751575887203217, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 521 -- Train loss: 0.014470942318439484, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 522 -- Train loss: 0.01477909181267023, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 523 -- Train loss: 0.014359372667968273, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 524 -- Train loss: 0.014494647271931171, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 525 -- Train loss: 0.01438988745212555, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 526 -- Train loss: 0.014130479656159878, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 527 -- Train loss: 0.014133310876786709, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 528 -- Train loss: 0.013930953107774258, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 529 -- Train loss: 0.014035664498806, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 530 -- Train loss: 0.013933113776147366, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 531 -- Train loss: 0.013639550656080246, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 532 -- Train loss: 0.013591969385743141, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 533 -- Train loss: 0.013273120857775211, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 534 -- Train loss: 0.013540639542043209, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 535 -- Train loss: 0.01348811574280262, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 536 -- Train loss: 0.013353545218706131, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 537 -- Train loss: 0.013232539407908916, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 538 -- Train loss: 0.012968577444553375, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 539 -- Train loss: 0.012833895161747932, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 540 -- Train loss: 0.012968562543392181, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 541 -- Train loss: 0.012952269986271858, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 542 -- Train loss: 0.0127763906493783, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 543 -- Train loss: 0.012589982710778713, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 544 -- Train loss: 0.012631835415959358, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 545 -- Train loss: 0.012620082125067711, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 546 -- Train loss: 0.012472478672862053, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 547 -- Train loss: 0.012394580990076065, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 548 -- Train loss: 0.012581831775605679, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 549 -- Train loss: 0.012294908985495567, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 550 -- Train loss: 0.012175395153462887, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 551 -- Train loss: 0.012121391482651234, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 552 -- Train loss: 0.01192964892834425, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 553 -- Train loss: 0.01200177427381277, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 554 -- Train loss: 0.011827671900391579, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 555 -- Train loss: 0.01166690606623888, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 556 -- Train loss: 0.011810080148279667, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 557 -- Train loss: 0.011762595735490322, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 558 -- Train loss: 0.01178030576556921, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 559 -- Train loss: 0.011567054316401482, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 560 -- Train loss: 0.01151175145059824, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 561 -- Train loss: 0.011410746723413467, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 562 -- Train loss: 0.011614941991865635, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 563 -- Train loss: 0.011375632137060165, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 564 -- Train loss: 0.011166430078446865, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 565 -- Train loss: 0.011207051575183868, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 566 -- Train loss: 0.011166147887706757, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 567 -- Train loss: 0.011140023358166218, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 568 -- Train loss: 0.011013518087565899, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 569 -- Train loss: 0.011004135943949223, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 570 -- Train loss: 0.010732115246355534, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 571 -- Train loss: 0.01091782096773386, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 572 -- Train loss: 0.010906527750194073, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 573 -- Train loss: 0.010786625556647778, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 574 -- Train loss: 0.01070703286677599, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 575 -- Train loss: 0.010702785104513168, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 576 -- Train loss: 0.010820127092301846, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 577 -- Train loss: 0.010588768869638443, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 578 -- Train loss: 0.01027844287455082, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 579 -- Train loss: 0.01039411686360836, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 580 -- Train loss: 0.010339966043829918, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 581 -- Train loss: 0.010349152609705925, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 582 -- Train loss: 0.010285045951604843, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 583 -- Train loss: 0.010218249633908272, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 584 -- Train loss: 0.010344087146222591, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 585 -- Train loss: 0.010122003965079784, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 586 -- Train loss: 0.010010403580963612, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 587 -- Train loss: 0.009976334869861603, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 588 -- Train loss: 0.010010207071900368, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 589 -- Train loss: 0.00994726549834013, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 590 -- Train loss: 0.009994600899517536, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 591 -- Train loss: 0.009839375503361225, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 592 -- Train loss: 0.009715749882161617, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 593 -- Train loss: 0.00966572854667902, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 594 -- Train loss: 0.009491312317550182, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 595 -- Train loss: 0.009566822089254856, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 596 -- Train loss: 0.00942272413522005, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 597 -- Train loss: 0.009394895285367966, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 598 -- Train loss: 0.009441059082746506, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 599 -- Train loss: 0.009456832893192768, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 600 -- Train loss: 0.009390834718942642, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 601 -- Train loss: 0.00935285072773695, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 602 -- Train loss: 0.009238079190254211, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 603 -- Train loss: 0.009312167763710022, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 604 -- Train loss: 0.009173775091767311, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 605 -- Train loss: 0.009132957085967064, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 606 -- Train loss: 0.009093203581869602, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 607 -- Train loss: 0.009054424241185188, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 608 -- Train loss: 0.00904502347111702, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 609 -- Train loss: 0.009040744043886662, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 610 -- Train loss: 0.008948993869125843, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 611 -- Train loss: 0.008915943093597889, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 612 -- Train loss: 0.008910467848181725, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 613 -- Train loss: 0.008796906098723412, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 614 -- Train loss: 0.008729295805096626, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 615 -- Train loss: 0.008760051801800728, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 616 -- Train loss: 0.008783698081970215, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 617 -- Train loss: 0.008635631762444973, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 618 -- Train loss: 0.008611518889665604, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 619 -- Train loss: 0.008537939749658108, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 620 -- Train loss: 0.008455704897642136, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 621 -- Train loss: 0.008433010429143906, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 622 -- Train loss: 0.008429761044681072, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 623 -- Train loss: 0.008515392430126667, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 624 -- Train loss: 0.008387792855501175, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 625 -- Train loss: 0.008418628014624119, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 626 -- Train loss: 0.008337095379829407, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 627 -- Train loss: 0.008052315562963486, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 628 -- Train loss: 0.008242167532444, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 629 -- Train loss: 0.008352122269570827, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 630 -- Train loss: 0.008118978701531887, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 631 -- Train loss: 0.00809243693947792, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 632 -- Train loss: 0.008129464462399483, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 633 -- Train loss: 0.008101322688162327, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 634 -- Train loss: 0.008151119574904442, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 635 -- Train loss: 0.007954724133014679, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 636 -- Train loss: 0.00795071106404066, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 637 -- Train loss: 0.007978650741279125, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 638 -- Train loss: 0.00793425552546978, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 639 -- Train loss: 0.00783520471304655, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 640 -- Train loss: 0.007864097133278847, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 641 -- Train loss: 0.007771333679556847, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 642 -- Train loss: 0.007738638669252396, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 643 -- Train loss: 0.007754305377602577, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 644 -- Train loss: 0.007681252434849739, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 645 -- Train loss: 0.007611095905303955, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 646 -- Train loss: 0.007652066648006439, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 647 -- Train loss: 0.007529950235038996, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 648 -- Train loss: 0.0076459660194814205, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 649 -- Train loss: 0.0076007782481610775, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 650 -- Train loss: 0.007487012539058924, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 651 -- Train loss: 0.007496361155062914, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 652 -- Train loss: 0.00730828195810318, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 653 -- Train loss: 0.007409264799207449, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 654 -- Train loss: 0.007382793352007866, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 655 -- Train loss: 0.007317299954593182, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 656 -- Train loss: 0.007268824614584446, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 657 -- Train loss: 0.007257690187543631, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 658 -- Train loss: 0.007323622237890959, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 659 -- Train loss: 0.007277925498783588, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 660 -- Train loss: 0.007239219266921282, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 661 -- Train loss: 0.0071558658964931965, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 662 -- Train loss: 0.007101643364876509, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 663 -- Train loss: 0.007164785638451576, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 664 -- Train loss: 0.007019314914941788, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 665 -- Train loss: 0.007074842695146799, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 666 -- Train loss: 0.007112451363354921, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 667 -- Train loss: 0.0070759616792202, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 668 -- Train loss: 0.006920419633388519, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 669 -- Train loss: 0.00694282678887248, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 670 -- Train loss: 0.007003091275691986, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 671 -- Train loss: 0.006922660395503044, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 672 -- Train loss: 0.006780521012842655, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 673 -- Train loss: 0.006847070995718241, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 674 -- Train loss: 0.006896348670125008, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 675 -- Train loss: 0.006850493606179953, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 676 -- Train loss: 0.0067507377825677395, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 677 -- Train loss: 0.006758328527212143, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 678 -- Train loss: 0.0067728785797953606, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 679 -- Train loss: 0.006729378364980221, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 680 -- Train loss: 0.006661037914454937, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 681 -- Train loss: 0.006711655296385288, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 682 -- Train loss: 0.006697720382362604, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 683 -- Train loss: 0.006545334588736296, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 684 -- Train loss: 0.0065345303155481815, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 685 -- Train loss: 0.006537309382110834, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 686 -- Train loss: 0.006498171016573906, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 687 -- Train loss: 0.006429428234696388, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 688 -- Train loss: 0.006462795659899712, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 689 -- Train loss: 0.006512258667498827, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 690 -- Train loss: 0.006417716853320599, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 691 -- Train loss: 0.006436272524297237, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 692 -- Train loss: 0.006346754729747772, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 693 -- Train loss: 0.006299843546003103, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 694 -- Train loss: 0.006280458997935057, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 695 -- Train loss: 0.0062482780776917934, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 696 -- Train loss: 0.006346884183585644, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 697 -- Train loss: 0.006175050046294928, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 698 -- Train loss: 0.006260776426643133, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 699 -- Train loss: 0.0062753185629844666, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 700 -- Train loss: 0.006222651805728674, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 701 -- Train loss: 0.00629538856446743, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 702 -- Train loss: 0.006132863461971283, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 703 -- Train loss: 0.006137616001069546, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 704 -- Train loss: 0.006119959056377411, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 705 -- Train loss: 0.0060180798172950745, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 706 -- Train loss: 0.006005133036524057, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 707 -- Train loss: 0.005968943238258362, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 708 -- Train loss: 0.00601578876376152, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 709 -- Train loss: 0.006090403068810701, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 710 -- Train loss: 0.005943214986473322, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 711 -- Train loss: 0.0060016922652721405, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 712 -- Train loss: 0.006008220836520195, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 713 -- Train loss: 0.005934867076575756, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 714 -- Train loss: 0.005895973648875952, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 715 -- Train loss: 0.0059493170119822025, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 716 -- Train loss: 0.005762201733887196, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 717 -- Train loss: 0.005820690654218197, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 718 -- Train loss: 0.005701036658138037, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 719 -- Train loss: 0.005775210447609425, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 720 -- Train loss: 0.005804714281111956, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 721 -- Train loss: 0.005809797439724207, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 722 -- Train loss: 0.00582479452714324, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 723 -- Train loss: 0.005637140944600105, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 724 -- Train loss: 0.005697460379451513, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 725 -- Train loss: 0.005645742639899254, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 726 -- Train loss: 0.005616420414298773, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 727 -- Train loss: 0.005587913561612368, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 728 -- Train loss: 0.005686948075890541, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 729 -- Train loss: 0.005562582053244114, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 730 -- Train loss: 0.005565519444644451, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 731 -- Train loss: 0.0055363476276397705, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 732 -- Train loss: 0.005507364869117737, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 733 -- Train loss: 0.00560141634196043, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 734 -- Train loss: 0.005517865065485239, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 735 -- Train loss: 0.005474277772009373, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 736 -- Train loss: 0.005449999123811722, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 737 -- Train loss: 0.005397046450525522, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 738 -- Train loss: 0.005448063835501671, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 739 -- Train loss: 0.0053977021016180515, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 740 -- Train loss: 0.005401386879384518, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 741 -- Train loss: 0.00537183927372098, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 742 -- Train loss: 0.005326998885720968, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 743 -- Train loss: 0.005276093725115061, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 744 -- Train loss: 0.005304327700287104, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 745 -- Train loss: 0.005369862541556358, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 746 -- Train loss: 0.005239357706159353, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 747 -- Train loss: 0.005287446081638336, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 748 -- Train loss: 0.005255391821265221, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 749 -- Train loss: 0.00525408610701561, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 750 -- Train loss: 0.00524046178907156, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 751 -- Train loss: 0.005216460209339857, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 752 -- Train loss: 0.005202549509704113, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 753 -- Train loss: 0.00509911822155118, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 754 -- Train loss: 0.005181384272873402, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 755 -- Train loss: 0.005164961330592632, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 756 -- Train loss: 0.005151259712874889, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 757 -- Train loss: 0.0050246333703398705, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 758 -- Train loss: 0.005176857579499483, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 759 -- Train loss: 0.0050897677429020405, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 760 -- Train loss: 0.00504726218059659, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 761 -- Train loss: 0.005041182041168213, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 762 -- Train loss: 0.004987805616110563, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 763 -- Train loss: 0.005069288890808821, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 764 -- Train loss: 0.005022066179662943, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 765 -- Train loss: 0.005072645843029022, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 766 -- Train loss: 0.004905716050416231, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 767 -- Train loss: 0.004942416679114103, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 768 -- Train loss: 0.004916484002023935, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 769 -- Train loss: 0.004911367315798998, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 770 -- Train loss: 0.005008678883314133, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 771 -- Train loss: 0.00487944670021534, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 772 -- Train loss: 0.004900470841675997, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 773 -- Train loss: 0.004789127502590418, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 774 -- Train loss: 0.004836869426071644, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 775 -- Train loss: 0.004809595178812742, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 776 -- Train loss: 0.004858603235334158, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 777 -- Train loss: 0.004794536158442497, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 778 -- Train loss: 0.004779587499797344, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 779 -- Train loss: 0.004851288162171841, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 780 -- Train loss: 0.004693842958658934, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 781 -- Train loss: 0.004746436607092619, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 782 -- Train loss: 0.004708375781774521, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 783 -- Train loss: 0.004663622006773949, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 784 -- Train loss: 0.004692903254181147, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 785 -- Train loss: 0.004681592807173729, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 786 -- Train loss: 0.004649656359106302, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 787 -- Train loss: 0.004639614839106798, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 788 -- Train loss: 0.004689495079219341, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 789 -- Train loss: 0.004636706784367561, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 790 -- Train loss: 0.0045587243512272835, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 791 -- Train loss: 0.00458896066993475, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 792 -- Train loss: 0.004577812738716602, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 793 -- Train loss: 0.004548166878521442, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 794 -- Train loss: 0.004496387671679258, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 795 -- Train loss: 0.004625949077308178, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 796 -- Train loss: 0.004563583061099052, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 797 -- Train loss: 0.004587186500430107, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 798 -- Train loss: 0.004502739291638136, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 799 -- Train loss: 0.004486013203859329, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 800 -- Train loss: 0.004507320001721382, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 801 -- Train loss: 0.004416365176439285, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 802 -- Train loss: 0.0044022598303854465, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 803 -- Train loss: 0.004439961165189743, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 804 -- Train loss: 0.004485685843974352, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 805 -- Train loss: 0.004412356298416853, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 806 -- Train loss: 0.004409878980368376, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 807 -- Train loss: 0.004427323583513498, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 808 -- Train loss: 0.004467388615012169, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 809 -- Train loss: 0.004394534509629011, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 810 -- Train loss: 0.004424194805324078, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 811 -- Train loss: 0.004285256378352642, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 812 -- Train loss: 0.004320286214351654, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 813 -- Train loss: 0.004304839763790369, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 814 -- Train loss: 0.004304345231503248, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 815 -- Train loss: 0.0042892200872302055, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 816 -- Train loss: 0.004333890043199062, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 817 -- Train loss: 0.0042686741799116135, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 818 -- Train loss: 0.004325230605900288, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 819 -- Train loss: 0.004228843841701746, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 820 -- Train loss: 0.004188993945717812, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 821 -- Train loss: 0.004281848669052124, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 822 -- Train loss: 0.004213768523186445, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 823 -- Train loss: 0.004228359088301659, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 824 -- Train loss: 0.004193551372736692, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 825 -- Train loss: 0.004210642538964748, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 826 -- Train loss: 0.004145871847867966, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 827 -- Train loss: 0.004122214857488871, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 828 -- Train loss: 0.0041504791006445885, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 829 -- Train loss: 0.004089518915861845, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 830 -- Train loss: 0.004099095705896616, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 831 -- Train loss: 0.004110374022275209, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 832 -- Train loss: 0.00412016874179244, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 833 -- Train loss: 0.004113721661269665, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 834 -- Train loss: 0.004036229569464922, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 835 -- Train loss: 0.004068403970450163, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 836 -- Train loss: 0.004058307968080044, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 837 -- Train loss: 0.00402944628149271, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 838 -- Train loss: 0.004049941897392273, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 839 -- Train loss: 0.00404210202395916, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 840 -- Train loss: 0.0040122452192008495, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 841 -- Train loss: 0.003983470611274242, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 842 -- Train loss: 0.004000377841293812, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 843 -- Train loss: 0.004033251199871302, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 844 -- Train loss: 0.003945460543036461, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 845 -- Train loss: 0.003960076253861189, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 846 -- Train loss: 0.003963956609368324, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 847 -- Train loss: 0.003951473161578178, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 848 -- Train loss: 0.00391983101144433, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 849 -- Train loss: 0.003913569264113903, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 850 -- Train loss: 0.003950769081711769, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 851 -- Train loss: 0.003846884937956929, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 852 -- Train loss: 0.0038342985790222883, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 853 -- Train loss: 0.003923167008906603, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 854 -- Train loss: 0.0038151987828314304, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 855 -- Train loss: 0.0038336808793246746, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 856 -- Train loss: 0.0038442325312644243, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 857 -- Train loss: 0.003839600132778287, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 858 -- Train loss: 0.0038122935220599174, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 859 -- Train loss: 0.0038444818928837776, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 860 -- Train loss: 0.003794329706579447, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 861 -- Train loss: 0.0038438881747424603, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 862 -- Train loss: 0.003737971419468522, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 863 -- Train loss: 0.003737076884135604, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 864 -- Train loss: 0.0037787556648254395, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 865 -- Train loss: 0.0037022533360868692, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 866 -- Train loss: 0.003712801728397608, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 867 -- Train loss: 0.003720917273312807, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 868 -- Train loss: 0.0036918679252266884, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 869 -- Train loss: 0.003723829984664917, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 870 -- Train loss: 0.0036939375568181276, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 871 -- Train loss: 0.0036620318423956633, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 872 -- Train loss: 0.003688275581225753, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 873 -- Train loss: 0.0036703236401081085, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 874 -- Train loss: 0.0036402724217623472, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 875 -- Train loss: 0.0036225472576916218, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 876 -- Train loss: 0.0036303959786891937, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 877 -- Train loss: 0.0036876185331493616, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 878 -- Train loss: 0.0036197034642100334, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 879 -- Train loss: 0.0035798107273876667, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 880 -- Train loss: 0.0035910026635974646, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 881 -- Train loss: 0.003533709794282913, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 882 -- Train loss: 0.003574695670977235, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 883 -- Train loss: 0.0035469899885356426, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 884 -- Train loss: 0.0035571123007684946, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 885 -- Train loss: 0.003543584607541561, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 886 -- Train loss: 0.0036027126479893923, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 887 -- Train loss: 0.003522672923281789, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 888 -- Train loss: 0.0034747575409710407, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 889 -- Train loss: 0.0034984403755515814, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 890 -- Train loss: 0.0035409540869295597, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 891 -- Train loss: 0.0034932114649564028, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 892 -- Train loss: 0.003498748643323779, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 893 -- Train loss: 0.003500003833323717, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 894 -- Train loss: 0.003470008261501789, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 895 -- Train loss: 0.0034437570720911026, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 896 -- Train loss: 0.003444959642365575, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 897 -- Train loss: 0.0034252190962433815, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 898 -- Train loss: 0.0034634671173989773, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 899 -- Train loss: 0.003443920984864235, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 900 -- Train loss: 0.0034630217123776674, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 901 -- Train loss: 0.0034432453103363514, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 902 -- Train loss: 0.003435059217736125, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 903 -- Train loss: 0.0034075516741722822, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 904 -- Train loss: 0.0033891547936946154, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 905 -- Train loss: 0.003400404006242752, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 906 -- Train loss: 0.0034120632335543633, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 907 -- Train loss: 0.00340139283798635, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 908 -- Train loss: 0.003338174894452095, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 909 -- Train loss: 0.003400098765268922, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 910 -- Train loss: 0.0033639948815107346, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 911 -- Train loss: 0.003331710584461689, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 912 -- Train loss: 0.0033422932028770447, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 913 -- Train loss: 0.0033643790520727634, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 914 -- Train loss: 0.0033101276494562626, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 915 -- Train loss: 0.0033098445273935795, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 916 -- Train loss: 0.0032998903188854456, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 917 -- Train loss: 0.0032568625174462795, Train Acc: 1.0 Test Acc: 1.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m~/.conda/envs/emerge/lib/python3.8/site-packages/PIL/ImageFile.py:515\u001b[0m, in \u001b[0;36m_save\u001b[0;34m(im, fp, tile, bufsize)\u001b[0m\n\u001b[1;32m    514\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 515\u001b[0m     fh \u001b[38;5;241m=\u001b[39m \u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfileno\u001b[49m()\n\u001b[1;32m    516\u001b[0m     fp\u001b[38;5;241m.\u001b[39mflush()\n",
      "\u001b[0;31mAttributeError\u001b[0m: '_idat' object has no attribute 'fileno'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 40\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Training loop\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(config\u001b[38;5;241m.\u001b[39mtrain\u001b[38;5;241m.\u001b[39mnum_steps):\n\u001b[0;32m---> 40\u001b[0m     \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata_samplers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_samplers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(config\u001b[38;5;241m.\u001b[39mtrain, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwandb\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m     50\u001b[0m     wandb\u001b[38;5;241m.\u001b[39mfinish()\n",
      "Cell \u001b[0;32mIn[22], line 227\u001b[0m, in \u001b[0;36mtrain_step\u001b[0;34m(model, optim, data_samplers, step, config, device)\u001b[0m\n\u001b[1;32m    224\u001b[0m     ax_head\u001b[38;5;241m.\u001b[39mset_yticks(\u001b[38;5;28mrange\u001b[39m(avg_attn_per_head[h]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m]))\n\u001b[1;32m    226\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39mtrain\u001b[38;5;241m.\u001b[39mwandb:\n\u001b[0;32m--> 227\u001b[0m         \u001b[43mwandb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHead_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mh\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_avg_attention\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfig_head\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    228\u001b[0m     plt\u001b[38;5;241m.\u001b[39mclose(fig_head)\n\u001b[1;32m    231\u001b[0m \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m    232\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStep \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m -- Train loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Train Acc: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_acc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Test Acc: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_acc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    233\u001b[0m )\n",
      "File \u001b[0;32m~/.conda/envs/emerge/lib/python3.8/site-packages/wandb/sdk/wandb_run.py:420\u001b[0m, in \u001b[0;36m_run_decorator._noop.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    417\u001b[0m         wandb\u001b[38;5;241m.\u001b[39mtermwarn(message, repeat\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    418\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mDummy()\n\u001b[0;32m--> 420\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/emerge/lib/python3.8/site-packages/wandb/sdk/wandb_run.py:371\u001b[0m, in \u001b[0;36m_run_decorator._noop_on_finish.<locals>.decorator_fn.<locals>.wrapper_fn\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    368\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    369\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapper_fn\u001b[39m(\u001b[38;5;28mself\u001b[39m: Type[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRun\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    370\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_is_finished\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m--> 371\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    373\u001b[0m     default_message \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    374\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRun (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mid\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) is finished. The call to `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` will be ignored. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    375\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease make sure that you are using an active run.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    376\u001b[0m     )\n\u001b[1;32m    377\u001b[0m     resolved_message \u001b[38;5;241m=\u001b[39m message \u001b[38;5;129;01mor\u001b[39;00m default_message\n",
      "File \u001b[0;32m~/.conda/envs/emerge/lib/python3.8/site-packages/wandb/sdk/wandb_run.py:361\u001b[0m, in \u001b[0;36m_run_decorator._attach.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    359\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    360\u001b[0m     \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_is_attaching \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 361\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/emerge/lib/python3.8/site-packages/wandb/sdk/wandb_run.py:1820\u001b[0m, in \u001b[0;36mRun.log\u001b[0;34m(self, data, step, commit, sync)\u001b[0m\n\u001b[1;32m   1813\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sync \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1814\u001b[0m     deprecate\u001b[38;5;241m.\u001b[39mdeprecate(\n\u001b[1;32m   1815\u001b[0m         field_name\u001b[38;5;241m=\u001b[39mdeprecate\u001b[38;5;241m.\u001b[39mDeprecated\u001b[38;5;241m.\u001b[39mrun__log_sync,\n\u001b[1;32m   1816\u001b[0m         warning_message\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1817\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`sync` argument is deprecated and does not affect the behaviour of `wandb.log`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1818\u001b[0m         ),\n\u001b[1;32m   1819\u001b[0m     )\n\u001b[0;32m-> 1820\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_log\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcommit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/emerge/lib/python3.8/site-packages/wandb/sdk/wandb_run.py:1595\u001b[0m, in \u001b[0;36mRun._log\u001b[0;34m(self, data, step, commit)\u001b[0m\n\u001b[1;32m   1592\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m data\u001b[38;5;241m.\u001b[39mkeys()):\n\u001b[1;32m   1593\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKey values passed to `wandb.log` must be strings.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1595\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_partial_history_callback\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcommit\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1597\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m step \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1598\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mgetpid() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_pid \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_attached:\n",
      "File \u001b[0;32m~/.conda/envs/emerge/lib/python3.8/site-packages/wandb/sdk/wandb_run.py:1467\u001b[0m, in \u001b[0;36mRun._partial_history_callback\u001b[0;34m(self, row, step, commit)\u001b[0m\n\u001b[1;32m   1464\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39minterface:\n\u001b[1;32m   1465\u001b[0m     not_using_tensorboard \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(wandb\u001b[38;5;241m.\u001b[39mpatched[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtensorboard\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1467\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterface\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpublish_partial_history\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1468\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1469\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_step\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_step\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1470\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1471\u001b[0m \u001b[43m        \u001b[49m\u001b[43mflush\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1472\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpublish_step\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnot_using_tensorboard\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1473\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/emerge/lib/python3.8/site-packages/wandb/sdk/interface/interface.py:551\u001b[0m, in \u001b[0;36mInterfaceBase.publish_partial_history\u001b[0;34m(self, data, user_step, step, flush, publish_step, run)\u001b[0m\n\u001b[1;32m    540\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpublish_partial_history\u001b[39m(\n\u001b[1;32m    541\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    542\u001b[0m     data: \u001b[38;5;28mdict\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    547\u001b[0m     run: Optional[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRun\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    548\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    549\u001b[0m     run \u001b[38;5;241m=\u001b[39m run \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run\n\u001b[0;32m--> 551\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mhistory_dict_to_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_copy_err\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    552\u001b[0m     data\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_step\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    554\u001b[0m     \u001b[38;5;66;03m# add timestamp to the history request, if not already present\u001b[39;00m\n\u001b[1;32m    555\u001b[0m     \u001b[38;5;66;03m# the timestamp might come from the tensorboard log logic\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/emerge/lib/python3.8/site-packages/wandb/sdk/data_types/utils.py:52\u001b[0m, in \u001b[0;36mhistory_dict_to_json\u001b[0;34m(run, payload, step, ignore_copy_err)\u001b[0m\n\u001b[1;32m     48\u001b[0m         payload[key] \u001b[38;5;241m=\u001b[39m history_dict_to_json(\n\u001b[1;32m     49\u001b[0m             run, val, step\u001b[38;5;241m=\u001b[39mstep, ignore_copy_err\u001b[38;5;241m=\u001b[39mignore_copy_err\n\u001b[1;32m     50\u001b[0m         )\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         payload[key] \u001b[38;5;241m=\u001b[39m \u001b[43mval_to_json\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnamespace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_copy_err\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_copy_err\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m payload\n",
      "File \u001b[0;32m~/.conda/envs/emerge/lib/python3.8/site-packages/wandb/sdk/data_types/utils.py:86\u001b[0m, in \u001b[0;36mval_to_json\u001b[0;34m(run, key, val, namespace, ignore_copy_err)\u001b[0m\n\u001b[1;32m     83\u001b[0m     val \u001b[38;5;241m=\u001b[39m wandb\u001b[38;5;241m.\u001b[39mTable(dataframe\u001b[38;5;241m=\u001b[39mval)\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m util\u001b[38;5;241m.\u001b[39mis_matplotlib_typename(typename) \u001b[38;5;129;01mor\u001b[39;00m util\u001b[38;5;241m.\u001b[39mis_plotly_typename(typename):\n\u001b[0;32m---> 86\u001b[0m     val \u001b[38;5;241m=\u001b[39m \u001b[43mPlotly\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_plot_media\u001b[49m\u001b[43m(\u001b[49m\u001b[43mval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(val, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mrange\u001b[39m)) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;28misinstance\u001b[39m(v, WBValue) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m val\n\u001b[1;32m     89\u001b[0m ):\n\u001b[1;32m     90\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m run\n",
      "File \u001b[0;32m~/.conda/envs/emerge/lib/python3.8/site-packages/wandb/sdk/data_types/plotly.py:48\u001b[0m, in \u001b[0;36mPlotly.make_plot_media\u001b[0;34m(cls, val)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m util\u001b[38;5;241m.\u001b[39mis_matplotlib_typename(util\u001b[38;5;241m.\u001b[39mget_full_typename(val)):\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m util\u001b[38;5;241m.\u001b[39mmatplotlib_contains_images(val):\n\u001b[0;32m---> 48\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mImage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m     val \u001b[38;5;241m=\u001b[39m util\u001b[38;5;241m.\u001b[39mmatplotlib_to_plotly(val)\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(val)\n",
      "File \u001b[0;32m~/.conda/envs/emerge/lib/python3.8/site-packages/wandb/sdk/data_types/image.py:161\u001b[0m, in \u001b[0;36mImage.__init__\u001b[0;34m(self, data_or_path, mode, caption, grouping, classes, boxes, masks)\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initialize_from_path(data_or_path)\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 161\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_initialize_from_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_initialization_meta(grouping, caption, classes, boxes, masks)\n",
      "File \u001b[0;32m~/.conda/envs/emerge/lib/python3.8/site-packages/wandb/sdk/data_types/image.py:308\u001b[0m, in \u001b[0;36mImage._initialize_from_data\u001b[0;34m(self, data, mode)\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpng\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    307\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_image \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 308\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_image\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtmp_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransparency\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_file(tmp_path, is_tmp\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/.conda/envs/emerge/lib/python3.8/site-packages/PIL/Image.py:2438\u001b[0m, in \u001b[0;36mImage.save\u001b[0;34m(self, fp, format, **params)\u001b[0m\n\u001b[1;32m   2435\u001b[0m         fp \u001b[38;5;241m=\u001b[39m builtins\u001b[38;5;241m.\u001b[39mopen(filename, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw+b\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2437\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2438\u001b[0m     \u001b[43msave_handler\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2439\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   2440\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m open_fp:\n",
      "File \u001b[0;32m~/.conda/envs/emerge/lib/python3.8/site-packages/PIL/PngImagePlugin.py:1394\u001b[0m, in \u001b[0;36m_save\u001b[0;34m(im, fp, filename, chunk, save_all)\u001b[0m\n\u001b[1;32m   1392\u001b[0m     _write_multiple_frames(im, fp, chunk, rawmode, default_image, append_images)\n\u001b[1;32m   1393\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1394\u001b[0m     \u001b[43mImageFile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_idat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mzip\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrawmode\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1396\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m info:\n\u001b[1;32m   1397\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m info_chunk \u001b[38;5;129;01min\u001b[39;00m info\u001b[38;5;241m.\u001b[39mchunks:\n",
      "File \u001b[0;32m~/.conda/envs/emerge/lib/python3.8/site-packages/PIL/ImageFile.py:519\u001b[0m, in \u001b[0;36m_save\u001b[0;34m(im, fp, tile, bufsize)\u001b[0m\n\u001b[1;32m    517\u001b[0m     _encode_tile(im, fp, tile, bufsize, fh)\n\u001b[1;32m    518\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mAttributeError\u001b[39;00m, io\u001b[38;5;241m.\u001b[39mUnsupportedOperation) \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m--> 519\u001b[0m     \u001b[43m_encode_tile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbufsize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    520\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(fp, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mflush\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    521\u001b[0m     fp\u001b[38;5;241m.\u001b[39mflush()\n",
      "File \u001b[0;32m~/.conda/envs/emerge/lib/python3.8/site-packages/PIL/ImageFile.py:538\u001b[0m, in \u001b[0;36m_encode_tile\u001b[0;34m(im, fp, tile, bufsize, fh, exc)\u001b[0m\n\u001b[1;32m    535\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m exc:\n\u001b[1;32m    536\u001b[0m     \u001b[38;5;66;03m# compress to Python file-compatible object\u001b[39;00m\n\u001b[1;32m    537\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 538\u001b[0m         errcode, data \u001b[38;5;241m=\u001b[39m \u001b[43mencoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbufsize\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m:]\n\u001b[1;32m    539\u001b[0m         fp\u001b[38;5;241m.\u001b[39mwrite(data)\n\u001b[1;32m    540\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m errcode:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAxAAAALJCAYAAAAkv8WuAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8WgzjOAAAACXBIWXMAAA9hAAAPYQGoP6dpAACOTklEQVR4nOzdfXyT1f3/8XfaQlqhDRZbSqVQbpRbQUREwGGBWloYgvIVb5gCKkzXoog6qZsKc67qnKKMwdwPAaeIohYRtazjVpTbYhUUi9xJBUpFJIUWQmmu3x+MYGwLSXqFxPB6Ph7nseW6rnzOJ20S+fSccx2LYRiGAAAAAMADYYFOAAAAAMAvBwUEAAAAAI9RQAAAAADwGAUEAAAAAI9RQAAAAADwGAUEAAAAAI9RQAAAAADwGAUEAAAAAI9FBDoBAAAAwEzHjh3T8ePHA52GJKl+/fqKjIwMdBqmooAAAABAyDh27JhatmioktKqQKciSUpISNDOnTtDqoiggAAAAEDIOH78uEpKq/RtQbJiogM7W7/ssFMtuu3S8ePHKSAAAACAYBYTHaaY6PBApxGSKCAAAAAQcpwy5JQz4DmEIu7CBAAAAMBjjEAAAAAg5FQZTlUFeACgygjsCIi/MAIBAAAAwGMUEAAAAAA8xhQmAAAAhJyTi6gDO4cp0P37CyMQAAAAADxGAQEAAADAY0xhAgAAQMhxBnwXCAVBBv7BCAQAAAAAj1FAAAAAAPAYBQSAX4xRo0YpOTk50GnAR8nJyRo1alSg0wBwnqgyjKBooYgCAoCb2bNny2KxaMOGDTWeT0lJUadOnc5xVt7bsmWL0tPT1bBhQ8XGxur222/X999/H+i0/Gbv3r2aNGmSCgsLq52bO3eupkyZck7y+PTTTzVp0iQdOnTonPQHADj3WEQNIOR899136tOnj2w2m/7yl7/oyJEjeu6557Rp0yatW7dO9evXD3SKptu7d68mT56s5ORkXX755W7n5s6dq82bN2v8+PF+z+PTTz/V5MmTNWrUKDVq1MjtXFFRkcLC+LsVgHODfSD8hwICQMj5y1/+ovLychUUFKh58+aSpKuuukrXXXedZs+erbFjxwY4w/OT1WoNdAoAABPwpyAApnjttdfUrVs3RUVFKTY2VrfccouKi4vdrvn444910003qXnz5rJarUpKStIDDzygo0ePVou3YMECderUSZGRkerUqZNyc3M9zuWdd97Rr3/9a1fxIEmpqam69NJL9dZbb531+c8995x69eqlxo0bKyoqSt26ddPbb7/tdk2nTp3Ut2/fas91Op26+OKL9X//93+uYz/88INuv/12xcTEqFGjRho5cqQ+//xzWSwWzZ49+4y5HDx4UA899JAuu+wyNWzYUDExMcrIyNDnn3/uumb58uXq3r27JGn06NGyWCyu2CkpKfrggw/07bffuo7/dB2Jw+HQE088oTZt2rh+J7///e/lcDjc8rBYLMrKynL9XqxWqzp27Ki8vDzXNZMmTdLDDz8sSWrZsqWrv127dkmqeQ3Ejh07dNNNNyk2NlYXXHCBrr76an3wwQdu1yxfvlwWi0VvvfWWnnrqKTVr1kyRkZHq37+/tm3bdsafHwDAfIxAAKiR3W7XgQMHqh2vrKysduypp57SY489puHDh+vuu+/W999/r6lTp6pPnz767LPPXFNZ5s+fr4qKCt17771q3Lix1q1bp6lTp+q7777T/PnzXfH+85//aNiwYerQoYNycnL0ww8/aPTo0WrWrNlZ896zZ49KS0t15ZVXVjt31VVX6cMPPzxrjBdffFHXX3+9RowYoePHj2vevHm66aabtGjRIg0aNEiSdPPNN2vSpEkqKSlRQkKC67mrVq3S3r17dcstt0g6WVAMHjxY69at07333qt27drpvffe08iRI8+ah3TyH9gLFizQTTfdpJYtW2r//v365z//qWuvvVZfffWVEhMT1b59e/3pT3/S448/rrFjx+pXv/qVJKlXr166+OKLZbfb9d133+mFF16QJDVs2NCV2/XXX69Vq1Zp7Nixat++vTZt2qQXXnhBW7du1YIFC9xyWbVqld5991397ne/U3R0tF566SUNGzZMu3fvVuPGjXXjjTdq69ateuONN/TCCy/ooosukiTFxcXV+Nr279+vXr16qaKiQvfdd58aN26sOXPm6Prrr9fbb7+tG264we36p59+WmFhYXrooYdkt9v17LPPasSIEVq7dq1HP0sA5xenDFUxhck/DAD4iVmzZhmSztg6duzoun7Xrl1GeHi48dRTT7nF2bRpkxEREeF2vKKiolp/OTk5hsViMb799lvXscsvv9xo2rSpcejQIdex//znP4Yko0WLFmfMf/369YYk49VXX6127uGHHzYkGceOHTtjjJ/nefz4caNTp05Gv379XMeKiooMScbUqVPdrv3d735nNGzY0BXjnXfeMSQZU6ZMcV1TVVVl9OvXz5BkzJo164y5HDt2zKiqqnI7tnPnTsNqtRp/+tOfqr3umuINGjSoxp/bv//9byMsLMz4+OOP3Y7PmDHDkGR88sknrmOSjPr16xvbtm1zHfv888+r/Qz++te/GpKMnTt3VuuvRYsWxsiRI12Px48fb0hy6//w4cNGy5YtjeTkZNfrXrZsmSHJaN++veFwOFzXvvjii4YkY9OmTdX6AnD+stvtJ7+Hvm5qHNhzcUDbzq+bGpIMu90e6B+LqZjCBKBG06ZNU35+frXWuXNnt+veffddOZ1ODR8+XAcOHHC1hIQEXXLJJVq2bJnr2qioKNf/Ly8v14EDB9SrVy8ZhqHPPvtMkrRv3z4VFhZq5MiRstlsruuvu+46dejQ4ax5n5oOVdN8+8jISLdravPTPH/88UfZ7Xb96le/0saNG13HL730Ul1++eV68803Xceqqqr09ttva/Dgwa4YeXl5qlevnsaMGeO6LiwsTJmZmWd9Ladex6mFx1VVVfrhhx/UsGFDtW3b1i0fX8yfP1/t27dXu3bt3H53/fr1kyS33510chpY69atXY87d+6smJgY7dixw6f+P/zwQ1111VW65pprXMcaNmyosWPHateuXfrqq6/crh89erTbAvhTIy2+9g8A8A1TmADU6KqrrqpxGtCFF17oNrXpm2++kWEYuuSSS2qMU69ePdf/3717tx5//HEtXLhQP/74o9t1drtdkvTtt99KUo3xPPlH86l/uP98Dr8kHTt2zO2a2ixatEh//vOfVVhY6BbHYrG4XXfzzTfr0Ucf1Z49e3TxxRdr+fLlKi0t1c033+y65ttvv1XTpk11wQUXuD23TZs2Z8zhFKfTqRdffFH/+Mc/tHPnTlVVVbnONW7c2KMYtfnmm2+0ZcuWWqcYlZaWuj3+6ZqSUy688MJqv0tPffvtt+rRo0e14+3bt3ed/+ktg3/e/4UXXihJPvcPILRxFyb/oYAAUCdOp1MWi0UfffSRwsPDq50/Nd++qqpK1113nQ4ePKhHHnlE7dq1U4MGDbRnzx6NGjVKTqfTlHyaNm0q6eRIxs/t27dPsbGxZ7wb0Mcff6zrr79effr00T/+8Q81bdpU9erV06xZszR37ly3a2+++WZlZ2dr/vz5Gj9+vN566y3ZbDalp6eb8lqkk3eUeuyxx3TnnXfqySefVGxsrMLCwjR+/Pg6/8ycTqcuu+wyPf/88zWeT0pKcntc0+9XkoxztFFSoPsHAJxEAQGgTlq3bi3DMNSyZUtdeumltV63adMmbd26VXPmzNEdd9zhOp6fn+92XYsWLSSd/Ov4zxUVFZ01n4svvlhxcXE1boS3bt26ansk/Nw777yjyMhILV682K3QmDVrVrVrW7ZsqauuukpvvvmmsrKy9O6772ro0KFuz2vRooWWLVumiooKt1EIT+8e9Pbbb6tv376aOXOm2/FDhw65FilL1UdHfqq2c61bt9bnn3+u/v37n/H53vAmTosWLWr8nX799deu8wDgq2DYCTrQ/fsLayAA1MmNN96o8PBwTZ48udpfgg3D0A8//CDp9F+Pf3qNYRh68cUX3Z7TtGlTXX755ZozZ45rWpN0stD4+Zz42gwbNkyLFi1yu43skiVLtHXrVt10001nfG54eLgsFovbVKFdu3ZVuyPRKTfffLPWrFmjV155RQcOHHCbviRJAwYMUGVlpf71r3+5jjmdTk2bNs2j1xIeHl7t5zp//nzt2bPH7ViDBg0kqcYdoBs0aOD2szxl+PDh2rNnj1tupxw9elTl5eUe5ehpHj83cOBArVu3TqtXr3YdKy8v18svv6zk5GSP1rwAAM49RiAA1Enr1q315z//WdnZ2dq1a5eGDh2q6Oho7dy5U7m5uRo7dqweeughtWvXTq1bt9ZDDz2kPXv2KCYmRu+8806N89dzcnI0aNAgXXPNNbrzzjt18OBBTZ06VR07dtSRI0fOmtOjjz6q+fPnq2/fvrr//vt15MgR/fWvf9Vll12m0aNHn/G5gwYN0vPPP6/09HTddtttKi0t1bRp09SmTRt98cUX1a4fPny4HnroIT300EOKjY1Vamqq2/mhQ4fqqquu0oMPPqht27apXbt2WrhwoQ4ePCjp7H+x//Wvf60//elPGj16tHr16qVNmzbp9ddfV6tWrdyua926tRo1aqQZM2YoOjpaDRo0UI8ePdSyZUt169ZNb775piZMmKDu3burYcOGGjx4sG6//Xa99dZbuueee7Rs2TL17t1bVVVV+vrrr/XWW29p8eLFNa6DOZNu3bpJkv7whz/olltuUb169TR48GBXYfFTEydO1BtvvKGMjAzdd999io2N1Zw5c7Rz506988477FoNAEGKb2cAdTZx4kTXP/gmT56shx56SAsXLlRaWpquv/56SScXU7///vu6/PLLlZOTo8mTJ+uSSy7Rq6++Wi1eenq65s+fr6qqKmVnZ+vdd9/VrFmzPP7HbFJSklasWKHWrVtr4sSJevbZZzVw4EDl5+efdTfkfv36aebMmSopKdH48eP1xhtv6Jlnnqm2J8EpzZo1U69evXT48GHdeOONbovGpZMjCB988IFuvvlmzZkzR3/4wx+UmJjoGoE4dWeo2jz66KN68MEHtXjxYt1///3auHGjPvjgg2rrE+rVq6c5c+YoPDxc99xzj2699VatWLFCkvS73/1Ot912m2bNmqXbbrtN48aNk3TyblALFizQ008/rU2bNumhhx7S5MmTtX79et1///1nnJJWm+7du+vJJ5/U559/rlGjRunWW2/V999/X+O1TZo00aeffqrrrrtOU6dOVXZ2turXr6/333+/1p83AHjKGSQtFFkMVp8BwDm3YMEC3XDDDVq1apV69+4d6HQAIGSUlZXJZrPp6y1NFB0d2L+VHz7sVLv2+2W32xUTExPQXMzECAQA+NnP952oqqrS1KlTFRMToyuuuCJAWQEA4BvWQACAn40bN05Hjx5Vz5495XA49O677+rTTz/VX/7yl7PuSQEA8E2VDFUFeB+GQPfvLxQQAOBn/fr109/+9jctWrRIx44dU5s2bTR16lRlZWUFOjUAALzGGggAAACEjFNrIL74Kj4o1kB07lDKGggAAAAA5y8KCAAAAAAeYw0EAAAAQk4w7MMQ6P79JegKCKfTqb179yo6OvqsO7QCAADg3DMMQ4cPH1ZiYiK7xp+Hgq6A2Lt3b7UdVgEAABB8iouL1axZs0CngXMs6AqI6OhoSVLy9AcVFmWtc7y+ydvqHAMAAACnHS+v1L8H5br+3RaMnLKoSoGdzeIMcP/+EnQFxKlpS2FRVoVdEFnnePUb1qtzDAAAAFTHdPPzE5PWAAAAAHgs6EYgAAAAgLpyGidboHMIRX4bgZg2bZqSk5MVGRmpHj16aN26df7qCgAAAMA54pcC4s0339SECRP0xBNPaOPGjerSpYsGDBig0tJSf3QHAAAAuKn63yLqQLdQ5JcC4vnnn9eYMWM0evRodejQQTNmzNAFF1ygV155xR/dAQAAADhHTC8gjh8/roKCAqWmpp7uJCxMqampWr16dbXrHQ6HysrK3BoAAACA4GR6AXHgwAFVVVWpSZMmbsebNGmikpKSatfn5OTIZrO5GpvIAQAAoK4CPXWJKUx+lJ2dLbvd7mrFxcWBTgkAAABALUy/jetFF12k8PBw7d+/3+34/v37lZCQUO16q9Uqq7XuO04DAAAA8D/TRyDq16+vbt26acmSJa5jTqdTS5YsUc+ePc3uDgAAAKjGaViCooUiv2wkN2HCBI0cOVJXXnmlrrrqKk2ZMkXl5eUaPXq0P7oDAAAAcI74pYC4+eab9f333+vxxx9XSUmJLr/8cuXl5VVbWA0AAAD4QzAsYg50//7ilwJCkrKyspSVleWv8AAAAAACIOB3YQIAAADwy+G3EQgAAAAgUKoUpqoA/628KqC9+0/QFhCD2nwpa8N6dY5zwmnuG8fhrHtOAAAAwC8VU5gAAAAAeCxoRyAAAAAAXxlBsA+DEaL7QDACAQAAAMBjFBAAAAAAPMYUJgAAAIQcNpLzH9NHIFauXKnBgwcrMTFRFotFCxYsMLsLAAAAAAFiegFRXl6uLl26aNq0aWaHBgAAADxSZYQFRQtFpk9hysjIUEZGhtlhAQAAAASB0CyLAAAAAPhFwBdROxwOORwO1+OysrIAZgMAAIBQ4JRFzgD/rdwpI6D9+0vARyBycnJks9lcLSkpKdApAQAAAKhFwAuI7Oxs2e12VysuLg50SgAAAABqEfApTFarVVarNdBpAAAAIISwD4T/mF5AHDlyRNu2bXM93rlzpwoLCxUbG6vmzZub3R0AAACAc8j0AmLDhg3q27ev6/GECRMkSSNHjtTs2bPN7g4AAACoJhj2YagyQnMRtekFREpKiowQ/WEBAAAA57uAL6IGAAAA8MtBAQEAAICQc3IfiMA3b02bNk3JycmKjIxUjx49tG7dulqv/fLLLzVs2DAlJyfLYrFoypQpdfiJeY4CAgAAAAgCb775piZMmKAnnnhCGzduVJcuXTRgwACVlpbWeH1FRYVatWqlp59+WgkJCecsz4DfxrU2nx1spghH3W/vumNnExOyOe3CJubtlN2r6S7TYgEAAOCX7fnnn9eYMWM0evRoSdKMGTP0wQcf6JVXXtHEiROrXd+9e3d1795dkmo87y9BW0AAAAAAvnIqTFUBnmzj1MkbC5WVuf8BuqZ90I4fP66CggJlZ2e7joWFhSk1NVWrV6/2f7JeYAoTAAAA4EdJSUmy2WyulpOTU+2aAwcOqKqqSk2auM+eadKkiUpKSs5Vqh5hBAIAAADwo+LiYsXExLge/3z04ZeGAgIAAAAhJ5g2kouJiXErIGpy0UUXKTw8XPv373c7vn///nO6QNoTpv9Uc3Jy1L17d0VHRys+Pl5Dhw5VUVGR2d0AAAAAIaN+/frq1q2blixZ4jrmdDq1ZMkS9ezZM4CZVWd6AbFixQplZmZqzZo1ys/PV2VlpdLS0lReXm52VwAAAECNnAoLiuaNCRMm6F//+pfmzJmjLVu26N5771V5ebnrrkx33HGH2yLr48ePq7CwUIWFhTp+/Lj27NmjwsJCbdu2zdSf5c+ZPoUpLy/P7fHs2bMVHx+vgoIC9enTx+zuAAAAgJBw88036/vvv9fjjz+ukpISXX755crLy3MtrN69e7fCwk4XJXv37lXXrl1dj5977jk999xzuvbaa7V8+XK/5en3NRB2u12SFBsb6++uAAAAgF+0rKwsZWVl1Xju50VBcnKyjP+tsziX/FpAOJ1OjR8/Xr1791anTp1qvMbhcMjhcLge//w+uQAAAIC3qgyLqgxLwHMIRX5dmp6ZmanNmzdr3rx5tV6Tk5Pjdl/cpKQkf6YEAAAAoA78VkBkZWVp0aJFWrZsmZo1a1brddnZ2bLb7a5WXFzsr5QAAAAA1JHpU5gMw9C4ceOUm5ur5cuXq2XLlme8vqatvAEAAIC6qFKYqvw72caDHM79+oRzwfQCIjMzU3PnztV7772n6Oho19bbNptNUVFRZncHAAAA4BwyvYCYPn26JCklJcXt+KxZszRq1CizuwMAAACqcRphcgZ4J2pnAO6QdC74ZQoTAAAAgNAU2LIMAAAAwC+K3zeSAwAAAM41FlH7DyMQAAAAADwWtCMQFcfrK7xe/TrHybh8kwnZnNa5gXn7VOR9X/Pu3L5qdsEhU+MBAAAAPxe0BQQAAADgK6ekKsMS8BxCEVOYAAAAAHiMEQgAAACEHKfC5Azw38oD3b+/hOarAgAAAOAXFBAAAAAAPGZ6ATF9+nR17txZMTExiomJUc+ePfXRRx+Z3Q0AAABQqyojLChaKDL9VTVr1kxPP/20CgoKtGHDBvXr109DhgzRl19+aXZXAAAAAM4x0xdRDx482O3xU089penTp2vNmjXq2LGj2d0BAAAAOIf8ehemqqoqzZ8/X+Xl5erZs2eN1zgcDjkcDtfjsrIyf6YEAACA84BTFjkV6H0gAtu/v/hlYtamTZvUsGFDWa1W3XPPPcrNzVWHDh1qvDYnJ0c2m83VkpKS/JESAAAAABP4pYBo27atCgsLtXbtWt17770aOXKkvvrqqxqvzc7Olt1ud7Xi4mJ/pAQAAADABH6ZwlS/fn21adNGktStWzetX79eL774ov75z39Wu9ZqtcpqtfojDQAAAJynguEuSIHu31/OyatyOp1u6xwAAAAA/DKZPgKRnZ2tjIwMNW/eXIcPH9bcuXO1fPlyLV682OyuAAAAgBpVKUxVAd4zOdD9+4vpBURpaanuuOMO7du3TzabTZ07d9bixYt13XXXmd0VAAAAgHPM9AJi5syZZocEAAAAECT8ug8EAAAAEAhOwyKnEeB9IALcv7+E5sQsAAAAAH4RtCMQ3eN3q37DenWO0y+m5v0nfOU0seZ6qsUC02JJ0idHW5sW64sjbOgHAACA6oK2gAAAAAB85QyCuzCZ+YfnYBKarwoAAACAXzACAQAAgJDjNMLkDPBO0IHu319C81UBAAAA8AsKCAAAAAAe83sB8fTTT8tisWj8+PH+7goAAACQJFXJEhQtFPm1gFi/fr3++c9/qnPnzv7sBgAAAMA54rcC4siRIxoxYoT+9a9/6cILL/RXNwAAAADOIb8VEJmZmRo0aJBSU1P91QUAAABQo1N3YQp0C0V+uY3rvHnztHHjRq1fv/6s1zocDjkcDtfjsrIyf6QEAAAAwASml0XFxcW6//779frrrysyMvKs1+fk5Mhms7laUlKS2SkBAAAAMInpBURBQYFKS0t1xRVXKCIiQhEREVqxYoVeeuklRUREqKqqyu367Oxs2e12VysuLjY7JQAAAJxnqhQMd2IKTaZPYerfv782bdrkdmz06NFq166dHnnkEYWHh7uds1qtslqtZqcBAAAAwA9MLyCio6PVqVMnt2MNGjRQ48aNqx0HAAAA/CEYFjEHun9/Cc1XBQAAAMAv/HIXpp9bvnz5uegGAAAAgJ+dkwICAAAAOJeqjDBVBXgKUaD795fQfFUAAAAA/IICAgAAAIDHgnYKU5icCpezznEe3vh/JmRzWtMLzdsp+6ZmBabFkqTF33c0LVaZ4+ybAHqq04X7TIsFAADgCUMWOWUJeA6hiBEIAAAAAB4L2hEIAAAAwFcsovaf0HxVAAAAAPyCAgIAAACAx5jCBAAAgJDjNCxyGoFdxBzo/v3F9BGISZMmyWKxuLV27dqZ3Q0AAACAAPDLCETHjh313//+93QnEQx0AAAAAKHAL/+yj4iIUEJCgj9CAwAAAGdVpTBVBXi5b6D79xe/vKpvvvlGiYmJatWqlUaMGKHdu3fXeq3D4VBZWZlbAwAAABCcTC8gevToodmzZysvL0/Tp0/Xzp079atf/UqHDx+u8fqcnBzZbDZXS0pKMjslAAAAnGdOLaIOdAtFphcQGRkZuummm9S5c2cNGDBAH374oQ4dOqS33nqrxuuzs7Nlt9tdrbi42OyUAAAAAJjE76ubGzVqpEsvvVTbtm2r8bzVapXVavV3GgAAAABM4PeVHUeOHNH27dvVtGlTf3cFAAAASJKcCguKFopMf1UPPfSQVqxYoV27dunTTz/VDTfcoPDwcN16661mdwUAAADgHDN9CtN3332nW2+9VT/88IPi4uJ0zTXXaM2aNYqLizO7KwAAAADnmOkFxLx588wOCQAAAHilyrCoKsB3QQp0//4SmhOzAAAAAPgFBQQAAAAAj/n9Nq6+2no4XhHOut/etX3CfhOyOe2CiOOmxdrjuNC0WJIUGV5pWqzjEeGmxVq2u41psSSpb/OabwkMAABwSjBs5Bbo/v2FEQgAAAAAHgvaEQgAAADAV4YRJqcR2L+VGwHu319C81UBAAAAv0DTpk1TcnKyIiMj1aNHD61bt+6M18+fP1/t2rVTZGSkLrvsMn344Yd+z5ECAgAAAAgCb775piZMmKAnnnhCGzduVJcuXTRgwACVlpbWeP2nn36qW2+9VXfddZc+++wzDR06VEOHDtXmzZv9micFBAAAAEJOlSxB0bzx/PPPa8yYMRo9erQ6dOigGTNm6IILLtArr7xS4/Uvvvii0tPT9fDDD6t9+/Z68skndcUVV+jvf/+7GT/CWvmlgNizZ49+85vfqHHjxoqKitJll12mDRs2+KMrAAAAIKiVlZW5NYfDUe2a48ePq6CgQKmpqa5jYWFhSk1N1erVq2uMu3r1arfrJWnAgAG1Xm8W0wuIH3/8Ub1791a9evX00Ucf6auvvtLf/vY3XXihubcsBQAAAH4JkpKSZLPZXC0nJ6faNQcOHFBVVZWaNGnidrxJkyYqKSmpMW5JSYlX15vF9LswPfPMM0pKStKsWbNcx1q2bGl2NwAAAECtnEbg92FwGif/t7i4WDExMa7jVmvd9zoLJNNHIBYuXKgrr7xSN910k+Lj49W1a1f961//qvV6h8NRbVgHAAAACBUxMTFuraYC4qKLLlJ4eLj273ffBHn//v1KSEioMW5CQoJX15vF9AJix44dmj59ui655BItXrxY9957r+677z7NmTOnxutzcnLchnSSkpLMTgkAAADnGef/9oEIdPNU/fr11a1bNy1ZsuT0a3A6tWTJEvXs2bPG5/Ts2dPteknKz8+v9XqzmD6Fyel06sorr9Rf/vIXSVLXrl21efNmzZgxQyNHjqx2fXZ2tiZMmOB6XFZWRhEBAACA886ECRM0cuRIXXnllbrqqqs0ZcoUlZeXa/To0ZKkO+64QxdffLFrDcX999+va6+9Vn/72980aNAgzZs3Txs2bNDLL7/s1zxNLyCaNm2qDh06uB1r37693nnnnRqvt1qtv/h5YAAAAEBd3Xzzzfr+++/1+OOPq6SkRJdffrny8vJcC6V3796tsLDToxq9evXS3Llz9cc//lGPPvqoLrnkEi1YsECdOnXya56mFxC9e/dWUVGR27GtW7eqRYsWZncFAAAA1Mgpi5xe7sPgjxy8lZWVpaysrBrPLV++vNqxm266STfddJPX/dSF6WsgHnjgAa1Zs0Z/+ctftG3bNs2dO1cvv/yyMjMzze4KAAAAwDlmegHRvXt35ebm6o033lCnTp305JNPasqUKRoxYoTZXQEAAAA4x0yfwiRJv/71r/XrX//aH6EBAACAs6oyLKoK8D4Qge7fX0wfgQAAAAAQuiggAAAAAHjML1OYAAAAgEDydiM3f+UQioK2gNhTZlP4ibrvD1FVZe4vLuaCY6bF2lBs7oZ56W22mBartCLatFh9m28zLZYkdWrwnWmxNpc3My0WAADA+SBoCwgAAADAV05Z5AzwIuZA70PhL6E5rgIAAADALyggAAAAAHiMKUwAAAAIOYYsAZ9CZDCFyTPJycmyWCzVWmZmptldAQAAADjHTB+BWL9+vaqqqlyPN2/erOuuu0433XST2V0BAAAAOMdMLyDi4uLcHj/99NNq3bq1rr32WrO7AgAAAGrkNILgLkwB7t9f/LqI+vjx43rttdd05513ymIJzR8gAAAAcD7x6yLqBQsW6NChQxo1alSt1zgcDjkcDtfjsrIyf6YEAACA8wA7UfuPX1/VzJkzlZGRocTExFqvycnJkc1mc7WkJHN3ZwYAAABgHr8VEN9++63++9//6u677z7jddnZ2bLb7a5WXFzsr5QAAAAA1JHfpjDNmjVL8fHxGjRo0Bmvs1qtslqt/koDAAAA5yEWUfuPX0YgnE6nZs2apZEjRyoigr3qAAAAgFDhlwLiv//9r3bv3q0777zTH+EBAAAABIhfhgfS0tJkGIY/QgMAAABn5ZRFTgV4ClOA+/eX0Ly3FAAAAAC/YIECAAAAQg6LqP2HEQgAAAAAHgvaEYjmjQ6pXoP6dY7T2FpuQjanVTrDTYt1Y1KhabEkKS7isGmxBjTaZFqsl75NNS2WJFWauKtjw3DH2S/ygsNZz9R4AAAAwSZoCwgAAADAV0xh8h+mMAEAAADwGAUEAAAAAI8xhQkAAAAhhylM/sMIBAAAAACPmV5AVFVV6bHHHlPLli0VFRWl1q1b68knn2RnagAAACAEmD6F6ZlnntH06dM1Z84cdezYURs2bNDo0aNls9l03333md0dAAAAUA1TmPzH9ALi008/1ZAhQzRo0CBJUnJyst544w2tW7fO7K4AAAAAnGOmT2Hq1auXlixZoq1bt0qSPv/8c61atUoZGRk1Xu9wOFRWVubWAAAAgLowJDllCWgL1Qn8po9ATJw4UWVlZWrXrp3Cw8NVVVWlp556SiNGjKjx+pycHE2ePNnsNAAAAAD4gekjEG+99ZZef/11zZ07Vxs3btScOXP03HPPac6cOTVen52dLbvd7mrFxcVmpwQAAADAJKaPQDz88MOaOHGibrnlFknSZZddpm+//VY5OTkaOXJkteutVqusVqvZaQAAAOA8xiJq/zF9BKKiokJhYe5hw8PD5XQ6ze4KAAAAwDlm+gjE4MGD9dRTT6l58+bq2LGjPvvsMz3//PO68847ze4KAAAAwDlmegExdepUPfbYY/rd736n0tJSJSYm6re//a0ef/xxs7sCAAAAasQUJv8xvYCIjo7WlClTNGXKFLNDAwAAAAgw0wsIAAAAINAYgfAf0xdRAwAAAAhdQTsCse9wtMKr6n5715KwaBOyOS0y4oRpsdbuSjYtliT1bLnDtFhx9Y+YFuvy2O9MiyVJsRHlpsXad9xmWixJejxhqWmx/lTSz7RYAAAAZgnaAgIAAADwFVOY/IcpTAAAAAA8RgEBAAAAwGNMYQIAAEDIMQyLjABPIQp0//7CCAQAAAAAj/mlgDh8+LDGjx+vFi1aKCoqSr169dL69ev90RUAAACAc8gvU5juvvtubd68Wf/+97+VmJio1157Tampqfrqq6908cUX+6NLAAAAwMUpi5wK8F2YAty/v5g+AnH06FG98847evbZZ9WnTx+1adNGkyZNUps2bTR9+nSzuwMAAABwDpk+AnHixAlVVVUpMjLS7XhUVJRWrVpldncAAABANewD4T+mj0BER0erZ8+eevLJJ7V3715VVVXptdde0+rVq7Vv375q1zscDpWVlbk1AAAAAMHJL4uo//3vf8swDF188cWyWq166aWXdOuttyosrHp3OTk5stlsrpaUlOSPlAAAAACYwC8FROvWrbVixQodOXJExcXFWrdunSorK9WqVatq12ZnZ8tut7tacXGxP1ICAADAeeTUPhCBbqHIrxvJNWjQQA0aNNCPP/6oxYsX69lnn612jdVqldVq9WcaAAAAAEzilwJi8eLFMgxDbdu21bZt2/Twww+rXbt2Gj16tD+6AwAAAHCO+KWAsNvtys7O1nfffafY2FgNGzZMTz31lOrVq+eP7gAAAAA33IXJf/xSQAwfPlzDhw/3R2gAAAAAAeTXNRAAAABAIATDIuZA9+8vfrkLEwAAAIDQRAEBAAAAwGNBO4Xp10mbFdmw7ouuD1Q2NCGb0w4ev8C0WEcrzV1Uvq/CZlqsRvWOmhZrWfElpsWSpOMnzHvb9rx4l2mxJOnNsg6mxXI4zXud1rATpsUCAOCXwAiCRdRMYQIAAABw3qOAAAAAAOCxoJ3CBAAAAPjKkGQYgc8hFDECAQAAAMBjXhcQK1eu1ODBg5WYmCiLxaIFCxa4nTcMQ48//riaNm2qqKgopaam6ptvvjErXwAAAAAB5HUBUV5eri5dumjatGk1nn/22Wf10ksvacaMGVq7dq0aNGigAQMG6NixY3VOFgAAAPCEU5agaP5y8OBBjRgxQjExMWrUqJHuuusuHTly5IzPefnll5WSkqKYmBhZLBYdOnTIp769LiAyMjL05z//WTfccEO1c4ZhaMqUKfrjH/+oIUOGqHPnznr11Ve1d+/eaiMVAAAAAHwzYsQIffnll8rPz9eiRYu0cuVKjR079ozPqaioUHp6uh599NE69W3qIuqdO3eqpKREqamprmM2m009evTQ6tWrdcstt5jZHQAAAFAjw7AEfB8Gf/W/ZcsW5eXlaf369bryyislSVOnTtXAgQP13HPPKTExscbnjR8/XpK0fPnyOvVv6iLqkpISSVKTJk3cjjdp0sR17uccDofKysrcGgAAAICarV69Wo0aNXIVD5KUmpqqsLAwrV271u/9B/wuTDk5ObLZbK6WlJQU6JQAAAAA0/z8j+UOh6NO8UpKShQfH+92LCIiQrGxsbX+0d5MphYQCQkJkqT9+/e7Hd+/f7/r3M9lZ2fLbre7WnFxsZkpAQAA4DzkNCxB0SQpKSnJ7Q/mOTk5NeY8ceJEWSyWM7avv/76XP4Ya2TqGoiWLVsqISFBS5Ys0eWXXy7pZMW1du1a3XvvvTU+x2q1ymq1mpkGAAAAEDSKi4sVExPjelzbv30ffPBBjRo16oyxWrVqpYSEBJWWlrodP3HihA4ePFjrH+3N5HUBceTIEW3bts31eOfOnSosLFRsbKyaN2+u8ePH689//rMuueQStWzZUo899pgSExM1dOhQM/MGAAAAfhFiYmLcCojaxMXFKS4u7qzX9ezZU4cOHVJBQYG6desmSVq6dKmcTqd69OhR53zPxusCYsOGDerbt6/r8YQJEyRJI0eO1OzZs/X73/9e5eXlGjt2rA4dOqRrrrlGeXl5ioyMNC9rAAAA4AwM42QLdA7+0L59e6Wnp2vMmDGaMWOGKisrlZWVpVtuucV1B6Y9e/aof//+evXVV3XVVVdJOrl2oqSkxDUYsGnTJkVHR6t58+aKjY31uH+vC4iUlBQZZ/hpWCwW/elPf9Kf/vQnb0MDAAAA8MDrr7+urKws9e/fX2FhYRo2bJheeukl1/nKykoVFRWpoqLCdWzGjBmaPHmy63GfPn0kSbNmzTrr1KmfMnUNBAAAABAMQnkfCEmKjY3V3Llzaz2fnJxc7Y/+kyZN0qRJk+rcd8Bv4woAAADgl4MCAgAAAIDHgnYK09Gq+nJW1atznLWlLUzI5rTSH86+gt5TCRfZTYslST+UX2BarBWH25gW65ij7r/Hn/pL1wWmxeoXZe5mK/lHm5oWa92+XqbF+vfls0yLJUkvf3+tqfEAADBbqE9hCiRGIAAAAAB4jAICAAAAgMeCdgoTAAAA4CunYZElwFOInExhAgAAAHC+YwQCAAAAISeUd6IONK9HIFauXKnBgwcrMTFRFotFCxYscDv/7rvvKi0tTY0bN5bFYlFhYaFJqQIAAAAINK8LiPLycnXp0kXTpk2r9fw111yjZ555ps7JAQAAAAguXk9hysjIUEZGRq3nb7/9dknSrl27fE4KAAAAqIuTU5gCvQ9EQLv3m4CvgXA4HHI4HK7HZWVlAcwGAAAAwJkE/C5MOTk5stlsrpaUlBTolAAAAADUIuAFRHZ2tux2u6sVFxcHOiUAAAD8whmGJShaKAr4FCar1Sqr1RroNAAAAAB4IOAjEAAAAAB+ObwegThy5Ii2bdvmerxz504VFhYqNjZWzZs318GDB7V7927t3btXklRUVCRJSkhIUEJCgklpAwAAALUz/tcCnUMo8noEYsOGDeratau6du0qSZowYYK6du2qxx9/XJK0cOFCde3aVYMGDZIk3XLLLeratatmzJhhYtoAAAAAAsHrEYiUlBQZZ7ip7ahRozRq1Ki65AQAAADUSTAsYg50//7CGggAAAAAHqOAAAAAAOCxgN/GtTbbyuNUT/XrHCcy4oQJ2ZzWoOEx02KlNf3atFiS9OmBVqbF+vaHC02L1fyiH02LJUmLDnYxLdYjO240LZYkPXR5vmmxbmu9wbRYK8rbmhZLkhKsdtNilThspsUCAMCFVdR+wwgEAAAAAI9RQAAAAADwWNBOYQIAAAB8FgR3YVKg+/cTRiAAAAAAeIwRCAAAAIQcwzjZAp1DKPJ6BGLlypUaPHiwEhMTZbFYtGDBAte5yspKPfLII7rsssvUoEEDJSYm6o477tDevXvNzBkAAABAgHhdQJSXl6tLly6aNm1atXMVFRXauHGjHnvsMW3cuFHvvvuuioqKdP3115uSLAAAAIDA8noKU0ZGhjIyMmo8Z7PZlJ/vfh/8v//977rqqqu0e/duNW/e3LcsAQAAAC8YQbCIOtD9+4vfF1Hb7XZZLBY1atTI310BAAAA8DO/LqI+duyYHnnkEd16662KiYmp8RqHwyGHw+F6XFZW5s+UAAAAANSB30YgKisrNXz4cBmGoenTp9d6XU5Ojmw2m6slJSX5KyUAAACcLwxLcLQQ5JcC4lTx8O233yo/P7/W0QdJys7Olt1ud7Xi4mJ/pAQAAADABKZPYTpVPHzzzTdatmyZGjdufMbrrVarrFar2WkAAAAA8AOvC4gjR45o27Ztrsc7d+5UYWGhYmNj1bRpU/3f//2fNm7cqEWLFqmqqkolJSWSpNjYWNWvX9+8zAEAAIBasJGc/3hdQGzYsEF9+/Z1PZ4wYYIkaeTIkZo0aZIWLlwoSbr88svdnrds2TKlpKT4nikAAACAgPO6gEhJSZFxhnLqTOcAAACAc8L4Xwt0DiHI7/tAAAAAAAgdFBAAAAAAPObXjeQAAACAQDAMi4wA78MQ6P79JWgLiIgwpyLCnHWOU+U0d5ClSfQR02LtOnrmW9x6KyNhs2mxZpb1Mi3WwYoo02JJ0j577fuKeKt/myLTYknSjdFbTYv10HcZpsVqGHHctFiS1Caq1LRYn+5LNi2WJPVqusvUeAAAwB1TmAAAAAB4LGhHIAAAAIA6CdG7IAUaIxAAAAAAPMYIBAAAAEIOi6j9hxEIAAAAAB7zuoBYuXKlBg8erMTERFksFi1YsMDt/KRJk9SuXTs1aNBAF154oVJTU7V27Vqz8gUAAAAQQF4XEOXl5erSpYumTZtW4/lLL71Uf//737Vp0yatWrVKycnJSktL0/fff1/nZAEAAACPGEHSQpDXayAyMjKUkVH7/elvu+02t8fPP/+8Zs6cqS+++EL9+/f3PkMAAAAAQcOvi6iPHz+ul19+WTabTV26dKnxGofDIYfD4XpcVlbmz5QAAAAA1IFfFlEvWrRIDRs2VGRkpF544QXl5+froosuqvHanJwc2Ww2V0tKSvJHSgAAADivWIKkhR6/FBB9+/ZVYWGhPv30U6Wnp2v48OEqLS2t8drs7GzZ7XZXKy4u9kdKAAAAAEzglwKiQYMGatOmja6++mrNnDlTERERmjlzZo3XWq1WxcTEuDUAAACgTgK9eDqEF1Gfk30gnE6n2zoHAAAAAL9MXi+iPnLkiLZt2+Z6vHPnThUWFio2NlaNGzfWU089peuvv15NmzbVgQMHNG3aNO3Zs0c33XSTqYkDAAAAOPe8LiA2bNigvn37uh5PmDBBkjRy5EjNmDFDX3/9tebMmaMDBw6ocePG6t69uz7++GN17NjRvKwBAACAMwmGKUSB7t9PvC4gUlJSZBi1/zTefffdOiUEAAAAIHidkzUQAAAAAEKDXzeSAwAAAALCsJxsgc4hBAVtAXHoWJQiwq11jjP44i9MyOa017ZdZVqs7rHfmhZLkr48crFpsSorw02LVX440rRYkmRUmfdh3BPbyLRYknT71ptNi7V1R1PTYoVFnjAtliRtSWhiWqy8rq+YFkuS4sMbmBYra08P02IBABAqmMIEAAAAwGNBOwIBAAAA+MowTrZA5xCKGIEAAAAA4DFGIAAAABB62AfCbxiBAAAAAOAxrwuIlStXavDgwUpMTJTFYtGCBQtqvfaee+6RxWLRlClT6pAiAAAAgGDhdQFRXl6uLl26aNq0aWe8Ljc3V2vWrFFiYqLPyQEAAAA+ObUPRKBbCPJ6DURGRoYyMjLOeM2ePXs0btw4LV68WIMGDfI5OQAAAADBxfRF1E6nU7fffrsefvhhdezY8azXOxwOORwO1+OysjKzUwIAAABgEtMXUT/zzDOKiIjQfffd59H1OTk5stlsrpaUlGR2SgAAADjPWIzgaKHI1AKioKBAL774ombPni2LxbM5X9nZ2bLb7a5WXFxsZkoAAABAyDl48KBGjBihmJgYNWrUSHfddZeOHDlyxuvHjRuntm3bKioqSs2bN9d9990nu93udd+mFhAff/yxSktL1bx5c0VERCgiIkLffvutHnzwQSUnJ9f4HKvVqpiYGLcGAAAA1IkRJM1PRowYoS+//FL5+flatGiRVq5cqbFjx9Z6/d69e7V3714999xz2rx5s2bPnq28vDzdddddXvdt6hqI22+/XampqW7HBgwYoNtvv12jR482sysAAADgvLRlyxbl5eVp/fr1uvLKKyVJU6dO1cCBA/Xcc8/VeBfUTp066Z133nE9bt26tZ566in95je/0YkTJxQR4XlZ4HUBceTIEW3bts31eOfOnSosLFRsbKyaN2+uxo0bu11fr149JSQkqG3btt52BQAAAOBnVq9erUaNGrmKB0lKTU1VWFiY1q5dqxtuuMGjOHa7XTExMV4VD5IPBcSGDRvUt29f1+MJEyZIkkaOHKnZs2d7Gw4AAAAwXzDsw/C//n9+l1Gr1Sqr1epz2JKSEsXHx7sdi4iIUGxsrEpKSjyKceDAAT355JNnnPZUG68LiJSUFBmG5xO6du3a5W0XAAAAQMj4+V1Gn3jiCU2aNKnadRMnTtQzzzxzxlhbtmypcz5lZWUaNGiQOnToUGMeZ2P6PhAAAAAATisuLna7UVBtow8PPvigRo0adcZYrVq1UkJCgkpLS92OnzhxQgcPHlRCQsIZn3/48GGlp6crOjpaubm5qlevnmcv4icoIAAAABB6/HwXJI9zkDy+02hcXJzi4uLOel3Pnj116NAhFRQUqFu3bpKkpUuXyul0qkePHrU+r6ysTAMGDJDVatXChQsVGRnp2ev4GdM3kgMAAADgP+3bt1d6errGjBmjdevW6ZNPPlFWVpZuueUW1x2Y9uzZo3bt2mndunWSThYPaWlpKi8v18yZM1VWVqaSkhKVlJSoqqrKq/6DdgTisgv3ytrQ+yGVnztQGW1CNqelJhWZFqvgYHPTYklSyWHzXmtCo8OmxZrUeqFpsSTpwa/+z9R4ZrJZj5oWq00rzxZBecJp8iKyww7fF3793N07hpkWSzL3tT6UtNi0WG8f7G5aLAAAXn/9dWVlZal///4KCwvTsGHD9NJLL7nOV1ZWqqioSBUVFZKkjRs3au3atZKkNm3auMXauXNnrXu21SRoCwgAAADAZ0E0hckfYmNjNXfu3FrPJycnu934yNsbIZ0JU5gAAAAAeIwRCAAAAISeEB+BCCRGIAAAAAB4zOsCYuXKlRo8eLASExNlsVi0YMECt/OjRo2SxWJxa+np6WblCwAAACCAvJ7CVF5eri5duujOO+/UjTfeWOM16enpmjVrlutxXbbqBgAAALxmWE62QOcQgrwuIDIyMpSRkXHGa6xW61l3wQMAAADwy+OXNRDLly9XfHy82rZtq3vvvVc//PCDP7oBAAAAcI6Zfhem9PR03XjjjWrZsqW2b9+uRx99VBkZGVq9erXCw8OrXe9wOORwOFyPy8rKzE4JAAAA5xmLcbIFOodQZHoBccstt7j+/2WXXabOnTurdevWWr58ufr371/t+pycHE2ePNnsNAAAAAD4gd9v49qqVStddNFF2rZtW43ns7OzZbfbXa24uNjfKQEAACDUGUHSQpDfN5L77rvv9MMPP6hp06Y1nrdardylCQAAAPiF8LqAOHLkiNtows6dO1VYWKjY2FjFxsZq8uTJGjZsmBISErR9+3b9/ve/V5s2bTRgwABTEwcAAABw7nldQGzYsEF9+/Z1PZ4wYYIkaeTIkZo+fbq++OILzZkzR4cOHVJiYqLS0tL05JNPMsoAAAAAhACvC4iUlBQZRu0TuhYvXlynhAAAAAAEL78vogYAAAAQOvy+iBoAAAA41ywK/D4MlsB27zeMQAAAAADwWNCOQKz7voUiKoJv4fXQiz83LVZSwkHTYknSluiab5Xri6TIH02L9UNVQ9NiSVJqs62mxTp8ItK0WJI0LHa9abGWHe5gWixbRIVpsSTJfuIC02IdqTL3c35RvSOmxVpxpJ1psVbuaWVaLEnqc/EOU+MBAOCpoC0gAAAAAJ8ZlpMt0DmEIKYwAQAAAPAYIxAAAAAIPcb/WqBzCEGMQAAAAADwGAUEAAAAAI95XUCsXLlSgwcPVmJioiwWixYsWFDtmi1btuj666+XzWZTgwYN1L17d+3evduMfAEAAICzM4KkhSCvC4jy8nJ16dJF06ZNq/H89u3bdc0116hdu3Zavny5vvjiCz322GOKjDT3dpkAAAAAzj2vF1FnZGQoIyOj1vN/+MMfNHDgQD377LOuY61bt/YtOwAAAABBxdQ1EE6nUx988IEuvfRSDRgwQPHx8erRo0eN05xOcTgcKisrc2sAAABAXViM4GihyNQCorS0VEeOHNHTTz+t9PR0/ec//9ENN9ygG2+8UStWrKjxOTk5ObLZbK6WlJRkZkoAAAAATGTqPhBOp1OSNGTIED3wwAOSpMsvv1yffvqpZsyYoWuvvbbac7KzszVhwgTX47KyMooIAAAA1E0wLGIOdP9+YmoBcdFFFykiIkIdOnRwO96+fXutWrWqxudYrVZZrVYz0wAAAADgJ6ZOYapfv766d++uoqIit+Nbt25VixYtzOwKAAAAQAB4PQJx5MgRbdu2zfV4586dKiwsVGxsrJo3b66HH35YN998s/r06aO+ffsqLy9P77//vpYvX25m3gAAAEDtmMLkN14XEBs2bFDfvn1dj0+tXxg5cqRmz56tG264QTNmzFBOTo7uu+8+tW3bVu+8846uueYa87IGAAAAEBBeFxApKSkyjDOXU3feeafuvPNOn5MCAAAAEJxMXUQNAAAABINg2Ich0P37i6mLqAEAAACEtqAdgah4v4nC60fWOU783M0mZHParmWNTY1npnATy9zS49GmxTpQealpsSTphDPctFgRlirTYknSez9eYVosp2FefV92ou6fpZ8yM7cwi9O0WJJU4rCZFsvM3Jrdsce0WJK0+A/mvdcG9NtoWiwACBqG5WQLdA4hiBEIAAAAAB6jgAAAAADgsaCdwgQAAAD4jH0g/IYRCAAAAAAeo4AAAAAA4DGvC4iVK1dq8ODBSkxMlMVi0YIFC9zOWyyWGttf//pXs3IGAAAAzujUPhCBbqHI6wKivLxcXbp00bRp02o8v2/fPrf2yiuvyGKxaNiwYXVOFgAAAEBgeb2IOiMjQxkZGbWeT0hIcHv83nvvqW/fvmrVqpX32QEAAAAIKn69C9P+/fv1wQcfaM6cOf7sBgAAAHDHXZj8xq8FxJw5cxQdHa0bb7yx1mscDoccDofrcVlZmT9TAgAAAFAHfr0L0yuvvKIRI0YoMjKy1mtycnJks9lcLSkpyZ8pAQAA4HwQDAuoQ3QEwm8FxMcff6yioiLdfffdZ7wuOztbdrvd1YqLi/2VEgAAAIA68tsUppkzZ6pbt27q0qXLGa+zWq2yWq3+SgMAAACAibwuII4cOaJt27a5Hu/cuVOFhYWKjY1V8+bNJZ1cxzB//nz97W9/My9TAAAAwFPBMIUo0P37idcFxIYNG9S3b1/X4wkTJkiSRo4cqdmzZ0uS5s2bJ8MwdOutt5qTJQAAAICg4HUBkZKSIsM4czk1duxYjR071uekAAAAAAQnv97GFQAAAAgIpjD5jV9v4woAAAAgtDACAQAAgJDj2oshwDmEoqAtIE5cZ5dxwbE6x7G8ae4gS1H3E6bFar3O3NvXhpk4TnbCCDctVpXTYlosSQo38dNY6Qzaj4DqWZymxTpaVd+0WJJUL6zKtFhOE99rZjM1t3BzX+eM/3vZtFhLyjqaFkuSyk5EmRoPABBcmMIEAAAAwGMUEAAAAAA8RgEBAAAAwGMUEAAAAAA85nUBsXLlSg0ePFiJiYmyWCxasGCB2/kjR44oKytLzZo1U1RUlDp06KAZM2aYlS8AAABwdkaQtBDkdQFRXl6uLl26aNq0aTWenzBhgvLy8vTaa69py5YtGj9+vLKysrRw4cI6JwsAAAAgsLy+h2VGRoYyMjJqPf/pp59q5MiRSklJkSSNHTtW//znP7Vu3Tpdf/31PicKAAAAIPBMXwPRq1cvLVy4UHv27JFhGFq2bJm2bt2qtLQ0s7sCAAAAanRqI7lAt1Bk+i5aU6dO1dixY9WsWTNFREQoLCxM//rXv9SnT58ar3c4HHI4HK7HZWVlZqcEAAAAwCR+KSDWrFmjhQsXqkWLFlq5cqUyMzOVmJio1NTUatfn5ORo8uTJZqcBAACA812IjgAEmqkFxNGjR/Xoo48qNzdXgwYNkiR17txZhYWFeu6552osILKzszVhwgTX47KyMiUlJZmZFgAAAACTmFpAVFZWqrKyUmFh7ksrwsPD5XQ6a3yO1WqV1Wo1Mw0AAAAAfuJ1AXHkyBFt27bN9Xjnzp0qLCxUbGysmjdvrmuvvVYPP/ywoqKi1KJFC61YsUKvvvqqnn/+eVMTBwAAAGoVDPswBLp/P/G6gNiwYYP69u3renxq+tHIkSM1e/ZszZs3T9nZ2RoxYoQOHjyoFi1a6KmnntI999xjXtYAAAAAAsLr27impKTIMIxqbfbs2ZKkhIQEzZo1S3v27NHRo0f19ddfa8KECbJYLGbnDgAAAJyXDh48qBEjRigmJkaNGjXSXXfdpSNHjpzxOb/97W/VunVrRUVFKS4uTkOGDNHXX3/tdd+m7wMBAAAABFqg93/w9z4QI0aM0Jdffqn8/HwtWrRIK1eu1NixY8/4nG7dumnWrFnasmWLFi9eLMMwlJaWpqqqKq/6Nv02rgAAAAD8Z8uWLcrLy9P69et15ZVXSjq5lcLAgQP13HPPKTExscbn/bTASE5O1p///Gd16dJFu3btUuvWrT3unxEIAAAAhB4jSJofrF69Wo0aNXIVD5KUmpqqsLAwrV271qMY5eXlmjVrllq2bOn1FgpBOwIREV6l8HDvhlNqkpzvOPtFXthxTX3TYm2/ytzcWq8z73a4EZa6/+xPxzItlCTJaZhX90aEmfc6zXbCCDctVoMIc99rJ5zm5RZmqfkWz74y8/1ham5eDg+fTbnTvM972Yko02JJUqN6FabFOlR5gWmxACBQysrK3B7XdRuDkpISxcfHux2LiIhQbGysSkpKzvjcf/zjH/r973+v8vJytW3bVvn5+apf37t/3zICAQAAAPhRUlKSbDabq+Xk5NR43cSJE2WxWM7YfFn0/FMjRozQZ599phUrVujSSy/V8OHDdezYMa9iBO0IBAAAAOArfy9i9jQHSSouLlZMTIzreG2jDw8++KBGjRp1xpitWrVSQkKCSktL3Y6fOHFCBw8eVEJCwhmff6qIueSSS3T11VfrwgsvVG5urm699dazv6D/oYAAAAAA/CgmJsatgKhNXFyc4uLiznpdz549dejQIRUUFKhbt26SpKVLl8rpdKpHjx4e53VqOwaHw7upzkxhAgAAAH5B2rdvr/T0dI0ZM0br1q3TJ598oqysLN1yyy2uOzDt2bNH7dq107p16yRJO3bsUE5OjgoKCrR79259+umnuummmxQVFaWBAwd61T8FBAAAAEJPoO++5Me7MEnS66+/rnbt2ql///4aOHCgrrnmGr388suu85WVlSoqKlJFxckbW0RGRurjjz/WwIED1aZNG918882Kjo7Wp59+Wm1B9tl4PYVp5cqV+utf/6qCggLt27dPubm5Gjp0qOv8/v379cgjj+g///mPDh06pD59+mjq1Km65JJLvO0KAAAAQA1iY2M1d+7cWs8nJyfLME5XMImJifrwww9N6dvrEYjy8nJ16dJF06ZNq3bOMAwNHTpUO3bs0HvvvafPPvtMLVq0UGpqqsrLy01JGAAAADirQI88+HkEIpC8HoHIyMhQRkZGjee++eYbrVmzRps3b1bHjh0lSdOnT1dCQoLeeOMN3X333XXLFgAAAEBAmboG4tQK7sjIyNMdhIXJarVq1apVtT6nrKzMrQEAAAAITqYWEO3atVPz5s2VnZ2tH3/8UcePH9czzzyj7777Tvv27avxOTk5OW4ba3i7lTYAAADwc6f2gQh0C0WmFhD16tXTu+++q61btyo2NlYXXHCBli1bpoyMDIWF1dxVdna27Ha7qxUXF5uZEgAAAAATmb6RXLdu3VRYWCi73a7jx48rLi5OPXr00JVXXlnj9Vartdbd+AAAAAAEF7/tA2Gz2RQXF6dvvvlGGzZs0JAhQ/zVFQAAAOAu0Hdf4i5Mpx05ckTbtm1zPd65c6cKCwsVGxur5s2ba/78+YqLi1Pz5s21adMm3X///Ro6dKjS0tJMTRwAAADAued1AbFhwwb17dvX9XjChAmSpJEjR2r27Nnat2+fJkyYoP3796tp06a644479Nhjj5mXMQAAAICA8bqASElJcdvV7ufuu+8+3XfffXVKCgAAAKiTYJhCFOj+/cRvayAAAAAAhB7T78IEAAAABFow7MMQ6P79JWgLiPaN96teg/p1jhNhqTIhm9NarQo3Ldb27ua+q3b1r/vP65TkJcdNi1VlWEyLJUn1LE7TYp0wzPt9Sua+VjNf59GqeqbFkoL7d2Amp5m51TP36/aTI5eaFsspcz+j3x9vaFqsRvWOmhar/AS3DAcAMzCFCQAAAIDHgnYEAgAAAPAZi6j9hhEIAAAAAB6jgAAAAADgMaYwAQAAIORwFyb/8WoEIicnR927d1d0dLTi4+M1dOhQFRUVuV1z7NgxZWZmqnHjxmrYsKGGDRum/fv3m5o0AAAAgMDwqoBYsWKFMjMztWbNGuXn56uyslJpaWkqLy93XfPAAw/o/fff1/z587VixQrt3btXN954o+mJAwAAALUygqSFIK+mMOXl5bk9nj17tuLj41VQUKA+ffrIbrdr5syZmjt3rvr16ydJmjVrltq3b681a9bo6quvNi9zAAAAAOdcnRZR2+12SVJsbKwkqaCgQJWVlUpNTXVd065dOzVv3lyrV6+uS1cAAAAAgoDPi6idTqfGjx+v3r17q1OnTpKkkpIS1a9fX40aNXK7tkmTJiopKakxjsPhkMPhcD0uKyvzNSUAAADgpGCYQhTo/v3E5xGIzMxMbd68WfPmzatTAjk5ObLZbK6WlJRUp3gAAAAA/MenAiIrK0uLFi3SsmXL1KxZM9fxhIQEHT9+XIcOHXK7fv/+/UpISKgxVnZ2tux2u6sVFxf7khIAAACAc8CrAsIwDGVlZSk3N1dLly5Vy5Yt3c5369ZN9erV05IlS1zHioqKtHv3bvXs2bPGmFarVTExMW4NAAAAqAtLkLRQ5NUaiMzMTM2dO1fvvfeeoqOjXesabDaboqKiZLPZdNddd2nChAmKjY1VTEyMxo0bp549e3IHJgAAACAEeFVATJ8+XZKUkpLidnzWrFkaNWqUJOmFF15QWFiYhg0bJofDoQEDBugf//iHKckCAAAACCyvCgjDOPtS8sjISE2bNk3Tpk3zOSkAAACgTrgLk9/UaR8IAAAAAOcXn/eBAAAAAIKVxTjZAp1DKGIEAgAAAIDHgnYEYvuhixReaa1znK4X7TEhm9MiLFWmxWq9PtK0WJK0M8W83Hb92rzb6bZaZDctltnCTJ6cGGbinxqcJt78zRp2wrRYkuQ0zPvbg5mfKcnc3MIsTtNiqdLc30G3BjtNi7XS3s60WJIUEWbe7/TwCfO+J6MjjpkWS5LKT9T9v1EA8EsUtAUEAAAA4DMWUfsNU5gAAAAAeIwCAgAAAIDHmMIEAACA0BSiU4gCjREIAAAAAB7zqoDIyclR9+7dFR0drfj4eA0dOlRFRUVu17z88stKSUlRTEyMLBaLDh06ZGa+AAAAwFmd2gci0C0UeVVArFixQpmZmVqzZo3y8/NVWVmptLQ0lZeXu66pqKhQenq6Hn30UdOTBQAAABBYXq2ByMvLc3s8e/ZsxcfHq6CgQH369JEkjR8/XpK0fPlyUxIEAAAAEDzqtIjabj+5QVhsbKzPMRwOhxwOh+txWVlZXVICAAAA2AfCj3xeRO10OjV+/Hj17t1bnTp18jmBnJwc2Ww2V0tKSvI5FgAAAAD/8rmAyMzM1ObNmzVv3rw6JZCdnS273e5qxcXFdYoHAAAAwH98msKUlZWlRYsWaeXKlWrWrFmdErBarbJarXWKAQAAAPxUMNwFKdD9+4tXBYRhGBo3bpxyc3O1fPlytWzZ0l95AQAAAAhCXhUQmZmZmjt3rt577z1FR0erpKREkmSz2RQVFSVJKikpUUlJibZt2yZJ2rRpk6Kjo9W8efM6LbYGAAAAPMYiar/xag3E9OnTZbfblZKSoqZNm7ram2++6bpmxowZ6tq1q8aMGSNJ6tOnj7p27aqFCxeamzkAAACAc87rKUxnM2nSJE2aNMnXfAAAAAAEsTrtAwEAAAAEIxZR+4/Pt3EFAAAAcP4J2hGIxhdUqN4FJ+ocJ8zk1StOI3hrrrYrHWe/yENbB0SaFmtHf3Nv09tqiXmv0ymLabEkqcowL149i9O0WCeMcNNimc15nuTmyRRQb3x3vLFpscz+HJj5c4uwVJkWq/yEud9FDSLM+y4yOzcA8KegLSAAAAAAn3EXJr8J3j+nAwAAAAg6FBAAAAAAPMYUJgAAAIQepjD5DSMQAAAAADzmVQGRk5Oj7t27Kzo6WvHx8Ro6dKiKiopc5w8ePKhx48apbdu2ioqKUvPmzXXffffJbrebnjgAAABQm1P7QAS6hSKvCogVK1YoMzNTa9asUX5+viorK5WWlqby8nJJ0t69e7V3714999xz2rx5s2bPnq28vDzdddddfkkeAAAAwLnl1RqIvLw8t8ezZ89WfHy8CgoK1KdPH3Xq1EnvvPOO63zr1q311FNP6Te/+Y1OnDihiAiWXAAAAAC/ZHX6F/2pqUmxsbFnvCYmJqbW4sHhcMjhOL0ZT1lZWV1SAgAAAFhE7Uc+L6J2Op0aP368evfurU6dOtV4zYEDB/Tkk09q7NixtcbJycmRzWZztaSkJF9TAgAAAOBnPhcQmZmZ2rx5s+bNm1fj+bKyMg0aNEgdOnTQpEmTao2TnZ0tu93uasXFxb6mBAAAAMDPfJrClJWVpUWLFmnlypVq1qxZtfOHDx9Wenq6oqOjlZubq3r16tUay2q1ymq1+pIGAAAAUCOLYchiBHYOUaD79xevRiAMw1BWVpZyc3O1dOlStWzZsto1ZWVlSktLU/369bVw4UJFRkaaliwAAACAwPJqBCIzM1Nz587Ve++9p+joaJWUlEiSbDaboqKiXMVDRUWFXnvtNZWVlbkWRcfFxSk8PNz8VwAAAAD8HIuo/carAmL69OmSpJSUFLfjs2bN0qhRo7Rx40atXbtWktSmTRu3a3bu3Knk5GTfMwUAAAAQcF4VEMZZ5nGlpKSc9RoAAAAAv1zs7AYAAICQYzFOtkDnEIp8vo0rAAAAgPMPBQQAAAAAjwXtFCanYZHTsNQ9juoew1/CTF6af7Sq9v02vNXqoyOmxdrR39x9PnZcZ96tgVvlHzMtliRFhFWZFuuEYd5dy8x+r4VZnKbFMvN1Sua/VrNYLOZ+F5n5OwjWn5kkOZxB+58pHT5h3ndRdIS530XlJ9hfCQj1uzAdPHhQ48aN0/vvv6+wsDANGzZML774oho2bHj2tAxDAwcOVF5ennJzczV06FCv+mYEAgAAAPiFGTFihL788kvl5+e7NngeO3asR8+dMmVKnf6wFbx/2gEAAABQzZYtW5SXl6f169fryiuvlCRNnTpVAwcO1HPPPafExMRan1tYWKi//e1v2rBhg5o2bepT/4xAAAAAIOScugtToJs/rF69Wo0aNXIVD5KUmpqqsLAw155sNamoqNBtt92madOmKSEhwef+GYEAAAAA/KisrMztsdVqldXq+1qlkpISxcfHux2LiIhQbGysSkpKan3eAw88oF69emnIkCE+9y15OQKRk5Oj7t27Kzo6WvHx8Ro6dKiKiorcrvntb3+r1q1bKyoqSnFxcRoyZIi+/vrrOiUJAAAAeMUIkiYpKSlJNpvN1XJycmpMeeLEibJYLGdsvv67euHChVq6dKmmTJni0/N/yqsRiBUrVigzM1Pdu3fXiRMn9OijjyotLU1fffWVGjRoIEnq1q2bRowYoebNm+vgwYOaNGmS0tLStHPnToWHm3u3FQAAACDYFRcXKyYmxvW4ttGHBx98UKNGjTpjrFatWikhIUGlpaVux0+cOKGDBw/WOjVp6dKl2r59uxo1auR2fNiwYfrVr36l5cuXn/V1nOJVAZGXl+f2ePbs2YqPj1dBQYH69OkjSW6rv5OTk/XnP/9ZXbp00a5du9S6dWtvugMAAAB+8WJiYtwKiNrExcUpLi7urNf17NlThw4dUkFBgbp16ybpZIHgdDrVo0ePGp8zceJE3X333W7HLrvsMr3wwgsaPHiwB6/itDqtgbDb7ZKk2NjYGs+Xl5dr1qxZatmypZKSkurSFQAAAOAxfy5i9iYHf2jfvr3S09M1ZswYzZgxQ5WVlcrKytItt9ziugPTnj171L9/f7366qu66qqrlJCQUOPoRPPmzdWyZUuv+vf5LkxOp1Pjx49X79691alTJ7dz//jHP9SwYUM1bNhQH330kfLz81W/fv0a4zgcDpWVlbk1AAAAALV7/fXX1a5dO/Xv318DBw7UNddco5dfftl1vrKyUkVFRaqoqDC9b59HIDIzM7V582atWrWq2rkRI0bouuuu0759+/Tcc89p+PDh+uSTTxQZWX3XzpycHE2ePNnXNAAAAIDzTmxsrObOnVvr+eTkZBnGmYdAzna+Nj6NQGRlZWnRokVatmyZmjVrVu28zWbTJZdcoj59+ujtt9/W119/rdzc3BpjZWdny263u1pxcbEvKQEAAACnBfruSz+5C1Oo8WoEwjAMjRs3Trm5uVq+fLlH86UMw5BhGHI4HDWer+t9cAEAAACcO14VEJmZmZo7d67ee+89RUdHuzaqsNlsioqK0o4dO/Tmm28qLS1NcXFx+u677/T0008rKipKAwcO9MsLAAAAAGoS6EXUocqrKUzTp0+X3W5XSkqKmjZt6mpvvvmmJCkyMlIff/yxBg4cqDZt2ujmm29WdHS0Pv3002q75QEAAAD45fF6CtOZJCYm6sMPP6xTQgAAAACCV532gQAAAACCkmGcbIHOIQT5vA8EAAAAgPMPBQQAAAAAjwXtFKZG9Y+qXv2qOsepH3bChGxOcxoWE2OZW79FhVeaFsvM19lqSc238PXVjoyGpsXa3q/mHdJ91XKpeb+DsCC+efQJI9y0WMH8Ok0VZt5nSpJKj8eYGi9Y1bM4TYsVZmIsydzv8PIT5t7OPCr8uGmxjlaZ+z0JnCsWI/B3YQp0//7CCAQAAAAAj1FAAAAAAPBY0E5hAgAAAHxm/K8FOocQxAgEAAAAAI8xAgEAAICQY3GebIHOIRR5NQKRk5Oj7t27Kzo6WvHx8Ro6dKiKiopqvNYwDGVkZMhisWjBggVm5AoAAAAgwLwqIFasWKHMzEytWbNG+fn5qqysVFpamsrLy6tdO2XKFFks5t62EAAAAEBgeTWFKS8vz+3x7NmzFR8fr4KCAvXp08d1vLCwUH/729+0YcMGNW3a1JxMAQAAAE+xiNpv6rQGwm63S5JiY2NdxyoqKnTbbbdp2rRpSkhIOGsMh8Mhh+P0RmNlZWV1SQkAAACAH/l8Fyan06nx48erd+/e6tSpk+v4Aw88oF69emnIkCEexcnJyZHNZnO1pKQkX1MCAAAA4Gc+j0BkZmZq8+bNWrVqlevYwoULtXTpUn322Wcex8nOztaECRNcj8vKyigiAAAAUCcW42QLdA6hyKcRiKysLC1atEjLli1Ts2bNXMeXLl2q7du3q1GjRoqIiFBExMn6ZNiwYUpJSakxltVqVUxMjFsDAAAAEJy8GoEwDEPjxo1Tbm6uli9frpYtW7qdnzhxou6++263Y5dddpleeOEFDR48uO7ZAgAAAJ4wjJMt0DmEIK8KiMzMTM2dO1fvvfeeoqOjVVJSIkmy2WyKiopSQkJCjQunmzdvXq3YAAAAAPDL49UUpunTp8tutyslJUVNmzZ1tTfffNNf+QEAAAAIIl5PYfKWL88BAAAA6oJF1P7j821cAQAAAJx/KCAAAAAAeKxOO1H7U+P65apvPV7nOEer6pmQzWnhJo5FhZm8v7mZrzWYX2fyR+WmxdqVFmVaLEnacU3d37OntFplMS2W2cz8nTpl7us0+/0WrH6obGBaLDM/71Lw/g5OGOGBTuGcOVpV37RYMRFHTYslSWUnzP3eBWpl/K8FOocQxAgEAAAAAI8F7QgEAAAA4CsWUfsPIxAAAAAAPEYBAQAAAMBjTGECAABA6DGMky3QOYQgr0YgcnJy1L17d0VHRys+Pl5Dhw5VUVGR2zUpKSmyWCxu7Z577jE1aQAAAACB4VUBsWLFCmVmZmrNmjXKz89XZWWl0tLSVF7uflvNMWPGaN++fa727LPPmpo0AAAAgMDwagpTXl6e2+PZs2crPj5eBQUF6tOnj+v4BRdcoISEBHMyBAAAALzEXZj8p06LqO12uyQpNjbW7fjrr7+uiy66SJ06dVJ2drYqKirq0g0AAACAIOHzImqn06nx48erd+/e6tSpk+v4bbfdphYtWigxMVFffPGFHnnkERUVFendd9+tMY7D4ZDD4XA9Lisr8zUlAAAAAH7mcwGRmZmpzZs3a9WqVW7Hx44d6/r/l112mZo2bar+/ftr+/btat26dbU4OTk5mjx5sq9pAAAAANUZ/2uBziEE+TSFKSsrS4sWLdKyZcvUrFmzM17bo0cPSdK2bdtqPJ+dnS273e5qxcXFvqQEAAAA4BzwagTCMAyNGzdOubm5Wr58uVq2bHnW5xQWFkqSmjZtWuN5q9Uqq9XqTRoAAADAGbGI2n+8KiAyMzM1d+5cvffee4qOjlZJSYkkyWazKSoqStu3b9fcuXM1cOBANW7cWF988YUeeOAB9enTR507d/bLCwAAAABw7nhVQEyfPl3Syc3ifmrWrFkaNWqU6tevr//+97+aMmWKysvLlZSUpGHDhumPf/yjaQkDAAAACByvpzCdSVJSklasWFGnhAAAAIA6cxonW6BzCEF12gcCAAAAwPmFAgIAAACAx3zeBwIAAAAIWuwD4TdBW0A4DYuchqXOcaLCK03I5rQTznBT45nJGnbCtFhOw7zBKafq/nv8qTATP41Ji4+ZFkuSdl1t3u9gV0Yj02K1+uiIabHMZubv02ymvnctwTvgG2GpMjXeCSM4vyfNfq+Z+f4I5twOnbjAtFiS1CiiwrRYZSeiTIsFwHNBW0AAAAAAvrIo8PswmPsn1OARvH8SAwAAABB0KCAAAAAAeIwpTAAAAAg9hnGyBTqHEOTVCEROTo66d++u6OhoxcfHa+jQoSoqKqp23erVq9WvXz81aNBAMTEx6tOnj44ePWpa0gAAAAACw6sCYsWKFcrMzNSaNWuUn5+vyspKpaWlqby83HXN6tWrlZ6errS0NK1bt07r169XVlaWwsKYLQUAAAD80nk1hSkvL8/t8ezZsxUfH6+CggL16dNHkvTAAw/ovvvu08SJE13XtW3b1oRUAQAAAM9YjCC4C1NozmCq2yJqu90uSYqNjZUklZaWau3atYqPj1evXr3UpEkTXXvttVq1alXdMwUAAAAQcD4XEE6nU+PHj1fv3r3VqVMnSdKOHTskSZMmTdKYMWOUl5enK664Qv3799c333xTYxyHw6GysjK3BgAAACA4+VxAZGZmavPmzZo3b57rmNPplCT99re/1ejRo9W1a1e98MILatu2rV555ZUa4+Tk5Mhms7laUlKSrykBAAAAJxlB0kKQTwVEVlaWFi1apGXLlqlZs2au402bNpUkdejQwe369u3ba/fu3TXGys7Olt1ud7Xi4mJfUgIAAABwDni1iNowDI0bN065ublavny5WrZs6XY+OTlZiYmJ1W7tunXrVmVkZNQY02q1ymq1epk2AAAAUDuLYcgS4H0YAt2/v3hVQGRmZmru3Ll67733FB0drZKSEkmSzWZTVFSULBaLHn74YT3xxBPq0qWLLr/8cs2ZM0dff/213n77bb+8AAAAAADnjlcFxPTp0yVJKSkpbsdnzZqlUaNGSZLGjx+vY8eO6YEHHtDBgwfVpUsX5efnq3Xr1qYkDAAAACBwvJ7C5ImJEye67QMBAAAAnFPO/7VA5xCC2B4aAAAAgMcoIAAAAAB4zKspTAAAAMAvAXdh8p+gLSB+ON5A9Rz16xwnut4xE7I5rZ7FvMlsTllMiyVJlU7zfp1mvs4wk3dRMfPnZg07YVosSbpkrXm/gx3p5v3cdvQ391bJyUuOmxbL7PeHmSIsVabFMk6Y+16Lq3/EtFjfH29oWizJ/M+VWU4Y4abGM/P9YXZuwfy5KjsRZVqsBhEO02KVn+CW8oCngraAAAAAAHwWDDtBB7p/P2ENBAAAAACPUUAAAAAA8BhTmAAAABB6DONkC3QOIYgRCAAAAOAX5uDBgxoxYoRiYmLUqFEj3XXXXTpy5Mw32EhJSZHFYnFr99xzj9d9e1VA5OTkqHv37oqOjlZ8fLyGDh2qoqIi1/ldu3ZVS+pUmz9/vtfJAQAAAKhuxIgR+vLLL5Wfn69FixZp5cqVGjt27FmfN2bMGO3bt8/Vnn32Wa/79qqAWLFihTIzM7VmzRrl5+ersrJSaWlpKi8vlyQlJSW5JbRv3z5NnjxZDRs2VEZGhtfJAQAAAL6wGMHR/GHLli3Ky8vT//t//089evTQNddco6lTp2revHnau3fvGZ97wQUXKCEhwdViYmK87t+rAiIvL0+jRo1Sx44d1aVLF82ePVu7d+9WQUGBJCk8PNwtoYSEBOXm5mr48OFq2NDc+4wDAAAA56PVq1erUaNGuvLKK13HUlNTFRYWprVr157xua+//rouuugiderUSdnZ2aqoqPC6/zotorbb7ZKk2NjYGs8XFBSosLBQ06ZNqzWGw+GQw3F6I5iysrK6pAQAAAAE1SLqn//71mq1ymr1ffPCkpISxcfHux2LiIhQbGysSkpKan3ebbfdphYtWigxMVFffPGFHnnkERUVFendd9/1qn+fF1E7nU6NHz9evXv3VqdOnWq8ZubMmWrfvr169epVa5ycnBzZbDZXS0pK8jUlAAAAIOgkJSW5/Xs3JyenxusmTpxY63riU+3rr7/2OY+xY8dqwIABuuyyyzRixAi9+uqrys3N1fbt272K4/MIRGZmpjZv3qxVq1bVeP7o0aOaO3euHnvssTPGyc7O1oQJE1yPy8rKKCIAAAAQMoqLi93WGtQ2+vDggw9q1KhRZ4zVqlUrJSQkqLS01O34iRMndPDgQSUkJHicV48ePSRJ27ZtU+vWrT1+nk8FRFZWlmu1d7NmzWq85u2331ZFRYXuuOOOM8aq6xAOAAAA8HMW58kW6BwkKSYmxqPFynFxcYqLizvrdT179tShQ4dUUFCgbt26SZKWLl0qp9PpKgo8UVhYKElq2rSpx8+RvJzCZBiGsrKylJubq6VLl6ply5a1Xjtz5kxdf/31Hv0QAAAAAHimffv2Sk9P15gxY7Ru3Tp98sknysrK0i233KLExERJ0p49e9SuXTutW7dOkrR9+3Y9+eSTKigo0K5du7Rw4ULdcccd6tOnjzp37uxV/16NQGRmZmru3Ll67733FB0d7VqkYbPZFBUV5bpu27ZtWrlypT788EOvkgEAAABwdq+//rqysrLUv39/hYWFadiwYXrppZdc5ysrK1VUVOS6y1L9+vX13//+V1OmTFF5ebmSkpI0bNgw/fGPf/S6b68KiOnTp0s6uYvdT82aNcttvtYrr7yiZs2aKS0tzeuEAAAAgDoLorsw+UNsbKzmzp1b6/nk5GQZP+k/KSlJK1asMKVvrwoIw8Mfwl/+8hf95S9/8SkhAAAAAMHL59u4AgAAADj/1GkjOQAAACAoGf9rgc4hBAVtAdEpeo8iG9arc5x9x20mZHPacad5P7Iwk99VYZbgfJc6ZTE1npk/txNGuGmxJKnKMO+1tsorNy3WjusiTYslSd8OrXn3eV+0XPCDabEkc99vlSZ+3i316/599lOb7YmmxWre4KBpsSTJYeLPLdzE7zWzv3PN/P4wOzezv3eD1dEq8z5XDSIcpsWSpPIT3KIeoStoCwgAAADAVxbDkCXAi6gD3b+/sAYCAAAAgMcoIAAAAAB4jClMAAAACD0hvg9EIDECAQAAAMBjXhUQOTk56t69u6KjoxUfH6+hQ4eqqKjI7ZqSkhLdfvvtSkhIUIMGDXTFFVfonXfeMTVpAAAAAIHhVQGxYsUKZWZmas2aNcrPz1dlZaXS0tJUXn76dpN33HGHioqKtHDhQm3atEk33nijhg8frs8++8z05AEAAIAaGZKcAW6hOYPJuzUQeXl5bo9nz56t+Ph4FRQUqE+fPpKkTz/9VNOnT9dVV10lSfrjH/+oF154QQUFBeratatJaQMAAAAIhDqtgbDb7ZKk2NjTm0r16tVLb775pg4ePCin06l58+bp2LFjSklJqVOiAAAAgKdO7QMR6BaKfL4Lk9Pp1Pjx49W7d2916tTJdfytt97SzTffrMaNGysiIkIXXHCBcnNz1aZNmxrjOBwOORynd38sKyvzNSUAAAAAfubzCERmZqY2b96sefPmuR1/7LHHdOjQIf33v//Vhg0bNGHCBA0fPlybNm2qMU5OTo5sNpurJSUl+ZoSAAAAAD/zaQQiKytLixYt0sqVK9WsWTPX8e3bt+vvf/+7Nm/erI4dO0qSunTpoo8//ljTpk3TjBkzqsXKzs7WhAkTXI/LysooIgAAAFA3hgK/D0NozmDyroAwDEPjxo1Tbm6uli9frpYtW7qdr6iokCSFhbkPbISHh8vpdNYY02q1ymq1epMGAAAAgADxqoDIzMzU3Llz9d577yk6OlolJSWSJJvNpqioKLVr105t2rTRb3/7Wz333HNq3LixFixYoPz8fC1atMgvLwAAAADAuePVGojp06fLbrcrJSVFTZs2dbU333xTklSvXj19+OGHiouL0+DBg9W5c2e9+uqrmjNnjgYOHOiXFwAAAABUYxjB0UKQ11OYzuaSSy5h52kAAAAgRNVpHwgAAAAA5xef94EAAAAAgpZTkiUIcghBjEAAAAAA8FjQjkBsLW+i+pb6dY4TFV5pQjanhZl4Q19nwMvi2pn5Os2MZTazcwuzBOf7o1X+MdNiSdKODPO+OrZdY1ooSVKbVeb9DiLCqkyLZRw397uobcxB02IdqowyLZYk1bME55/czP7OPZ++24LVCSPctFjlJ8z9m6qZ3x8nnOa9zvOJxTBkCfAi5kD37y+MQAAAAADwGAUEAAAAAI8F7RQmAAAAwGfBsA9DoPv3E0YgAAAAAHiMAgIAAACAx7wqIHJyctS9e3dFR0crPj5eQ4cOVVFRkds127dv1w033KC4uDjFxMRo+PDh2r9/v6lJAwAAAGd0agpToFsI8qqAWLFihTIzM7VmzRrl5+ersrJSaWlpKi8vlySVl5crLS1NFotFS5cu1SeffKLjx49r8ODBcjqD87Z+AAAAADzn1SLqvLw8t8ezZ89WfHy8CgoK1KdPH33yySfatWuXPvvsM8XExEiS5syZowsvvFBLly5VamqqeZkDAAAAtQmGEYBA9+8ndVoDYbfbJUmxsbGSJIfDIYvFIqvV6romMjJSYWFhWrVqVY0xHA6HysrK3BoAAACA4ORzAeF0OjV+/Hj17t1bnTp1kiRdffXVatCggR555BFVVFSovLxcDz30kKqqqrRv374a4+Tk5Mhms7laUlKSrykBAAAA8DOfC4jMzExt3rxZ8+bNcx2Li4vT/Pnz9f7776thw4ay2Ww6dOiQrrjiCoWF1dxVdna27Ha7qxUXF/uaEgAAAHCSM0haCPJpI7msrCwtWrRIK1euVLNmzdzOpaWlafv27Tpw4IAiIiLUqFEjJSQkqFWrVjXGslqtblOeAAAAAAQvrwoIwzA0btw45ebmavny5WrZsmWt11500UWSpKVLl6q0tFTXX3993TIFAAAAEHBeFRCZmZmaO3eu3nvvPUVHR6ukpESSZLPZFBUVJUmaNWuW2rdvr7i4OK1evVr333+/HnjgAbVt29b87AEAAIAaWAxDlgDfBSnQ/fuLVwXE9OnTJUkpKSlux2fNmqVRo0ZJkoqKipSdna2DBw8qOTlZf/jDH/TAAw+YkiwAAACAwPJ6CtPZPP3003r66ad9TggAAACoM/aB8Js67QMBAAAA4PxCAQEAAADAYz7dxvVc2G6/SBEn6n571ysuMndfiRNGuGmxIixVpsWSJKdBPRhoTllMi2Xm+8PM960ktckzb8f4rb3Nfd9uu8a8WK1Wmfdzs1jMe29IUseo70yLtaGq9jvq+cLM91uYzBv+N/s710x8f/vGzN+p2b8Dp2HeZ75BhMO0WJJUbsK/r34RnIZkCfAUIidTmAAAAACc5yggAAAAAHgsaKcwAQAAAD7jLkx+wwgEAAAAAI9RQAAAAADwmFcFxPTp09W5c2fFxMQoJiZGPXv21EcffeQ6f+zYMWVmZqpx48Zq2LChhg0bpv3795ueNAAAAHBmxulpTIFqJt5JLph4VUA0a9ZMTz/9tAoKCrRhwwb169dPQ4YM0ZdffilJeuCBB/T+++9r/vz5WrFihfbu3asbb7zRL4kDAAAAOPe8WkQ9ePBgt8dPPfWUpk+frjVr1qhZs2aaOXOm5s6dq379+kmSZs2apfbt22vNmjW6+uqrzcsaAAAAOBMWUfuNz2sgqqqqNG/ePJWXl6tnz54qKChQZWWlUlNTXde0a9dOzZs31+rVq2uN43A4VFZW5tYAAAAABCevC4hNmzapYcOGslqtuueee5Sbm6sOHTqopKRE9evXV6NGjdyub9KkiUpKSmqNl5OTI5vN5mpJSUlevwgAAAAA54bXBUTbtm1VWFiotWvX6t5779XIkSP11Vdf+ZxAdna27Ha7qxUXF/scCwAAAJAkOY3gaCHI643k6tevrzZt2kiSunXrpvXr1+vFF1/UzTffrOPHj+vQoUNuoxD79+9XQkJCrfGsVqusVqv3mQMAAAA45+q8D4TT6ZTD4VC3bt1Ur149LVmyxHWuqKhIu3fvVs+ePevaDQAAAIAg4NUIRHZ2tjIyMtS8eXMdPnxYc+fO1fLly7V48WLZbDbdddddmjBhgmJjYxUTE6Nx48apZ8+e3IEJAAAA55bhPNkCnUMI8qqAKC0t1R133KF9+/bJZrOpc+fOWrx4sa677jpJ0gsvvKCwsDANGzZMDodDAwYM0D/+8Q+/JA4AAADg3POqgJg5c+YZz0dGRmratGmaNm1anZICAAAA6oR9IPymzmsgAAAAAJw/KCAAAAAAeMzr27gCAAAAQc9pSArwFCL2gTi3Lm+8R/Ub1qtznBNGuAnZnBZm4hvR7NzgPTN/n2YL5veH0zBv8LLVKotpsSRpxzXm/U7NjGWpb+57bc3hNqbGM1Owfq6C+TNl9s/MKXM/V8HKGcS/UzNzO3zC3NfZINxR5xhh4ZUmZIJfKqYwAQAAAPBY0I5AAAAAAD7jLkx+wwgEAAAAAI9RQAAAAADwmFcFxPTp09W5c2fFxMQoJiZGPXv21EcffeQ6//LLLyslJUUxMTGyWCw6dOiQ2fkCAAAAZ2fo9DSmgLVA/xD8w6sColmzZnr66adVUFCgDRs2qF+/fhoyZIi+/PJLSVJFRYXS09P16KOP+iVZAAAAAIHl1SLqwYMHuz1+6qmnNH36dK1Zs0YdO3bU+PHjJUnLly83Kz8AAADAeyyi9huf78JUVVWl+fPnq7y8XD179jQzJwAAAABByutF1Js2bVLDhg1ltVp1zz33KDc3Vx06dPA5AYfDobKyMrcGAAAAoHYHDx7UiBEjFBMTo0aNGumuu+7SkSNHzvq81atXq1+/fmrQoIFiYmLUp08fHT161Ku+vS4g2rZtq8LCQq1du1b33nuvRo4cqa+++srbMC45OTmy2WyulpSU5HMsAAAAQJLkdAZH85MRI0boyy+/VH5+vhYtWqSVK1dq7NixZ3zO6tWrlZ6errS0NK1bt07r169XVlaWwsK8KwkshlG3yVmpqalq3bq1/vnPf7qOLV++XH379tWPP/6oRo0anfH5DodDDsfpLdXLysqUlJSkMSv+T/Ub1qtLapKkEyZvcx9m4nJ6pyymxYJvzPx9SufP7zSYPwc7rgnO+aaW+vVNjddyaaWp8cxk9ufKLMH8+eS7COdSg3DH2S86C8eRSk371QLZ7XbFxMSYkJV5ysrKZLPZlBp/tyLCzP3u9dYJ53H9t/T/mf5z2rJlizp06KD169fryiuvlCTl5eVp4MCB+u6775SYmFjj866++mpdd911evLJJ+vUf533gXA6nW4FgLesVqvrtrCnGgAAABAqfj5dvy7/dpZOjiQ0atTIVTxIJ/+oHxYWprVr19b4nNLSUq1du1bx8fHq1auXmjRpomuvvVarVq3yun+vCojs7GytXLlSu3bt0qZNm5Sdna3ly5drxIgRkqSSkhIVFhZq27Ztkk6ulygsLNTBgwe9TgwAAADwWcD3gDh9F6ikpCS3Kfs5OTl1emklJSWKj493OxYREaHY2FiVlJTU+JwdO3ZIkiZNmqQxY8YoLy9PV1xxhfr3769vvvnGq/69ugtTaWmp7rjjDu3bt082m02dO3fW4sWLdd1110mSZsyYocmTJ7uu79OnjyRp1qxZGjVqlFeJAQAAAKGguLjYbZaN1Wqt8bqJEyfqmWeeOWOsLVu2+JSD83/rMX77299q9OjRkqSuXbtqyZIleuWVV7wqarwqIGbOnHnG85MmTdKkSZO8CQkAAACYL4j2gfB0mv6DDz541j+6t2rVSgkJCSotLXU7fuLECR08eFAJCQk1Pq9p06aSVO3uqe3bt9fu3bvPmttP+bwPBAAAAADzxMXFKS4u7qzX9ezZU4cOHVJBQYG6desmSVq6dKmcTqd69OhR43OSk5OVmJiooqIit+Nbt25VRkaGV3nWeRE1AAAAgHOnffv2Sk9P15gxY7Ru3Tp98sknysrK0i233OK6A9OePXvUrl07rVu3TpJksVj08MMP66WXXtLbb7+tbdu26bHHHtPXX3+tu+66y6v+GYEAAABA6HEaUqBvK+30X/+vv/66srKy1L9/f4WFhWnYsGF66aWXXOcrKytVVFSkiooK17Hx48fr2LFjeuCBB3Tw4EF16dJF+fn5at26tVd913kfCLPZ7XY1atSo2mITAAAABIdT+3YdOnRINpst0Om4ce0DETs6OPaBODgrKPfLqIugG4E4fPiwJLEjNQAAQJA7fPhw0BUQ8L+gKyASExNVXFys6OhoWSy176R5qvI1a6TCzHjBGiuYcztfXmcw58brDHy88yU3Xmfg450vufE6/RfPMAwdPny41h2Pg4FhOGUYzoDnEIqCroAICwtTs2bNPL7e7N2rzYwXrLHMjhesscyOd77kxusMfLzzJTdeZ+DjnS+58Tr9E4+Rh/MXd2ECAAAA4LGgG4EAAAAA6sww/HoXJI9zCEG/2BEIq9WqJ554otatwAMZL1hjmR0vWGOZHe98yY3XGfh450tuvM7AxztfcuN1Bkc8hJ6gu40rAAAA4KtTt3Htb7tdEZYA38bVOK4l9n+H3G1cf7EjEAAAAADOPQoIAAAAAB5jETUAAABCj9MpWQK8D0OI7gPxix2BmDZtmpKTkxUZGakePXpo3bp1PsVZuXKlBg8erMTERFksFi1YsMDnnHJyctS9e3dFR0crPj5eQ4cOVVFRkU+xpk+frs6dO7vuwdyzZ0999NFHPuf2U08//bQsFovGjx/v0/MnTZoki8Xi1tq1a+dzPnv27NFvfvMbNW7cWFFRUbrsssu0YcMGn2IlJydXy81isSgzM9PrWFVVVXrsscfUsmVLRUVFqXXr1nryySfl67Khw4cPa/z48WrRooWioqLUq1cvrV+/3qPnnu19ahiGHn/8cTVt2lRRUVFKTU3VN99841Osd999V2lpaWrcuLEsFosKCwt9yquyslKPPPKILrvsMjVo0ECJiYm64447tHfvXp9f56RJk9SuXTs1aNBAF154oVJTU7V27VqfYv3UPffcI4vFoilTpvic26hRo6q979LT033ObcuWLbr++utls9nUoEEDde/eXbt37/Y6Vk2fB4vFor/+9a8+5XbkyBFlZWWpWbNmioqKUocOHTRjxgyfYu3fv1+jRo1SYmKiLrjgAqWnp9f6vvXk+/XYsWPKzMxU48aN1bBhQw0bNkz79+/3KdbLL7+slJQUxcTEyGKx6NChQzXm5Um8gwcPaty4cWrbtq2ioqLUvHlz3XfffbLb7T7l9tvf/latW7dWVFSU4uLiNGTIEH399dc+xTrFMAxlZGSc8bPiSbyUlJRq77V77rnH59xWr16tfv36qUGDBoqJiVGfPn109OhRr+Pt2rWr1s/C/Pnzvc6tpKREt99+uxISEtSgQQNdccUVeuedd3x6ndu3b9cNN9yguLg4xcTEaPjw4TW+b6Wz/9vA08+AJ7G8+Qzg/POLLCDefPNNTZgwQU888YQ2btyoLl26aMCAASotLfU6Vnl5ubp06aJp06bVOa8VK1YoMzNTa9asUX5+viorK5WWlqby8nKvYzVr1kxPP/20CgoKtGHDBvXr109DhgzRl19+Wacc169fr3/+85/q3LlzneJ07NhR+/btc7VVq1b5FOfHH39U7969Va9ePX300Uf66quv9Le//U0XXnihT/HWr1/vlld+fr4k6aabbvI61jPPPKPp06fr73//u7Zs2aJnnnlGzz77rKZOnepTbnfffbfy8/P173//W5s2bVJaWppSU1O1Z8+esz73bO/TZ599Vi+99JJmzJihtWvXqkGDBhowYICOHTvmdazy8nJdc801euaZZ+qUV0VFhTZu3KjHHntMGzdu1LvvvquioiJdf/31Pr/OSy+9VH//+9+1adMmrVq1SsnJyUpLS9P333/vdaxTcnNztWbNmrPupupJvPT0dLf33xtvvOFTrO3bt+uaa65Ru3bttHz5cn3xxRd67LHHFBkZ6XWsn+azb98+vfLKK7JYLBo2bJhPuU2YMEF5eXl67bXXtGXLFo0fP15ZWVlauHChV7EMw9DQoUO1Y8cOvffee/rss8/UokULpaam1vid6cn36wMPPKD3339f8+fP14oVK7R3717deOONPsWqqKhQenq6Hn300Rp/Dt7E27t3r/bu3avnnntOmzdv1uzZs5WXl6e77rrLp9y6deumWbNmacuWLVq8eLEMw1BaWpqqqqq8jnXKlClTZLFY6vQ6TxkzZozbe+7ZZ5/1Kdbq1auVnp6utLQ0rVu3TuvXr1dWVpbCwqr/0+Vs8ZKSkqp9FiZPnqyGDRsqIyPD69zuuOMOFRUVaeHChdq0aZNuvPFGDR8+XJ999plXscrLy5WWliaLxaKlS5fqk08+0fHjxzV48GA5ndX/cn22fxt4+hnwJJY3nwGch4xfoKuuusrIzMx0Pa6qqjISExONnJycOsWVZOTm5tYxu9NKS0sNScaKFStMiXfhhRca/+///T+fn3/48GHjkksuMfLz841rr73WuP/++32K88QTTxhdunTxOY+feuSRR4xrrrnGlFg1uf/++43WrVsbTqfT6+cOGjTIuPPOO92O3XjjjcaIESO8jlVRUWGEh4cbixYtcjt+xRVXGH/4wx+8ivXz96nT6TQSEhKMv/71r65jhw4dMqxWq/HGG294Feundu7caUgyPvvsM5/yqsm6desMSca3335rSjy73W5IMv773//6FOu7774zLr74YmPz5s1GixYtjBdeeOGsedUWb+TIkcaQIUM8ev7ZYt18883Gb37zG1Ni/dyQIUOMfv36+RyvY8eOxp/+9Ce3Y568j38eq6ioyJBkbN682XWsqqrKiIuLM/71r3+dNbeff78eOnTIqFevnjF//nzXNVu2bDEkGatXr/Yq1k8tW7bMkGT8+OOPZ83Jk3invPXWW0b9+vWNysrKOsf6/PPPDUnGtm3bfIr12WefGRdffLGxb98+r/47WFM8X//bUlOsHj16GH/84x+9jlVbvJ+7/PLLq33HexqrQYMGxquvvup2XWxs7Fnfuz+PtXjxYiMsLMyw2+2uaw4dOmRYLBYjPz//rLkZxul/G9TlM/DzWD/ly2cg0E79t6F/w9uMAdGjAtr6N7zNkOT2Ow4Fv7gRiOPHj6ugoECpqamuY2FhYUpNTdXq1asDmFl1p4anY2Nj6xSnqqpK8+bNU3l5uXr27OlznMzMTA0aNMjtZ+erb775RomJiWrVqpVGjBhR49QKTyxcuFBXXnmlbrrpJsXHx6tr167617/+Vef8pJPvlddee0133nnnWf+yVpNevXppyZIl2rp1qyTp888/16pVq6r9tcoTJ06cUFVVVbW/IEdFRfk8enPKzp07VVJS4vZ7tdls6tGjR1B+JiwWixo1alTnWMePH9fLL78sm82mLl26eP18p9Op22+/XQ8//LA6duxY53wkafny5YqPj1fbtm1177336ocffvAprw8++ECXXnqpBgwYoPj4ePXo0aNO0ytP2b9/vz744IMa//LtqV69emnhwoXas2ePDMPQsmXLtHXrVqWlpXkVx+FwSJLbZyIsLExWq9Wjz8TPv18LCgpUWVnp9jlo166dmjdvftbPgVnf1d7EO3VLx4iIMy9FPFus8vJyzZo1Sy1btlRSUpLXsSoqKnTbbbdp2rRpSkhIOOPzPc3t9ddf10UXXaROnTopOztbFRUVXscqLS3V2rVrFR8fr169eqlJkya69tprPf6+PNvPraCgQIWFhR59FmqK1atXL7355ps6ePCgnE6n5s2bp2PHjiklJcWrWA6HQxaLxW2/hcjISIWFhZ31tf783wZ1+QyY9e8MnD9+cQXEgQMHVFVVpSZNmrgdb9KkiUpKSgKUVXVOp1Pjx49X79691alTJ59ibNq0SQ0bNpTVatU999yj3NxcdejQwadY8+bN08aNG5WTk+PT83+qR48eriH46dOna+fOnfrVr36lw4cPex1rx44dmj59ui655BItXrxY9957r+677z7NmTOnznkuWLBAhw4d0qhRo3x6/sSJE3XLLbeoXbt2qlevnrp27arx48drxIgRXseKjo5Wz5499eSTT2rv3r2qqqrSa6+9ptWrV2vfvn0+5XfKqfd9sH8mjh07pkceeUS33nprne6FvWjRIjVs2FCRkZF64YUXlJ+fr4suusjrOM8884wiIiJ03333+ZzLT6Wnp+vVV1/VkiVL9Mwzz2jFihXKyMioNq3kbEpLS3XkyBE9/fTTSk9P13/+8x/dcMMNuvHGG7VixYo65ThnzhxFR0fXOqXBE1OnTlWHDh3UrFkz1a9fX+np6Zo2bZr69OnjVZxT/7DJzs7Wjz/+qOPHj+uZZ57Rd999d9bPRE3fryUlJapfv3614vRsnwMzvqu9jXfgwAE9+eSTGjt2rM+x/vGPf6hhw4Zq2LChPvroI+Xn56t+/drvd19brAceeEC9evXSkCFDvHiVtce77bbb9Nprr2nZsmXKzs7Wv//9b/3mN7/xOtaOHTsknVz3NGbMGOXl5emKK65Q//79a10nc7bcfmrmzJlq3769evXq5VOst956S5WVlWrcuLGsVqt++9vfKjc3V23atPEq1tVXX60GDRrokUceUUVFhcrLy/XQQw+pqqqq1s9Bbf828OUzYOa/M4KR4XQGRQtF3IXJTzIzM7X5/7d3/zFR138cwJ+Gd/wagcDh3Ul3AUdgxo+ABNfCGUWhkzQbRKwoXHOFDamuomLUltofaJK6lIasX2wIE0LWMoH48a3Mwm7iH6JcBmkIo7IzISF4f/9gd1O54z6fA798xedju82d48nr4N6f+7zfn/fnxYkT01pdjoiIgMlkwl9//YWamhrk5OSgtbVV9uD+9ddfkZ+fj8OHD9vdQy3XlSvw0dHRSExMhF6vx/79+2WvbI6PjyMhIQFbtmwBANx99904ceIE9uzZg5ycnGnVWV5ejrS0NKd72x3Zv38/PvvsM1RWVmLJkiUwmUzYtGkTtFqtS7V98sknyM3NxaJFi+Dm5oa4uDhkZWWho6PDpfpuJKOjo8jIyIAQAh988MG0slasWAGTyYTBwUF8+OGHyMjIsK1UStXR0YHS0lIcO3bMpatT9jz++OO2f0dFRSE6OhphYWFoaWlBSkqK5BzrvudHHnkEBQUFAIDY2Fh8++232LNnD5YvX+5yjfv27UN2dva0jgM7d+7EkSNHUF9fD71ej7a2NuTl5UGr1cq6uqlQKHDgwAGsX78e/v7+cHNzwwMPPIC0tDSnjQpm4vh6PbKk5FksFqxatQp33nkn3nrrLZezsrOz8eCDD6Kvrw8lJSXIyMjAN9984/B3ay+rvr4ezc3Nk/btS+GotisnRVFRUdBoNEhJSYHZbEZYWJjkLOs42LBhA5555hkAE58PTU1N2Ldv35SLYc5+B8PDw6isrERRUZHLr7OoqAgXLlxAY2MjAgMDUVdXh4yMDLS3tyMqKkpylkqlQnV1NZ577jm8//77uOWWW5CVlYW4uDi793oAjs8NXDFT5xl087nhJhCBgYFwc3Ob1FWgv79f9uXX62Xjxo1oaGhAW1sbgoODXc5RKpW21Yz4+Hj88MMPKC0txd69e2XldHR0YGBgAHFxcbbnxsbG0NbWhl27duHy5ctwc3NzuU4/Pz/ccccd6O7ulv21Go1m0oFq8eLFdrtZyNHT04PGxkYcOHDA5Qyj0Wi7CgFMfBj29PRg69atLk0gwsLC0NraikuXLsFisUCj0SAzMxOhoaEu1wjA9r7v7++HRqOxPd/f34/Y2NhpZc8E6+Shp6cHzc3N0/5LnN7e3jAYDDAYDEhKSkJ4eDjKy8tRWFgoOaO9vR0DAwPQ6XS258bGxvDSSy9hx44d+OWXX6ZVIwCEhoYiMDAQ3d3dsiYQgYGBmD9/vt1xMZ2T3Pb2dnR1daGqqsrljOHhYbz++uuora3FqlWrAEwsIphMJpSUlMjeHhkfH287eRkZGYFKpUJiYiISEhIcfo2j46tarcbIyAguXLhw1QrsVJ8NM3Wslpp38eJFPPzww/Dx8UFtbS0UCoXLWb6+vvD19UV4eDiSkpKwYMEC1NbWIisrS3JWc3MzzGbzpBXrdevW4b777kNLS4tLtV0pMTERANDd3W13AuEoy3osszcOptoyK6W2mpoaDA0N4amnnpqydkdZZrMZu3btwokTJ2zbH2NiYtDe3o7du3fb7Uo2VV2pqakwm80YHBzE/Pnz4efnB7Va7fCzwdG5QWZmpuwxMFPnGXTzueG2MCmVSsTHx6Opqcn23Pj4OJqammZ9354QAhs3bkRtbS2am5sREhIyo/nj4+O2fcNypKSkoLOzEyaTyfZISEhAdnY2TCbTtCYPwERbR7PZfNXJq1T33nvvpJZ2p06dgl6vn1ZNFRUVCAoKsp3kuGJoaGjSCpCbm5vdzhhyeHt7Q6PR4M8//8ShQ4dkbx24VkhICNRq9VVjwmKx4Pvvv5/1MWGdPJw+fRqNjY0ICAiY8e/hyrh48skncfz48avGhFarhdFoxKFDh2akrrNnz+L333+XPS6USiXuueeeGR8X5eXliI+Pd+l+EavR0VGMjo7O+Ljw9fWFSqXC6dOn8eOPP9odE86Or/Hx8VAoFFeNg66uLvT29k4aBzN9rJaSZ7FYkJqaCqVSifr6eodXClypTQgBIcSkceAs67XXXps0DgDgvffeQ0VFxYzUZs28dhw4y7r99tuh1WoljwM5tZWXlyM9PR0qlcru/zvLst7TIWUcyKkrMDAQfn5+aG5uxsDAwJQd665kPQbKGQPOsuYMIf4/HnPQDXcFAphoI5iTk4OEhAQsXboUO3bswKVLl2yXOeX4+++/r1o5P3PmDEwmE/z9/a9anZQiLy8PlZWV+Pzzz+Hj42Pbc+jr6wtPT09ZWYWFhUhLS4NOp8PFixdRWVmJlpYWl05ufHx8Ju0D9fb2RkBAgEt7fl9++WWsXr0aer0ev/32G4qLi+Hm5mZ35csZ6/7bLVu2ICMjA0ePHkVZWRnKyspkZ1mNj4+joqICOTk5Tm9QnMrq1auxefNm6HQ6LFmyBD/99BO2b9+O3Nxcl/Ks7RYjIiLQ3d0No9GIyMhISe9bZ+/TTZs24Z133kF4eDhCQkJQVFQErVaLNWvWyM76448/0Nvba/t7DdYPcLVaPWkVa6osjUaDxx57DMeOHUNDQwPGxsZsY8Lf39/ufu2p8gICArB582akp6dDo9FgcHAQu3fvxrlz5+y26XX2Oq+dzCgUCqjVakREREz+BTjJ8/f3x9tvv41169ZBrVbDbDbjlVdegcFgwEMPPSS7NqPRiMzMTCQnJ2PFihX48ssvcfDgQbsrwlKOYRaLBdXV1di2bZvd1yYnb/ny5TAajfD09IRer0drays+/vhjbN++XXZWdXU1VCoVdDodOjs7kZ+fjzVr1ti9IdvZ8dXX1xfr16/Hiy++CH9/f9x666144YUXsGzZMiQlJcnKAibuqTh//ryt/s7OTvj4+ECn0026MddZnnXyMDQ0hE8//RQWiwUWiwXAxBaWKxdxnGX9/PPPqKqqQmpqKlQqFc6ePYt3330Xnp6eWLlypay67I1pANDpdHZPdJ3lmc1mVFZWYuXKlQgICMDx48dRUFCA5OTkSa3DnWXNmzcPRqMRxcXFiImJQWxsLD766COcPHkSNTU1smuz6u7uRltbG7744otJGVKzIiMjYTAYsGHDBpSUlCAgIAB1dXU4fPgwGhoaZNdVUVGBxYsXQ6VS4bvvvkN+fj4KCgrsHoumOjeQMwacZQHyxgDdhP7nfZ9myM6dO4VOpxNKpVIsXbpUHDlyxKUca3uyax85OTmys+zlABAVFRWys3Jzc4VerxdKpVKoVCqRkpIivvrqK9k5jkynjWtmZqbQaDRCqVSKRYsWiczMTKftA6dy8OBBcddddwl3d3cRGRkpysrKXM4SYqItHgDR1dU1rRyLxSLy8/OFTqcTHh4eIjQ0VLzxxhvi8uXLLuVVVVWJ0NBQoVQqhVqtFnl5eeLChQuSvtbZ+3R8fFwUFRWJhQsXCnd3d5GSkuLw9TvLqqiosPv/xcXFsrKsbWDtPb7++mvZtQ0PD4u1a9cKrVYrlEql0Gg0Ij09XRw9etSl13ktZ21cp8obGhoSqampQqVSCYVCIfR6vXj22WfF+fPnXa6tvLxcGAwG4eHhIWJiYkRdXZ3LWXv37hWenp6S3m/O8vr6+sTTTz8ttFqt8PDwEBEREWLbtm12WyU7yyotLRXBwcFCoVAInU4n3nzzTYfjS8rxdXh4WDz//PNiwYIFwsvLS6xdu1b09fW5lFVcXCz5eO4sz9HPAYA4c+aMrKxz586JtLQ0ERQUJBQKhQgODhZPPPGEOHnypEuv097XOGrj6iyvt7dXJCcnC39/f+Hu7i4MBoMwGo1221dKrW3r1q0iODhYeHl5iWXLlon29naXarMqLCwUt912mxgbG5vyZ+As69SpU+LRRx8VQUFBwsvLS0RHR09q6yo169VXXxULFy4UCoVChIeHOxxPQjg/N5A6BqRkyRkD/2+sbVzv98wUqV5Pzurjfs9MAcy9Nq7zhJij11aIiIiI6KZjsVjg6+uL+90zMH+e4+5k/wv/ihE0X95va908V9xw90AQEREREdHsuSHvgSAiIiIimpIQAGb57zDM0Y0+vAJBRERERESScQJBRERERESScQsTEREREc05YlxAzJvdLURztVcRr0AQEREREZFknEAQEREREZFk3MJERERERHOPGMfsd2Ga5e9/nfAKBBERERERScYJBBERERERScYtTEREREQ057AL0/XDKxBERERERCQZr0AQERER0dzDm6ivG16BICIiIiIiyXgFgoiIiIjmnH8xCszyLQj/YnR2C7hOOIEgIiIiojlDqVRCrVbjP+e/mO1SAABqtRpKpXK2y5hR88RcvT2ciIiIiG5K//zzD0ZGRma7DAATExoPD4/ZLmNGcQJBRERERESS8SZqIiIiIiKSjBMIIiIiIiKSjBMIIiIiIiKSjBMIIiIiIiKSjBMIIiIiIiKSjBMIIiIiIiKSjBMIIiIiIiKS7L/O7oM6gpSkmQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 1000x1000 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "config = load_config('src/configs/mix1_mws_mwp_2head.yaml')\n",
    "\n",
    "# Set device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Set seeds\n",
    "seed = getattr(config.train, \"seed\", 42)\n",
    "set_seed(seed)\n",
    "\n",
    "# Model settings -- May needs tweaking\n",
    "config.model.vocab_size = max(getattr(config.data, \"p\", 16), config.data.max_num) + 1\n",
    "config.model.block_size = 2 * config.data.num_tokens + 1\n",
    "\n",
    "# Prepare data samplers\n",
    "data_samplers = prepare_data_samplers(config)\n",
    "\n",
    "# Initialize model\n",
    "if config.model.linear:\n",
    "    model = GPTLinear(config.model, return_att=True).to(device)\n",
    "else:\n",
    "    model = GPTSoftmax(config.model, return_att=True).to(device)\n",
    "\n",
    "if config.train.freeze_embedding:\n",
    "    for param in model.transformer.wte.parameters():\n",
    "        param.requires_grad = False\n",
    "    for param in model.transformer.wpe.parameters():\n",
    "        param.requires_grad = False\n",
    "# Optimizer\n",
    "optim = Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=config.train.lr)\n",
    "\n",
    "# WandB setup\n",
    "if getattr(config.train, \"wandb\", False):\n",
    "    wandb_run_name = getattr(config.train, \"wandb_run_name\", None)\n",
    "    wandb.login(key=\"\")\n",
    "    wandb.init(project=config.train.wandb_project, name=wandb_run_name, config=config)\n",
    "    wandb.watch(model)\n",
    "\n",
    "# Training loop\n",
    "for step in range(config.train.num_steps):\n",
    "    train_step(\n",
    "        model=model,\n",
    "        optim=optim,\n",
    "        data_samplers=data_samplers,\n",
    "        step=step,\n",
    "        config=config,\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "if getattr(config.train, \"wandb\", False):\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_samplers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train, n_test, num_tokens = (\n",
    "    config.data.n_train,\n",
    "    config.data.n_test,\n",
    "    config.data.num_tokens,\n",
    ")\n",
    "\n",
    "task_names = list(data_samplers.keys())\n",
    "n_tasks = len(task_names)\n",
    "\n",
    "n_train_each = n_train // n_tasks\n",
    "n_test_each = n_test // n_tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixed_train = {}\n",
    "mixed_test = {}\n",
    "\n",
    "for name, sampler in data_samplers.items():\n",
    "    data = sampler.sample(\n",
    "        num_samples=n_train_each + n_test_each,\n",
    "        num_tokens=num_tokens,\n",
    "    )\n",
    "    train_part = data[:n_train_each, :]\n",
    "    test_part = data[n_train_each:, :]\n",
    "    mixed_train[name] = train_part\n",
    "    mixed_test[name] = test_part\n",
    "\n",
    "train_data = torch.cat(list(mixed_train.values()), dim=0)\n",
    "test_data = torch.cat(list(mixed_test.values()), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
