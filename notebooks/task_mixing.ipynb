{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "YiF5Vq1LGhEw"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import yaml\n",
    "import argparse\n",
    "from dotmap import DotMap\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.optim import Adam\n",
    "from torch.nn.functional import cosine_similarity\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"./src\")  # make sure Python can find src/\n",
    "import data\n",
    "from model_linear import GPTLinear\n",
    "from model_softmax import GPTSoftmax\n",
    "# from train_step import train_step\n",
    "from multi_task_train import train_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "config = {\n",
    "'model':\n",
    "  {\n",
    "    'n_layer': 1,\n",
    "    'n_head': 1,\n",
    "    'n_embd': 256,\n",
    "    'linear': True,\n",
    "  },\n",
    "\n",
    "'data':\n",
    "  {\n",
    "    'name': 'window',\n",
    "    'min_num': 1,\n",
    "    'max_num': 16,\n",
    "    'k': 2,\n",
    "    'p': 17,\n",
    "    'sep': 17,\n",
    "    'cot': False,\n",
    "    'num_tokens': 16,\n",
    "    'n_train': 256,\n",
    "    'n_test': 64,\n",
    "    'fixed_len': True,\n",
    "  },\n",
    "\n",
    "'train':\n",
    "  {\n",
    "    'lr': 0.0001,\n",
    "    'grad_clip': -1,\n",
    "    'num_steps': 500,\n",
    "    'norm_type': \"none_rank\",\n",
    "    'wandb': True,\n",
    "    'save_ckpt': False,\n",
    "    'ckpt_freq': 20,\n",
    "    'seed' = 67,\n",
    "  }\n",
    "}\n",
    "\n",
    "\n",
    "config = DotMap(config)\n",
    "config.model.vocab_size = max(config.data.p, config.data.max_num) + 1\n",
    "config.model.block_size = 2 * config.data.num_tokens + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MWP Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jyue/.conda/envs/emerge/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjetyue04\u001b[0m (\u001b[33mwth_ucsd\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.22.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jyue/private/tf-loss-plateau/wandb/run-20251106_230932-sb9puhni</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/wth_ucsd/loss_plateau_tf/runs/sb9puhni' target=\"_blank\">mwp_linear</a></strong> to <a href='https://wandb.ai/wth_ucsd/loss_plateau_tf' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/wth_ucsd/loss_plateau_tf' target=\"_blank\">https://wandb.ai/wth_ucsd/loss_plateau_tf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/wth_ucsd/loss_plateau_tf/runs/sb9puhni' target=\"_blank\">https://wandb.ai/wth_ucsd/loss_plateau_tf/runs/sb9puhni</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0 -- Train loss: 2.8693161010742188, Train Acc: 0.068603515625 Test Acc: 0.060546875\n",
      "Step 1 -- Train loss: 2.8518381118774414, Train Acc: 0.069091796875 Test Acc: 0.05078125\n",
      "Step 2 -- Train loss: 2.838197946548462, Train Acc: 0.075439453125 Test Acc: 0.0732421875\n",
      "Step 3 -- Train loss: 2.8244495391845703, Train Acc: 0.0673828125 Test Acc: 0.0703125\n",
      "Step 4 -- Train loss: 2.8101773262023926, Train Acc: 0.06640625 Test Acc: 0.072265625\n",
      "Step 5 -- Train loss: 2.803687572479248, Train Acc: 0.065673828125 Test Acc: 0.0849609375\n",
      "Step 6 -- Train loss: 2.7909626960754395, Train Acc: 0.07861328125 Test Acc: 0.0751953125\n",
      "Step 7 -- Train loss: 2.786344528198242, Train Acc: 0.079833984375 Test Acc: 0.076171875\n",
      "Step 8 -- Train loss: 2.7790911197662354, Train Acc: 0.078857421875 Test Acc: 0.091796875\n",
      "Step 9 -- Train loss: 2.773648500442505, Train Acc: 0.09521484375 Test Acc: 0.0869140625\n",
      "Step 10 -- Train loss: 2.767432689666748, Train Acc: 0.093994140625 Test Acc: 0.0888671875\n",
      "Step 11 -- Train loss: 2.7600061893463135, Train Acc: 0.10205078125 Test Acc: 0.09375\n",
      "Step 12 -- Train loss: 2.753516912460327, Train Acc: 0.105224609375 Test Acc: 0.0947265625\n",
      "Step 13 -- Train loss: 2.7517220973968506, Train Acc: 0.110595703125 Test Acc: 0.1015625\n",
      "Step 14 -- Train loss: 2.7418718338012695, Train Acc: 0.110107421875 Test Acc: 0.119140625\n",
      "Step 15 -- Train loss: 2.7331650257110596, Train Acc: 0.115234375 Test Acc: 0.119140625\n",
      "Step 16 -- Train loss: 2.7236852645874023, Train Acc: 0.111083984375 Test Acc: 0.119140625\n",
      "Step 17 -- Train loss: 2.714409351348877, Train Acc: 0.115234375 Test Acc: 0.1201171875\n",
      "Step 18 -- Train loss: 2.702641725540161, Train Acc: 0.1181640625 Test Acc: 0.1181640625\n",
      "Step 19 -- Train loss: 2.6921844482421875, Train Acc: 0.1240234375 Test Acc: 0.115234375\n",
      "Step 20 -- Train loss: 2.6844992637634277, Train Acc: 0.12744140625 Test Acc: 0.119140625\n",
      "Step 21 -- Train loss: 2.679619550704956, Train Acc: 0.117919921875 Test Acc: 0.1240234375\n",
      "Step 22 -- Train loss: 2.6676034927368164, Train Acc: 0.1220703125 Test Acc: 0.12109375\n",
      "Step 23 -- Train loss: 2.665066957473755, Train Acc: 0.126953125 Test Acc: 0.1201171875\n",
      "Step 24 -- Train loss: 2.6601366996765137, Train Acc: 0.119873046875 Test Acc: 0.1279296875\n",
      "Step 25 -- Train loss: 2.6534955501556396, Train Acc: 0.1201171875 Test Acc: 0.1220703125\n",
      "Step 26 -- Train loss: 2.648939609527588, Train Acc: 0.116943359375 Test Acc: 0.115234375\n",
      "Step 27 -- Train loss: 2.645327091217041, Train Acc: 0.122314453125 Test Acc: 0.119140625\n",
      "Step 28 -- Train loss: 2.643921375274658, Train Acc: 0.11767578125 Test Acc: 0.1201171875\n",
      "Step 29 -- Train loss: 2.6364142894744873, Train Acc: 0.12060546875 Test Acc: 0.109375\n",
      "Step 30 -- Train loss: 2.63529372215271, Train Acc: 0.120849609375 Test Acc: 0.126953125\n",
      "Step 31 -- Train loss: 2.6334517002105713, Train Acc: 0.119873046875 Test Acc: 0.1298828125\n",
      "Step 32 -- Train loss: 2.6344292163848877, Train Acc: 0.1171875 Test Acc: 0.126953125\n",
      "Step 33 -- Train loss: 2.6285183429718018, Train Acc: 0.123779296875 Test Acc: 0.125\n",
      "Step 34 -- Train loss: 2.623494863510132, Train Acc: 0.125732421875 Test Acc: 0.125\n",
      "Step 35 -- Train loss: 2.6313180923461914, Train Acc: 0.113525390625 Test Acc: 0.1171875\n",
      "Step 36 -- Train loss: 2.6255762577056885, Train Acc: 0.12158203125 Test Acc: 0.1220703125\n",
      "Step 37 -- Train loss: 2.6258344650268555, Train Acc: 0.121337890625 Test Acc: 0.1220703125\n",
      "Step 38 -- Train loss: 2.627347946166992, Train Acc: 0.119140625 Test Acc: 0.1259765625\n",
      "Step 39 -- Train loss: 2.6219730377197266, Train Acc: 0.120361328125 Test Acc: 0.1083984375\n",
      "Step 40 -- Train loss: 2.6226091384887695, Train Acc: 0.12158203125 Test Acc: 0.119140625\n",
      "Step 41 -- Train loss: 2.6183102130889893, Train Acc: 0.1220703125 Test Acc: 0.1279296875\n",
      "Step 42 -- Train loss: 2.6203839778900146, Train Acc: 0.122802734375 Test Acc: 0.12890625\n",
      "Step 43 -- Train loss: 2.6200764179229736, Train Acc: 0.12451171875 Test Acc: 0.111328125\n",
      "Step 44 -- Train loss: 2.617219924926758, Train Acc: 0.1181640625 Test Acc: 0.1279296875\n",
      "Step 45 -- Train loss: 2.6205217838287354, Train Acc: 0.124755859375 Test Acc: 0.11328125\n",
      "Step 46 -- Train loss: 2.6178336143493652, Train Acc: 0.123046875 Test Acc: 0.1171875\n",
      "Step 47 -- Train loss: 2.615546464920044, Train Acc: 0.12353515625 Test Acc: 0.1259765625\n",
      "Step 48 -- Train loss: 2.619750499725342, Train Acc: 0.119384765625 Test Acc: 0.1240234375\n",
      "Step 49 -- Train loss: 2.6170310974121094, Train Acc: 0.116943359375 Test Acc: 0.1201171875\n",
      "Step 50 -- Train loss: 2.614349603652954, Train Acc: 0.1220703125 Test Acc: 0.1142578125\n",
      "Step 51 -- Train loss: 2.6161441802978516, Train Acc: 0.1220703125 Test Acc: 0.1337890625\n",
      "Step 52 -- Train loss: 2.616114377975464, Train Acc: 0.115478515625 Test Acc: 0.115234375\n",
      "Step 53 -- Train loss: 2.615920305252075, Train Acc: 0.12890625 Test Acc: 0.125\n",
      "Step 54 -- Train loss: 2.614152431488037, Train Acc: 0.1298828125 Test Acc: 0.1201171875\n",
      "Step 55 -- Train loss: 2.6143643856048584, Train Acc: 0.124755859375 Test Acc: 0.119140625\n",
      "Step 56 -- Train loss: 2.6108648777008057, Train Acc: 0.123046875 Test Acc: 0.11328125\n",
      "Step 57 -- Train loss: 2.6118273735046387, Train Acc: 0.134033203125 Test Acc: 0.1201171875\n",
      "Step 58 -- Train loss: 2.6150758266448975, Train Acc: 0.123046875 Test Acc: 0.126953125\n",
      "Step 59 -- Train loss: 2.61506724357605, Train Acc: 0.118408203125 Test Acc: 0.12890625\n",
      "Step 60 -- Train loss: 2.6117565631866455, Train Acc: 0.123779296875 Test Acc: 0.109375\n",
      "Step 61 -- Train loss: 2.613232135772705, Train Acc: 0.1240234375 Test Acc: 0.126953125\n",
      "Step 62 -- Train loss: 2.612306833267212, Train Acc: 0.12060546875 Test Acc: 0.125\n",
      "Step 63 -- Train loss: 2.6123995780944824, Train Acc: 0.12060546875 Test Acc: 0.125\n",
      "Step 64 -- Train loss: 2.6127941608428955, Train Acc: 0.11669921875 Test Acc: 0.126953125\n",
      "Step 65 -- Train loss: 2.6132545471191406, Train Acc: 0.123779296875 Test Acc: 0.1220703125\n",
      "Step 66 -- Train loss: 2.612401008605957, Train Acc: 0.1171875 Test Acc: 0.142578125\n",
      "Step 67 -- Train loss: 2.609248638153076, Train Acc: 0.126708984375 Test Acc: 0.125\n",
      "Step 68 -- Train loss: 2.612292528152466, Train Acc: 0.115478515625 Test Acc: 0.125\n",
      "Step 69 -- Train loss: 2.6101198196411133, Train Acc: 0.116455078125 Test Acc: 0.126953125\n",
      "Step 70 -- Train loss: 2.6107077598571777, Train Acc: 0.12548828125 Test Acc: 0.1201171875\n",
      "Step 71 -- Train loss: 2.6073012351989746, Train Acc: 0.123779296875 Test Acc: 0.103515625\n",
      "Step 72 -- Train loss: 2.6094086170196533, Train Acc: 0.123291015625 Test Acc: 0.1171875\n",
      "Step 73 -- Train loss: 2.610746383666992, Train Acc: 0.122314453125 Test Acc: 0.1318359375\n",
      "Step 74 -- Train loss: 2.6092207431793213, Train Acc: 0.1220703125 Test Acc: 0.1181640625\n",
      "Step 75 -- Train loss: 2.6095924377441406, Train Acc: 0.123291015625 Test Acc: 0.1201171875\n",
      "Step 76 -- Train loss: 2.609882354736328, Train Acc: 0.117431640625 Test Acc: 0.12890625\n",
      "Step 77 -- Train loss: 2.6097640991210938, Train Acc: 0.124267578125 Test Acc: 0.1142578125\n",
      "Step 78 -- Train loss: 2.6072261333465576, Train Acc: 0.125 Test Acc: 0.115234375\n",
      "Step 79 -- Train loss: 2.6074492931365967, Train Acc: 0.122802734375 Test Acc: 0.1220703125\n",
      "Step 80 -- Train loss: 2.6097090244293213, Train Acc: 0.12158203125 Test Acc: 0.1279296875\n",
      "Step 81 -- Train loss: 2.6086108684539795, Train Acc: 0.12109375 Test Acc: 0.12890625\n",
      "Step 82 -- Train loss: 2.6087493896484375, Train Acc: 0.121826171875 Test Acc: 0.115234375\n",
      "Step 83 -- Train loss: 2.607855796813965, Train Acc: 0.1201171875 Test Acc: 0.1142578125\n",
      "Step 84 -- Train loss: 2.6081788539886475, Train Acc: 0.119873046875 Test Acc: 0.1357421875\n",
      "Step 85 -- Train loss: 2.6067895889282227, Train Acc: 0.11669921875 Test Acc: 0.1142578125\n",
      "Step 86 -- Train loss: 2.610661745071411, Train Acc: 0.118408203125 Test Acc: 0.1201171875\n",
      "Step 87 -- Train loss: 2.607199192047119, Train Acc: 0.123779296875 Test Acc: 0.1240234375\n",
      "Step 88 -- Train loss: 2.6091809272766113, Train Acc: 0.121826171875 Test Acc: 0.1123046875\n",
      "Step 89 -- Train loss: 2.6084799766540527, Train Acc: 0.121826171875 Test Acc: 0.1396484375\n",
      "Step 90 -- Train loss: 2.609644651412964, Train Acc: 0.117919921875 Test Acc: 0.1318359375\n",
      "Step 91 -- Train loss: 2.6076624393463135, Train Acc: 0.12548828125 Test Acc: 0.1005859375\n",
      "Step 92 -- Train loss: 2.6065547466278076, Train Acc: 0.12451171875 Test Acc: 0.123046875\n",
      "Step 93 -- Train loss: 2.6073503494262695, Train Acc: 0.120361328125 Test Acc: 0.1240234375\n",
      "Step 94 -- Train loss: 2.6093616485595703, Train Acc: 0.1279296875 Test Acc: 0.12109375\n",
      "Step 95 -- Train loss: 2.606638193130493, Train Acc: 0.123779296875 Test Acc: 0.1220703125\n",
      "Step 96 -- Train loss: 2.6081790924072266, Train Acc: 0.12548828125 Test Acc: 0.1279296875\n",
      "Step 97 -- Train loss: 2.6076414585113525, Train Acc: 0.126708984375 Test Acc: 0.1142578125\n",
      "Step 98 -- Train loss: 2.606959581375122, Train Acc: 0.12255859375 Test Acc: 0.1376953125\n",
      "Step 99 -- Train loss: 2.6064200401306152, Train Acc: 0.11767578125 Test Acc: 0.123046875\n",
      "Step 100 -- Train loss: 2.6057515144348145, Train Acc: 0.12744140625 Test Acc: 0.1201171875\n",
      "Step 101 -- Train loss: 2.607023000717163, Train Acc: 0.127685546875 Test Acc: 0.1201171875\n",
      "Step 102 -- Train loss: 2.6057701110839844, Train Acc: 0.123291015625 Test Acc: 0.119140625\n",
      "Step 103 -- Train loss: 2.6051671504974365, Train Acc: 0.126220703125 Test Acc: 0.125\n",
      "Step 104 -- Train loss: 2.6064722537994385, Train Acc: 0.125 Test Acc: 0.12109375\n",
      "Step 105 -- Train loss: 2.6051862239837646, Train Acc: 0.128173828125 Test Acc: 0.1318359375\n",
      "Step 106 -- Train loss: 2.6060683727264404, Train Acc: 0.128662109375 Test Acc: 0.1298828125\n",
      "Step 107 -- Train loss: 2.607487440109253, Train Acc: 0.119384765625 Test Acc: 0.123046875\n",
      "Step 108 -- Train loss: 2.6033213138580322, Train Acc: 0.131103515625 Test Acc: 0.1181640625\n",
      "Step 109 -- Train loss: 2.6044187545776367, Train Acc: 0.126708984375 Test Acc: 0.1435546875\n",
      "Step 110 -- Train loss: 2.5984158515930176, Train Acc: 0.125732421875 Test Acc: 0.1318359375\n",
      "Step 111 -- Train loss: 2.6019229888916016, Train Acc: 0.12841796875 Test Acc: 0.1259765625\n",
      "Step 112 -- Train loss: 2.6042280197143555, Train Acc: 0.127685546875 Test Acc: 0.1171875\n",
      "Step 113 -- Train loss: 2.6046080589294434, Train Acc: 0.11767578125 Test Acc: 0.1259765625\n",
      "Step 114 -- Train loss: 2.6014950275421143, Train Acc: 0.126220703125 Test Acc: 0.13671875\n",
      "Step 115 -- Train loss: 2.5995588302612305, Train Acc: 0.128662109375 Test Acc: 0.126953125\n",
      "Step 116 -- Train loss: 2.6013240814208984, Train Acc: 0.126953125 Test Acc: 0.119140625\n",
      "Step 117 -- Train loss: 2.594193458557129, Train Acc: 0.132568359375 Test Acc: 0.1181640625\n",
      "Step 118 -- Train loss: 2.6007697582244873, Train Acc: 0.125 Test Acc: 0.1259765625\n",
      "Step 119 -- Train loss: 2.602280378341675, Train Acc: 0.12353515625 Test Acc: 0.123046875\n",
      "Step 120 -- Train loss: 2.5984466075897217, Train Acc: 0.12158203125 Test Acc: 0.1240234375\n",
      "Step 121 -- Train loss: 2.5954437255859375, Train Acc: 0.129638671875 Test Acc: 0.123046875\n",
      "Step 122 -- Train loss: 2.5994300842285156, Train Acc: 0.125 Test Acc: 0.1162109375\n",
      "Step 123 -- Train loss: 2.602766990661621, Train Acc: 0.128662109375 Test Acc: 0.1259765625\n",
      "Step 124 -- Train loss: 2.5996875762939453, Train Acc: 0.126708984375 Test Acc: 0.1298828125\n",
      "Step 125 -- Train loss: 2.5985074043273926, Train Acc: 0.120849609375 Test Acc: 0.130859375\n",
      "Step 126 -- Train loss: 2.598109006881714, Train Acc: 0.1279296875 Test Acc: 0.1201171875\n",
      "Step 127 -- Train loss: 2.5950183868408203, Train Acc: 0.1298828125 Test Acc: 0.109375\n",
      "Step 128 -- Train loss: 2.592104911804199, Train Acc: 0.130615234375 Test Acc: 0.1376953125\n",
      "Step 129 -- Train loss: 2.5934066772460938, Train Acc: 0.1259765625 Test Acc: 0.1298828125\n",
      "Step 130 -- Train loss: 2.599106550216675, Train Acc: 0.1201171875 Test Acc: 0.1279296875\n",
      "Step 131 -- Train loss: 2.5948078632354736, Train Acc: 0.122802734375 Test Acc: 0.12890625\n",
      "Step 132 -- Train loss: 2.5978825092315674, Train Acc: 0.123046875 Test Acc: 0.1279296875\n",
      "Step 133 -- Train loss: 2.5991568565368652, Train Acc: 0.126708984375 Test Acc: 0.1240234375\n",
      "Step 134 -- Train loss: 2.5912652015686035, Train Acc: 0.125732421875 Test Acc: 0.12109375\n",
      "Step 135 -- Train loss: 2.591843605041504, Train Acc: 0.1240234375 Test Acc: 0.12109375\n",
      "Step 136 -- Train loss: 2.6003477573394775, Train Acc: 0.124755859375 Test Acc: 0.1220703125\n",
      "Step 137 -- Train loss: 2.59915828704834, Train Acc: 0.126220703125 Test Acc: 0.1298828125\n",
      "Step 138 -- Train loss: 2.5985822677612305, Train Acc: 0.131591796875 Test Acc: 0.1279296875\n",
      "Step 139 -- Train loss: 2.593729257583618, Train Acc: 0.126953125 Test Acc: 0.1328125\n",
      "Step 140 -- Train loss: 2.601022243499756, Train Acc: 0.125732421875 Test Acc: 0.111328125\n",
      "Step 141 -- Train loss: 2.596979856491089, Train Acc: 0.119873046875 Test Acc: 0.1259765625\n",
      "Step 142 -- Train loss: 2.5971004962921143, Train Acc: 0.127685546875 Test Acc: 0.125\n",
      "Step 143 -- Train loss: 2.596604585647583, Train Acc: 0.12939453125 Test Acc: 0.1259765625\n",
      "Step 144 -- Train loss: 2.597724437713623, Train Acc: 0.117919921875 Test Acc: 0.1220703125\n",
      "Step 145 -- Train loss: 2.5961122512817383, Train Acc: 0.12890625 Test Acc: 0.1279296875\n",
      "Step 146 -- Train loss: 2.5961766242980957, Train Acc: 0.123291015625 Test Acc: 0.11328125\n",
      "Step 147 -- Train loss: 2.591686487197876, Train Acc: 0.126708984375 Test Acc: 0.125\n",
      "Step 148 -- Train loss: 2.5963566303253174, Train Acc: 0.130126953125 Test Acc: 0.12890625\n",
      "Step 149 -- Train loss: 2.593172311782837, Train Acc: 0.1279296875 Test Acc: 0.1201171875\n",
      "Step 150 -- Train loss: 2.5977935791015625, Train Acc: 0.131103515625 Test Acc: 0.126953125\n",
      "Step 151 -- Train loss: 2.5906505584716797, Train Acc: 0.130615234375 Test Acc: 0.1376953125\n",
      "Step 152 -- Train loss: 2.5972402095794678, Train Acc: 0.12841796875 Test Acc: 0.1337890625\n",
      "Step 153 -- Train loss: 2.591947555541992, Train Acc: 0.12646484375 Test Acc: 0.1376953125\n",
      "Step 154 -- Train loss: 2.59385085105896, Train Acc: 0.1298828125 Test Acc: 0.1298828125\n",
      "Step 155 -- Train loss: 2.598803758621216, Train Acc: 0.1240234375 Test Acc: 0.119140625\n",
      "Step 156 -- Train loss: 2.5955162048339844, Train Acc: 0.125732421875 Test Acc: 0.12109375\n",
      "Step 157 -- Train loss: 2.5976696014404297, Train Acc: 0.123779296875 Test Acc: 0.1201171875\n",
      "Step 158 -- Train loss: 2.5999298095703125, Train Acc: 0.1201171875 Test Acc: 0.126953125\n",
      "Step 159 -- Train loss: 2.591825485229492, Train Acc: 0.134033203125 Test Acc: 0.1279296875\n",
      "Step 160 -- Train loss: 2.593505382537842, Train Acc: 0.12353515625 Test Acc: 0.1220703125\n",
      "Step 161 -- Train loss: 2.5953516960144043, Train Acc: 0.125 Test Acc: 0.12890625\n",
      "Step 162 -- Train loss: 2.5973429679870605, Train Acc: 0.1220703125 Test Acc: 0.1484375\n",
      "Step 163 -- Train loss: 2.5899620056152344, Train Acc: 0.130615234375 Test Acc: 0.126953125\n",
      "Step 164 -- Train loss: 2.5913949012756348, Train Acc: 0.1298828125 Test Acc: 0.138671875\n",
      "Step 165 -- Train loss: 2.5926995277404785, Train Acc: 0.12646484375 Test Acc: 0.123046875\n",
      "Step 166 -- Train loss: 2.5911002159118652, Train Acc: 0.128662109375 Test Acc: 0.11328125\n",
      "Step 167 -- Train loss: 2.5947985649108887, Train Acc: 0.127197265625 Test Acc: 0.1328125\n",
      "Step 168 -- Train loss: 2.5956685543060303, Train Acc: 0.119140625 Test Acc: 0.125\n",
      "Step 169 -- Train loss: 2.5925331115722656, Train Acc: 0.1318359375 Test Acc: 0.134765625\n",
      "Step 170 -- Train loss: 2.594717502593994, Train Acc: 0.1328125 Test Acc: 0.1181640625\n",
      "Step 171 -- Train loss: 2.5936450958251953, Train Acc: 0.125 Test Acc: 0.111328125\n",
      "Step 172 -- Train loss: 2.587303638458252, Train Acc: 0.132568359375 Test Acc: 0.1337890625\n",
      "Step 173 -- Train loss: 2.594855785369873, Train Acc: 0.12060546875 Test Acc: 0.130859375\n",
      "Step 174 -- Train loss: 2.600766181945801, Train Acc: 0.121826171875 Test Acc: 0.134765625\n",
      "Step 175 -- Train loss: 2.5921630859375, Train Acc: 0.12451171875 Test Acc: 0.1201171875\n",
      "Step 176 -- Train loss: 2.5945723056793213, Train Acc: 0.12646484375 Test Acc: 0.125\n",
      "Step 177 -- Train loss: 2.59565806388855, Train Acc: 0.127197265625 Test Acc: 0.1181640625\n",
      "Step 178 -- Train loss: 2.5912110805511475, Train Acc: 0.128173828125 Test Acc: 0.1279296875\n",
      "Step 179 -- Train loss: 2.59784197807312, Train Acc: 0.122314453125 Test Acc: 0.12109375\n",
      "Step 180 -- Train loss: 2.594611406326294, Train Acc: 0.12646484375 Test Acc: 0.126953125\n",
      "Step 181 -- Train loss: 2.592987060546875, Train Acc: 0.1279296875 Test Acc: 0.1328125\n",
      "Step 182 -- Train loss: 2.5959582328796387, Train Acc: 0.12744140625 Test Acc: 0.1240234375\n",
      "Step 183 -- Train loss: 2.5914056301116943, Train Acc: 0.120361328125 Test Acc: 0.1318359375\n",
      "Step 184 -- Train loss: 2.5917720794677734, Train Acc: 0.12158203125 Test Acc: 0.1201171875\n",
      "Step 185 -- Train loss: 2.592449903488159, Train Acc: 0.126708984375 Test Acc: 0.1298828125\n",
      "Step 186 -- Train loss: 2.591395378112793, Train Acc: 0.122314453125 Test Acc: 0.1220703125\n",
      "Step 187 -- Train loss: 2.594832181930542, Train Acc: 0.129638671875 Test Acc: 0.140625\n",
      "Step 188 -- Train loss: 2.5982987880706787, Train Acc: 0.1240234375 Test Acc: 0.126953125\n",
      "Step 189 -- Train loss: 2.5939698219299316, Train Acc: 0.124755859375 Test Acc: 0.12109375\n",
      "Step 190 -- Train loss: 2.5919148921966553, Train Acc: 0.12841796875 Test Acc: 0.1357421875\n",
      "Step 191 -- Train loss: 2.591339349746704, Train Acc: 0.130859375 Test Acc: 0.1318359375\n",
      "Step 192 -- Train loss: 2.5950348377227783, Train Acc: 0.12646484375 Test Acc: 0.1298828125\n",
      "Step 193 -- Train loss: 2.595104694366455, Train Acc: 0.126708984375 Test Acc: 0.138671875\n",
      "Step 194 -- Train loss: 2.5927469730377197, Train Acc: 0.1201171875 Test Acc: 0.1298828125\n",
      "Step 195 -- Train loss: 2.590498208999634, Train Acc: 0.12841796875 Test Acc: 0.1435546875\n",
      "Step 196 -- Train loss: 2.5944008827209473, Train Acc: 0.126953125 Test Acc: 0.123046875\n",
      "Step 197 -- Train loss: 2.591601848602295, Train Acc: 0.128662109375 Test Acc: 0.1142578125\n",
      "Step 198 -- Train loss: 2.5888710021972656, Train Acc: 0.1337890625 Test Acc: 0.1328125\n",
      "Step 199 -- Train loss: 2.5920486450195312, Train Acc: 0.125244140625 Test Acc: 0.123046875\n",
      "Step 200 -- Train loss: 2.5921506881713867, Train Acc: 0.121337890625 Test Acc: 0.130859375\n",
      "Step 201 -- Train loss: 2.5883114337921143, Train Acc: 0.12353515625 Test Acc: 0.11328125\n",
      "Step 202 -- Train loss: 2.5893445014953613, Train Acc: 0.125732421875 Test Acc: 0.1259765625\n",
      "Step 203 -- Train loss: 2.5935707092285156, Train Acc: 0.1279296875 Test Acc: 0.1337890625\n",
      "Step 204 -- Train loss: 2.592369318008423, Train Acc: 0.130126953125 Test Acc: 0.115234375\n",
      "Step 205 -- Train loss: 2.592573404312134, Train Acc: 0.1259765625 Test Acc: 0.125\n",
      "Step 206 -- Train loss: 2.593926429748535, Train Acc: 0.131591796875 Test Acc: 0.1171875\n",
      "Step 207 -- Train loss: 2.59330153465271, Train Acc: 0.13330078125 Test Acc: 0.1357421875\n",
      "Step 208 -- Train loss: 2.5873475074768066, Train Acc: 0.125732421875 Test Acc: 0.119140625\n",
      "Step 209 -- Train loss: 2.597350835800171, Train Acc: 0.127197265625 Test Acc: 0.13671875\n",
      "Step 210 -- Train loss: 2.589158535003662, Train Acc: 0.1328125 Test Acc: 0.1298828125\n",
      "Step 211 -- Train loss: 2.5958516597747803, Train Acc: 0.12451171875 Test Acc: 0.12890625\n",
      "Step 212 -- Train loss: 2.5887908935546875, Train Acc: 0.136962890625 Test Acc: 0.1240234375\n",
      "Step 213 -- Train loss: 2.596421957015991, Train Acc: 0.125 Test Acc: 0.126953125\n",
      "Step 214 -- Train loss: 2.5887539386749268, Train Acc: 0.125 Test Acc: 0.1142578125\n",
      "Step 215 -- Train loss: 2.5945634841918945, Train Acc: 0.123779296875 Test Acc: 0.134765625\n",
      "Step 216 -- Train loss: 2.5933425426483154, Train Acc: 0.127197265625 Test Acc: 0.134765625\n",
      "Step 217 -- Train loss: 2.59405517578125, Train Acc: 0.129638671875 Test Acc: 0.125\n",
      "Step 218 -- Train loss: 2.5919060707092285, Train Acc: 0.126708984375 Test Acc: 0.13671875\n",
      "Step 219 -- Train loss: 2.591193914413452, Train Acc: 0.1318359375 Test Acc: 0.126953125\n",
      "Step 220 -- Train loss: 2.5877745151519775, Train Acc: 0.12353515625 Test Acc: 0.119140625\n",
      "Step 221 -- Train loss: 2.592916250228882, Train Acc: 0.126708984375 Test Acc: 0.134765625\n",
      "Step 222 -- Train loss: 2.585594654083252, Train Acc: 0.12939453125 Test Acc: 0.12890625\n",
      "Step 223 -- Train loss: 2.588331937789917, Train Acc: 0.127685546875 Test Acc: 0.1376953125\n",
      "Step 224 -- Train loss: 2.590360641479492, Train Acc: 0.128173828125 Test Acc: 0.1328125\n",
      "Step 225 -- Train loss: 2.59232759475708, Train Acc: 0.122802734375 Test Acc: 0.107421875\n",
      "Step 226 -- Train loss: 2.593728542327881, Train Acc: 0.13037109375 Test Acc: 0.1259765625\n",
      "Step 227 -- Train loss: 2.5893077850341797, Train Acc: 0.1298828125 Test Acc: 0.1435546875\n",
      "Step 228 -- Train loss: 2.5885653495788574, Train Acc: 0.125732421875 Test Acc: 0.126953125\n",
      "Step 229 -- Train loss: 2.589869976043701, Train Acc: 0.127197265625 Test Acc: 0.1298828125\n",
      "Step 230 -- Train loss: 2.5842182636260986, Train Acc: 0.135009765625 Test Acc: 0.1318359375\n",
      "Step 231 -- Train loss: 2.583709716796875, Train Acc: 0.13623046875 Test Acc: 0.1220703125\n",
      "Step 232 -- Train loss: 2.5910377502441406, Train Acc: 0.126220703125 Test Acc: 0.1396484375\n",
      "Step 233 -- Train loss: 2.5858914852142334, Train Acc: 0.1298828125 Test Acc: 0.126953125\n",
      "Step 234 -- Train loss: 2.5917794704437256, Train Acc: 0.1240234375 Test Acc: 0.12890625\n",
      "Step 235 -- Train loss: 2.587441921234131, Train Acc: 0.1259765625 Test Acc: 0.1171875\n",
      "Step 236 -- Train loss: 2.592761516571045, Train Acc: 0.123779296875 Test Acc: 0.125\n",
      "Step 237 -- Train loss: 2.587137460708618, Train Acc: 0.1328125 Test Acc: 0.1240234375\n",
      "Step 238 -- Train loss: 2.586674451828003, Train Acc: 0.127685546875 Test Acc: 0.123046875\n",
      "Step 239 -- Train loss: 2.5860040187835693, Train Acc: 0.125 Test Acc: 0.1171875\n",
      "Step 240 -- Train loss: 2.592214822769165, Train Acc: 0.1220703125 Test Acc: 0.130859375\n",
      "Step 241 -- Train loss: 2.5842556953430176, Train Acc: 0.139404296875 Test Acc: 0.1220703125\n",
      "Step 242 -- Train loss: 2.5811052322387695, Train Acc: 0.12890625 Test Acc: 0.134765625\n",
      "Step 243 -- Train loss: 2.5861318111419678, Train Acc: 0.13232421875 Test Acc: 0.1357421875\n",
      "Step 244 -- Train loss: 2.5845985412597656, Train Acc: 0.126220703125 Test Acc: 0.1259765625\n",
      "Step 245 -- Train loss: 2.58723783493042, Train Acc: 0.12548828125 Test Acc: 0.1337890625\n",
      "Step 246 -- Train loss: 2.5805015563964844, Train Acc: 0.13330078125 Test Acc: 0.12890625\n",
      "Step 247 -- Train loss: 2.578644275665283, Train Acc: 0.136474609375 Test Acc: 0.123046875\n",
      "Step 248 -- Train loss: 2.587918281555176, Train Acc: 0.1220703125 Test Acc: 0.146484375\n",
      "Step 249 -- Train loss: 2.591305732727051, Train Acc: 0.126708984375 Test Acc: 0.125\n",
      "Step 250 -- Train loss: 2.579761505126953, Train Acc: 0.12939453125 Test Acc: 0.1337890625\n",
      "Step 251 -- Train loss: 2.586385726928711, Train Acc: 0.123046875 Test Acc: 0.12109375\n",
      "Step 252 -- Train loss: 2.5829684734344482, Train Acc: 0.13232421875 Test Acc: 0.12109375\n",
      "Step 253 -- Train loss: 2.582557201385498, Train Acc: 0.135498046875 Test Acc: 0.1181640625\n",
      "Step 254 -- Train loss: 2.5816073417663574, Train Acc: 0.134033203125 Test Acc: 0.1279296875\n",
      "Step 255 -- Train loss: 2.583022117614746, Train Acc: 0.12939453125 Test Acc: 0.138671875\n",
      "Step 256 -- Train loss: 2.5772054195404053, Train Acc: 0.13623046875 Test Acc: 0.126953125\n",
      "Step 257 -- Train loss: 2.58471417427063, Train Acc: 0.1259765625 Test Acc: 0.123046875\n",
      "Step 258 -- Train loss: 2.5859506130218506, Train Acc: 0.1240234375 Test Acc: 0.12890625\n",
      "Step 259 -- Train loss: 2.5828795433044434, Train Acc: 0.129150390625 Test Acc: 0.1259765625\n",
      "Step 260 -- Train loss: 2.585012197494507, Train Acc: 0.126220703125 Test Acc: 0.1142578125\n",
      "Step 261 -- Train loss: 2.5865581035614014, Train Acc: 0.130126953125 Test Acc: 0.1328125\n",
      "Step 262 -- Train loss: 2.5798277854919434, Train Acc: 0.13427734375 Test Acc: 0.1328125\n",
      "Step 263 -- Train loss: 2.579716682434082, Train Acc: 0.1259765625 Test Acc: 0.1181640625\n",
      "Step 264 -- Train loss: 2.582484245300293, Train Acc: 0.1298828125 Test Acc: 0.138671875\n",
      "Step 265 -- Train loss: 2.5777573585510254, Train Acc: 0.134033203125 Test Acc: 0.1279296875\n",
      "Step 266 -- Train loss: 2.580265522003174, Train Acc: 0.132568359375 Test Acc: 0.1201171875\n",
      "Step 267 -- Train loss: 2.5832977294921875, Train Acc: 0.130615234375 Test Acc: 0.1181640625\n",
      "Step 268 -- Train loss: 2.575289011001587, Train Acc: 0.13818359375 Test Acc: 0.1240234375\n",
      "Step 269 -- Train loss: 2.578399181365967, Train Acc: 0.127685546875 Test Acc: 0.12890625\n",
      "Step 270 -- Train loss: 2.576151132583618, Train Acc: 0.1337890625 Test Acc: 0.1181640625\n",
      "Step 271 -- Train loss: 2.5752618312835693, Train Acc: 0.13671875 Test Acc: 0.123046875\n",
      "Step 272 -- Train loss: 2.5719752311706543, Train Acc: 0.135009765625 Test Acc: 0.13671875\n",
      "Step 273 -- Train loss: 2.5792438983917236, Train Acc: 0.13134765625 Test Acc: 0.1298828125\n",
      "Step 274 -- Train loss: 2.574605941772461, Train Acc: 0.13623046875 Test Acc: 0.1298828125\n",
      "Step 275 -- Train loss: 2.5786638259887695, Train Acc: 0.135009765625 Test Acc: 0.130859375\n",
      "Step 276 -- Train loss: 2.572115659713745, Train Acc: 0.135986328125 Test Acc: 0.1201171875\n",
      "Step 277 -- Train loss: 2.574070453643799, Train Acc: 0.12939453125 Test Acc: 0.138671875\n",
      "Step 278 -- Train loss: 2.584240436553955, Train Acc: 0.136474609375 Test Acc: 0.1376953125\n",
      "Step 279 -- Train loss: 2.5778143405914307, Train Acc: 0.134033203125 Test Acc: 0.138671875\n",
      "Step 280 -- Train loss: 2.575042724609375, Train Acc: 0.13916015625 Test Acc: 0.140625\n",
      "Step 281 -- Train loss: 2.5795586109161377, Train Acc: 0.136474609375 Test Acc: 0.15625\n",
      "Step 282 -- Train loss: 2.5710151195526123, Train Acc: 0.13525390625 Test Acc: 0.1240234375\n",
      "Step 283 -- Train loss: 2.5718860626220703, Train Acc: 0.13671875 Test Acc: 0.1279296875\n",
      "Step 284 -- Train loss: 2.575188159942627, Train Acc: 0.1318359375 Test Acc: 0.1240234375\n",
      "Step 285 -- Train loss: 2.5733039379119873, Train Acc: 0.130859375 Test Acc: 0.1416015625\n",
      "Step 286 -- Train loss: 2.5741231441497803, Train Acc: 0.140380859375 Test Acc: 0.125\n",
      "Step 287 -- Train loss: 2.574831247329712, Train Acc: 0.1357421875 Test Acc: 0.1298828125\n",
      "Step 288 -- Train loss: 2.5766217708587646, Train Acc: 0.128173828125 Test Acc: 0.1416015625\n",
      "Step 289 -- Train loss: 2.5670061111450195, Train Acc: 0.139892578125 Test Acc: 0.1240234375\n",
      "Step 290 -- Train loss: 2.5664985179901123, Train Acc: 0.137939453125 Test Acc: 0.1396484375\n",
      "Step 291 -- Train loss: 2.57309627532959, Train Acc: 0.1328125 Test Acc: 0.1416015625\n",
      "Step 292 -- Train loss: 2.57143235206604, Train Acc: 0.13037109375 Test Acc: 0.1328125\n",
      "Step 293 -- Train loss: 2.5675392150878906, Train Acc: 0.133056640625 Test Acc: 0.13671875\n",
      "Step 294 -- Train loss: 2.5617895126342773, Train Acc: 0.137451171875 Test Acc: 0.1435546875\n",
      "Step 295 -- Train loss: 2.5700597763061523, Train Acc: 0.136962890625 Test Acc: 0.15234375\n",
      "Step 296 -- Train loss: 2.570352792739868, Train Acc: 0.137939453125 Test Acc: 0.1376953125\n",
      "Step 297 -- Train loss: 2.5680160522460938, Train Acc: 0.139892578125 Test Acc: 0.130859375\n",
      "Step 298 -- Train loss: 2.560849905014038, Train Acc: 0.1416015625 Test Acc: 0.1328125\n",
      "Step 299 -- Train loss: 2.5705935955047607, Train Acc: 0.147216796875 Test Acc: 0.1357421875\n",
      "Step 300 -- Train loss: 2.5612757205963135, Train Acc: 0.149658203125 Test Acc: 0.1357421875\n",
      "Step 301 -- Train loss: 2.556936264038086, Train Acc: 0.141845703125 Test Acc: 0.1533203125\n",
      "Step 302 -- Train loss: 2.566922426223755, Train Acc: 0.144287109375 Test Acc: 0.16015625\n",
      "Step 303 -- Train loss: 2.56390380859375, Train Acc: 0.141845703125 Test Acc: 0.142578125\n",
      "Step 304 -- Train loss: 2.5568106174468994, Train Acc: 0.145751953125 Test Acc: 0.13671875\n",
      "Step 305 -- Train loss: 2.56371808052063, Train Acc: 0.1435546875 Test Acc: 0.1376953125\n",
      "Step 306 -- Train loss: 2.554598331451416, Train Acc: 0.150146484375 Test Acc: 0.1484375\n",
      "Step 307 -- Train loss: 2.5607218742370605, Train Acc: 0.146240234375 Test Acc: 0.1396484375\n",
      "Step 308 -- Train loss: 2.5540356636047363, Train Acc: 0.143798828125 Test Acc: 0.130859375\n",
      "Step 309 -- Train loss: 2.5631442070007324, Train Acc: 0.1494140625 Test Acc: 0.13671875\n",
      "Step 310 -- Train loss: 2.5577330589294434, Train Acc: 0.13720703125 Test Acc: 0.1416015625\n",
      "Step 311 -- Train loss: 2.552358865737915, Train Acc: 0.14306640625 Test Acc: 0.1396484375\n",
      "Step 312 -- Train loss: 2.5549068450927734, Train Acc: 0.148681640625 Test Acc: 0.1611328125\n",
      "Step 313 -- Train loss: 2.5566248893737793, Train Acc: 0.15625 Test Acc: 0.12890625\n",
      "Step 314 -- Train loss: 2.555375814437866, Train Acc: 0.15234375 Test Acc: 0.16015625\n",
      "Step 315 -- Train loss: 2.555647134780884, Train Acc: 0.147216796875 Test Acc: 0.1357421875\n",
      "Step 316 -- Train loss: 2.5700056552886963, Train Acc: 0.138427734375 Test Acc: 0.146484375\n",
      "Step 317 -- Train loss: 2.558565378189087, Train Acc: 0.1494140625 Test Acc: 0.14453125\n",
      "Step 318 -- Train loss: 2.566251039505005, Train Acc: 0.150634765625 Test Acc: 0.1689453125\n",
      "Step 319 -- Train loss: 2.5509862899780273, Train Acc: 0.149658203125 Test Acc: 0.1494140625\n",
      "Step 320 -- Train loss: 2.5504279136657715, Train Acc: 0.1533203125 Test Acc: 0.1494140625\n",
      "Step 321 -- Train loss: 2.5572848320007324, Train Acc: 0.154541015625 Test Acc: 0.166015625\n",
      "Step 322 -- Train loss: 2.5500309467315674, Train Acc: 0.154296875 Test Acc: 0.1396484375\n",
      "Step 323 -- Train loss: 2.546346426010132, Train Acc: 0.156494140625 Test Acc: 0.14453125\n",
      "Step 324 -- Train loss: 2.5437207221984863, Train Acc: 0.150390625 Test Acc: 0.146484375\n",
      "Step 325 -- Train loss: 2.540152072906494, Train Acc: 0.153564453125 Test Acc: 0.1513671875\n",
      "Step 326 -- Train loss: 2.5389699935913086, Train Acc: 0.155029296875 Test Acc: 0.154296875\n",
      "Step 327 -- Train loss: 2.53963565826416, Train Acc: 0.158935546875 Test Acc: 0.1474609375\n",
      "Step 328 -- Train loss: 2.531832218170166, Train Acc: 0.16650390625 Test Acc: 0.1669921875\n",
      "Step 329 -- Train loss: 2.542144298553467, Train Acc: 0.156982421875 Test Acc: 0.1572265625\n",
      "Step 330 -- Train loss: 2.5286707878112793, Train Acc: 0.16162109375 Test Acc: 0.1640625\n",
      "Step 331 -- Train loss: 2.5320425033569336, Train Acc: 0.1572265625 Test Acc: 0.1640625\n",
      "Step 332 -- Train loss: 2.523768424987793, Train Acc: 0.17138671875 Test Acc: 0.1669921875\n",
      "Step 333 -- Train loss: 2.530855655670166, Train Acc: 0.16845703125 Test Acc: 0.1591796875\n",
      "Step 334 -- Train loss: 2.525606632232666, Train Acc: 0.165771484375 Test Acc: 0.173828125\n",
      "Step 335 -- Train loss: 2.5148873329162598, Train Acc: 0.165771484375 Test Acc: 0.16796875\n",
      "Step 336 -- Train loss: 2.5219621658325195, Train Acc: 0.175537109375 Test Acc: 0.1640625\n",
      "Step 337 -- Train loss: 2.5168039798736572, Train Acc: 0.170166015625 Test Acc: 0.169921875\n",
      "Step 338 -- Train loss: 2.515137195587158, Train Acc: 0.169921875 Test Acc: 0.1689453125\n",
      "Step 339 -- Train loss: 2.507082462310791, Train Acc: 0.174072265625 Test Acc: 0.1689453125\n",
      "Step 340 -- Train loss: 2.508100748062134, Train Acc: 0.174072265625 Test Acc: 0.1689453125\n",
      "Step 341 -- Train loss: 2.501339912414551, Train Acc: 0.173095703125 Test Acc: 0.1806640625\n",
      "Step 342 -- Train loss: 2.4932425022125244, Train Acc: 0.176025390625 Test Acc: 0.1904296875\n",
      "Step 343 -- Train loss: 2.4879913330078125, Train Acc: 0.1826171875 Test Acc: 0.1796875\n",
      "Step 344 -- Train loss: 2.4861888885498047, Train Acc: 0.18701171875 Test Acc: 0.181640625\n",
      "Step 345 -- Train loss: 2.472743511199951, Train Acc: 0.2001953125 Test Acc: 0.2197265625\n",
      "Step 346 -- Train loss: 2.4702796936035156, Train Acc: 0.198974609375 Test Acc: 0.201171875\n",
      "Step 347 -- Train loss: 2.4539361000061035, Train Acc: 0.203125 Test Acc: 0.1962890625\n",
      "Step 348 -- Train loss: 2.4425690174102783, Train Acc: 0.212890625 Test Acc: 0.20703125\n",
      "Step 349 -- Train loss: 2.422781467437744, Train Acc: 0.225830078125 Test Acc: 0.216796875\n",
      "Step 350 -- Train loss: 2.3983359336853027, Train Acc: 0.23681640625 Test Acc: 0.21875\n",
      "Step 351 -- Train loss: 2.3645317554473877, Train Acc: 0.2431640625 Test Acc: 0.244140625\n",
      "Step 352 -- Train loss: 2.3276286125183105, Train Acc: 0.275146484375 Test Acc: 0.2744140625\n",
      "Step 353 -- Train loss: 2.268820285797119, Train Acc: 0.294921875 Test Acc: 0.27734375\n",
      "Step 354 -- Train loss: 2.195868968963623, Train Acc: 0.337646484375 Test Acc: 0.314453125\n",
      "Step 355 -- Train loss: 2.1043264865875244, Train Acc: 0.38427734375 Test Acc: 0.359375\n",
      "Step 356 -- Train loss: 1.9898180961608887, Train Acc: 0.4423828125 Test Acc: 0.4111328125\n",
      "Step 357 -- Train loss: 1.8846545219421387, Train Acc: 0.473876953125 Test Acc: 0.466796875\n",
      "Step 358 -- Train loss: 1.7586737871170044, Train Acc: 0.54150390625 Test Acc: 0.5185546875\n",
      "Step 359 -- Train loss: 1.6193664073944092, Train Acc: 0.59765625 Test Acc: 0.5966796875\n",
      "Step 360 -- Train loss: 1.4643326997756958, Train Acc: 0.659423828125 Test Acc: 0.673828125\n",
      "Step 361 -- Train loss: 1.284552812576294, Train Acc: 0.73388671875 Test Acc: 0.712890625\n",
      "Step 362 -- Train loss: 1.1494826078414917, Train Acc: 0.77197265625 Test Acc: 0.7578125\n",
      "Step 363 -- Train loss: 0.9623763561248779, Train Acc: 0.8388671875 Test Acc: 0.82421875\n",
      "Step 364 -- Train loss: 0.843838632106781, Train Acc: 0.876953125 Test Acc: 0.8740234375\n",
      "Step 365 -- Train loss: 0.7093127965927124, Train Acc: 0.909912109375 Test Acc: 0.9111328125\n",
      "Step 366 -- Train loss: 0.6187074184417725, Train Acc: 0.92724609375 Test Acc: 0.93359375\n",
      "Step 367 -- Train loss: 0.5390657782554626, Train Acc: 0.939697265625 Test Acc: 0.951171875\n",
      "Step 368 -- Train loss: 0.4649854302406311, Train Acc: 0.956787109375 Test Acc: 0.94921875\n",
      "Step 369 -- Train loss: 0.41523319482803345, Train Acc: 0.958984375 Test Acc: 0.9609375\n",
      "Step 370 -- Train loss: 0.37058359384536743, Train Acc: 0.964599609375 Test Acc: 0.9677734375\n",
      "Step 371 -- Train loss: 0.33901354670524597, Train Acc: 0.965087890625 Test Acc: 0.9677734375\n",
      "Step 372 -- Train loss: 0.2930642068386078, Train Acc: 0.9736328125 Test Acc: 0.9619140625\n",
      "Step 373 -- Train loss: 0.2672944962978363, Train Acc: 0.974365234375 Test Acc: 0.98046875\n",
      "Step 374 -- Train loss: 0.22965994477272034, Train Acc: 0.98193359375 Test Acc: 0.9775390625\n",
      "Step 375 -- Train loss: 0.21340550482273102, Train Acc: 0.983154296875 Test Acc: 0.9765625\n",
      "Step 376 -- Train loss: 0.18677861988544464, Train Acc: 0.9921875 Test Acc: 0.98828125\n",
      "Step 377 -- Train loss: 0.16184356808662415, Train Acc: 0.993896484375 Test Acc: 0.99609375\n",
      "Step 378 -- Train loss: 0.14202998578548431, Train Acc: 0.99658203125 Test Acc: 0.9970703125\n",
      "Step 379 -- Train loss: 0.12456521391868591, Train Acc: 0.998291015625 Test Acc: 0.9990234375\n",
      "Step 380 -- Train loss: 0.11192218959331512, Train Acc: 0.998779296875 Test Acc: 0.9990234375\n",
      "Step 381 -- Train loss: 0.10372354090213776, Train Acc: 0.999755859375 Test Acc: 1.0\n",
      "Step 382 -- Train loss: 0.09604629129171371, Train Acc: 0.999755859375 Test Acc: 0.9990234375\n",
      "Step 383 -- Train loss: 0.09236511588096619, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 384 -- Train loss: 0.08428482711315155, Train Acc: 0.999755859375 Test Acc: 1.0\n",
      "Step 385 -- Train loss: 0.07808472961187363, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 386 -- Train loss: 0.0725637897849083, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 387 -- Train loss: 0.06868278235197067, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 388 -- Train loss: 0.06543268263339996, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 389 -- Train loss: 0.06161930784583092, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 390 -- Train loss: 0.05719410255551338, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 391 -- Train loss: 0.05393395945429802, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 392 -- Train loss: 0.0510639026761055, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 393 -- Train loss: 0.04850823059678078, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 394 -- Train loss: 0.046957869082689285, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 395 -- Train loss: 0.044325292110443115, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 396 -- Train loss: 0.0420655719935894, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 397 -- Train loss: 0.039979081600904465, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 398 -- Train loss: 0.03844181075692177, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 399 -- Train loss: 0.03684096783399582, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 400 -- Train loss: 0.035758860409259796, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 401 -- Train loss: 0.03412165865302086, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 402 -- Train loss: 0.033042192459106445, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 403 -- Train loss: 0.031935472041368484, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 404 -- Train loss: 0.030931517481803894, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 405 -- Train loss: 0.03001425787806511, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 406 -- Train loss: 0.02877313457429409, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 407 -- Train loss: 0.028389088809490204, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 408 -- Train loss: 0.027595844119787216, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 409 -- Train loss: 0.02707095816731453, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 410 -- Train loss: 0.026191605255007744, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 411 -- Train loss: 0.02553473971784115, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 412 -- Train loss: 0.025133168324828148, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 413 -- Train loss: 0.02426588162779808, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 414 -- Train loss: 0.023970112204551697, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 415 -- Train loss: 0.023574456572532654, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 416 -- Train loss: 0.023018527776002884, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 417 -- Train loss: 0.02261132001876831, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 418 -- Train loss: 0.02210564911365509, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 419 -- Train loss: 0.021747643128037453, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 420 -- Train loss: 0.021345704793930054, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 421 -- Train loss: 0.021035755053162575, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 422 -- Train loss: 0.02073366753757, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 423 -- Train loss: 0.02048543654382229, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 424 -- Train loss: 0.02008829638361931, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 425 -- Train loss: 0.01968793384730816, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 426 -- Train loss: 0.019436823204159737, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 427 -- Train loss: 0.019388969987630844, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 428 -- Train loss: 0.01894650235772133, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 429 -- Train loss: 0.018760764971375465, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 430 -- Train loss: 0.018276197835803032, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 431 -- Train loss: 0.018324706703424454, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 432 -- Train loss: 0.01804930903017521, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 433 -- Train loss: 0.017826944589614868, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 434 -- Train loss: 0.017679156735539436, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 435 -- Train loss: 0.01732533425092697, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 436 -- Train loss: 0.017239931970834732, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 437 -- Train loss: 0.016947442665696144, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 438 -- Train loss: 0.016973575577139854, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 439 -- Train loss: 0.016549810767173767, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 440 -- Train loss: 0.016517654061317444, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 441 -- Train loss: 0.016262944787740707, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 442 -- Train loss: 0.016126954928040504, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 443 -- Train loss: 0.015912460163235664, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 444 -- Train loss: 0.015815675258636475, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 445 -- Train loss: 0.01568167842924595, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 446 -- Train loss: 0.015632595866918564, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 447 -- Train loss: 0.01533895917236805, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 448 -- Train loss: 0.01509902160614729, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 449 -- Train loss: 0.014958261512219906, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 450 -- Train loss: 0.014913521707057953, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 451 -- Train loss: 0.01473432406783104, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 452 -- Train loss: 0.014547295868396759, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 453 -- Train loss: 0.014530174434185028, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 454 -- Train loss: 0.014355437830090523, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 455 -- Train loss: 0.01418391615152359, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 456 -- Train loss: 0.01408008299767971, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 457 -- Train loss: 0.013954969123005867, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 458 -- Train loss: 0.013822040520608425, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 459 -- Train loss: 0.013837631791830063, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 460 -- Train loss: 0.013599405996501446, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 461 -- Train loss: 0.013488879427313805, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 462 -- Train loss: 0.01346259843558073, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 463 -- Train loss: 0.013252167031168938, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 464 -- Train loss: 0.013137363828718662, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 465 -- Train loss: 0.013083190657198429, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 466 -- Train loss: 0.012932787649333477, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 467 -- Train loss: 0.012902023270726204, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 468 -- Train loss: 0.0126963434740901, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 469 -- Train loss: 0.01265670731663704, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 470 -- Train loss: 0.012611453421413898, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 471 -- Train loss: 0.012603141367435455, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 472 -- Train loss: 0.012442713603377342, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 473 -- Train loss: 0.012159297242760658, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 474 -- Train loss: 0.012148544192314148, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 475 -- Train loss: 0.012094426900148392, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 476 -- Train loss: 0.012028084136545658, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 477 -- Train loss: 0.011940307915210724, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 478 -- Train loss: 0.01186528243124485, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 479 -- Train loss: 0.011833112686872482, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 480 -- Train loss: 0.011608025059103966, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 481 -- Train loss: 0.01162341982126236, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 482 -- Train loss: 0.011594228446483612, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 483 -- Train loss: 0.011502141132950783, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 484 -- Train loss: 0.01135572511702776, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 485 -- Train loss: 0.011268680915236473, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 486 -- Train loss: 0.0111619234085083, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 487 -- Train loss: 0.01118302159011364, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 488 -- Train loss: 0.010954093188047409, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 489 -- Train loss: 0.011014360003173351, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 490 -- Train loss: 0.010821853764355183, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 491 -- Train loss: 0.010777685791254044, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 492 -- Train loss: 0.010848762467503548, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 493 -- Train loss: 0.01060427539050579, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 494 -- Train loss: 0.010639945045113564, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 495 -- Train loss: 0.010627111420035362, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 496 -- Train loss: 0.010403790511190891, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 497 -- Train loss: 0.010342591442167759, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 498 -- Train loss: 0.010267072357237339, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 499 -- Train loss: 0.010289691388607025, Train Acc: 1.0 Test Acc: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>att_prog_measure</td><td></td></tr><tr><td>data_repeat_frac</td><td></td></tr><tr><td>idx0_check</td><td></td></tr><tr><td>idx10_check</td><td></td></tr><tr><td>idx11_check</td><td></td></tr><tr><td>idx12_check</td><td></td></tr><tr><td>idx13_check</td><td></td></tr><tr><td>idx14_check</td><td></td></tr><tr><td>idx15_check</td><td></td></tr><tr><td>idx1_check</td><td></td></tr><tr><td>idx2_check</td><td></td></tr><tr><td>idx3_check</td><td></td></tr><tr><td>idx4_check</td><td></td></tr><tr><td>idx5_check</td><td></td></tr><tr><td>idx6_check</td><td></td></tr><tr><td>idx7_check</td><td></td></tr><tr><td>idx8_check</td><td></td></tr><tr><td>idx9_check</td><td></td></tr><tr><td>mean_cosine_sim</td><td></td></tr><tr><td>mean_cosine_sim_0</td><td></td></tr><tr><td>mean_cosine_sim_1</td><td></td></tr><tr><td>mean_cosine_sim_10</td><td></td></tr><tr><td>mean_cosine_sim_11</td><td></td></tr><tr><td>mean_cosine_sim_12</td><td></td></tr><tr><td>mean_cosine_sim_13</td><td></td></tr><tr><td>mean_cosine_sim_14</td><td></td></tr><tr><td>mean_cosine_sim_2</td><td></td></tr><tr><td>mean_cosine_sim_3</td><td></td></tr><tr><td>mean_cosine_sim_4</td><td></td></tr><tr><td>mean_cosine_sim_5</td><td></td></tr><tr><td>mean_cosine_sim_6</td><td></td></tr><tr><td>mean_cosine_sim_7</td><td></td></tr><tr><td>mean_cosine_sim_8</td><td></td></tr><tr><td>mean_cosine_sim_9</td><td></td></tr><tr><td>model_repeat_frac</td><td></td></tr><tr><td>test_acc</td><td></td></tr><tr><td>train_acc</td><td></td></tr><tr><td>train_loss</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>att_prog_measure</td><td>0.83799</td></tr><tr><td>data_repeat_frac</td><td>0.06563</td></tr><tr><td>idx0_check</td><td>1.0</td></tr><tr><td>idx10_check</td><td>1.0</td></tr><tr><td>idx11_check</td><td>1.0</td></tr><tr><td>idx12_check</td><td>1.0</td></tr><tr><td>idx13_check</td><td>1.0</td></tr><tr><td>idx14_check</td><td>1.0</td></tr><tr><td>idx15_check</td><td>1.0</td></tr><tr><td>idx1_check</td><td>1.0</td></tr><tr><td>idx2_check</td><td>1.0</td></tr><tr><td>idx3_check</td><td>1.0</td></tr><tr><td>idx4_check</td><td>1.0</td></tr><tr><td>idx5_check</td><td>1.0</td></tr><tr><td>idx6_check</td><td>1.0</td></tr><tr><td>idx7_check</td><td>1.0</td></tr><tr><td>idx8_check</td><td>1.0</td></tr><tr><td>idx9_check</td><td>1.0</td></tr><tr><td>mean_cosine_sim</td><td>0.00576</td></tr><tr><td>mean_cosine_sim_0</td><td>0.00885</td></tr><tr><td>mean_cosine_sim_1</td><td>0.01268</td></tr><tr><td>mean_cosine_sim_10</td><td>0.01747</td></tr><tr><td>mean_cosine_sim_11</td><td>0.00711</td></tr><tr><td>mean_cosine_sim_12</td><td>-0.00236</td></tr><tr><td>mean_cosine_sim_13</td><td>-0.02117</td></tr><tr><td>mean_cosine_sim_14</td><td>0.00369</td></tr><tr><td>mean_cosine_sim_2</td><td>0.00661</td></tr><tr><td>mean_cosine_sim_3</td><td>0.00442</td></tr><tr><td>mean_cosine_sim_4</td><td>2e-05</td></tr><tr><td>mean_cosine_sim_5</td><td>0.00291</td></tr><tr><td>mean_cosine_sim_6</td><td>0.00525</td></tr><tr><td>mean_cosine_sim_7</td><td>0.00825</td></tr><tr><td>mean_cosine_sim_8</td><td>0.01028</td></tr><tr><td>mean_cosine_sim_9</td><td>0.00067</td></tr><tr><td>model_repeat_frac</td><td>0.06563</td></tr><tr><td>test_acc</td><td>1.0</td></tr><tr><td>train_acc</td><td>1.0</td></tr><tr><td>train_loss</td><td>0.01029</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">mwp_linear</strong> at: <a href='https://wandb.ai/wth_ucsd/loss_plateau_tf/runs/sb9puhni' target=\"_blank\">https://wandb.ai/wth_ucsd/loss_plateau_tf/runs/sb9puhni</a><br/>Synced 6 W&B file(s), 500 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251106_230932-sb9puhni/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_sampler = data.MovingWindowProduct(\n",
    "    min_num=config.data.min_num,\n",
    "    max_num=config.data.max_num,\n",
    "    k=config.data.k,\n",
    "    p=config.data.p,\n",
    ")\n",
    "\n",
    "model = GPTLinear(config.model, return_att=True).to(device)\n",
    "optim = Adam(model.parameters(), lr=config.train.lr)\n",
    "\n",
    "if config.train.wandb:\n",
    "    wandb_run_name = 'mwp_linear'\n",
    "    wandb.login(key=\"\")\n",
    "    wandb.init(project=\"loss_plateau_tf\", name=wandb_run_name, config=config)\n",
    "    wandb.watch(model)\n",
    "\n",
    "for step in range(config.train.num_steps):\n",
    "    train_step(\n",
    "        model=model,\n",
    "        optim=optim,\n",
    "        data_sampler=data_sampler,\n",
    "        step=step,\n",
    "        config=config,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "if config.train.wandb:\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mixing MWP and MWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.22.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jyue/private/tf-loss-plateau/wandb/run-20251107_013333-d2mhf1co</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/wth_ucsd/loss_plateau_tf/runs/d2mhf1co' target=\"_blank\">mws_mwp_linear_frozen_embedding</a></strong> to <a href='https://wandb.ai/wth_ucsd/loss_plateau_tf' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/wth_ucsd/loss_plateau_tf' target=\"_blank\">https://wandb.ai/wth_ucsd/loss_plateau_tf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/wth_ucsd/loss_plateau_tf/runs/d2mhf1co' target=\"_blank\">https://wandb.ai/wth_ucsd/loss_plateau_tf/runs/d2mhf1co</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0 -- Train loss: 2.8831145763397217, Train Acc: 0.0693359375 Test Acc: 0.0546875\n",
      "Step 1 -- Train loss: 2.877596855163574, Train Acc: 0.06494140625 Test Acc: 0.0556640625\n",
      "Step 2 -- Train loss: 2.8668479919433594, Train Acc: 0.06787109375 Test Acc: 0.0576171875\n",
      "Step 3 -- Train loss: 2.859556198120117, Train Acc: 0.058349609375 Test Acc: 0.0595703125\n",
      "Step 4 -- Train loss: 2.8550333976745605, Train Acc: 0.067626953125 Test Acc: 0.0556640625\n",
      "Step 5 -- Train loss: 2.849236249923706, Train Acc: 0.071533203125 Test Acc: 0.0615234375\n",
      "Step 6 -- Train loss: 2.8438851833343506, Train Acc: 0.066650390625 Test Acc: 0.0546875\n",
      "Step 7 -- Train loss: 2.844261884689331, Train Acc: 0.06201171875 Test Acc: 0.072265625\n",
      "Step 8 -- Train loss: 2.8383939266204834, Train Acc: 0.072021484375 Test Acc: 0.0595703125\n",
      "Step 9 -- Train loss: 2.8402185440063477, Train Acc: 0.05810546875 Test Acc: 0.0537109375\n",
      "Step 10 -- Train loss: 2.8312153816223145, Train Acc: 0.066650390625 Test Acc: 0.0625\n",
      "Step 11 -- Train loss: 2.8290011882781982, Train Acc: 0.064208984375 Test Acc: 0.0654296875\n",
      "Step 12 -- Train loss: 2.8272361755371094, Train Acc: 0.06982421875 Test Acc: 0.0625\n",
      "Step 13 -- Train loss: 2.832382917404175, Train Acc: 0.07177734375 Test Acc: 0.060546875\n",
      "Step 14 -- Train loss: 2.8266923427581787, Train Acc: 0.082275390625 Test Acc: 0.064453125\n",
      "Step 15 -- Train loss: 2.825108766555786, Train Acc: 0.087646484375 Test Acc: 0.0849609375\n",
      "Step 16 -- Train loss: 2.8203797340393066, Train Acc: 0.0849609375 Test Acc: 0.0791015625\n",
      "Step 17 -- Train loss: 2.8176791667938232, Train Acc: 0.08154296875 Test Acc: 0.087890625\n",
      "Step 18 -- Train loss: 2.8121469020843506, Train Acc: 0.0908203125 Test Acc: 0.0859375\n",
      "Step 19 -- Train loss: 2.8110873699188232, Train Acc: 0.09228515625 Test Acc: 0.0908203125\n",
      "Step 20 -- Train loss: 2.8042452335357666, Train Acc: 0.10595703125 Test Acc: 0.0986328125\n",
      "Step 21 -- Train loss: 2.8013076782226562, Train Acc: 0.110107421875 Test Acc: 0.0859375\n",
      "Step 22 -- Train loss: 2.795102834701538, Train Acc: 0.109130859375 Test Acc: 0.1103515625\n",
      "Step 23 -- Train loss: 2.788032054901123, Train Acc: 0.1123046875 Test Acc: 0.115234375\n",
      "Step 24 -- Train loss: 2.7854158878326416, Train Acc: 0.114501953125 Test Acc: 0.1044921875\n",
      "Step 25 -- Train loss: 2.77485728263855, Train Acc: 0.11083984375 Test Acc: 0.0986328125\n",
      "Step 26 -- Train loss: 2.7714669704437256, Train Acc: 0.113525390625 Test Acc: 0.109375\n",
      "Step 27 -- Train loss: 2.7613768577575684, Train Acc: 0.114013671875 Test Acc: 0.11328125\n",
      "Step 28 -- Train loss: 2.7609634399414062, Train Acc: 0.115478515625 Test Acc: 0.115234375\n",
      "Step 29 -- Train loss: 2.743966817855835, Train Acc: 0.123779296875 Test Acc: 0.1142578125\n",
      "Step 30 -- Train loss: 2.7425436973571777, Train Acc: 0.115234375 Test Acc: 0.11328125\n",
      "Step 31 -- Train loss: 2.7360098361968994, Train Acc: 0.118896484375 Test Acc: 0.1142578125\n",
      "Step 32 -- Train loss: 2.7250945568084717, Train Acc: 0.1181640625 Test Acc: 0.109375\n",
      "Step 33 -- Train loss: 2.716397762298584, Train Acc: 0.1171875 Test Acc: 0.123046875\n",
      "Step 34 -- Train loss: 2.7132275104522705, Train Acc: 0.11572265625 Test Acc: 0.1259765625\n",
      "Step 35 -- Train loss: 2.7100107669830322, Train Acc: 0.12158203125 Test Acc: 0.1123046875\n",
      "Step 36 -- Train loss: 2.699246883392334, Train Acc: 0.117431640625 Test Acc: 0.1474609375\n",
      "Step 37 -- Train loss: 2.704090118408203, Train Acc: 0.1171875 Test Acc: 0.1171875\n",
      "Step 38 -- Train loss: 2.6909191608428955, Train Acc: 0.12451171875 Test Acc: 0.115234375\n",
      "Step 39 -- Train loss: 2.695985794067383, Train Acc: 0.120849609375 Test Acc: 0.1181640625\n",
      "Step 40 -- Train loss: 2.6895811557769775, Train Acc: 0.12255859375 Test Acc: 0.1201171875\n",
      "Step 41 -- Train loss: 2.685185432434082, Train Acc: 0.11669921875 Test Acc: 0.1162109375\n",
      "Step 42 -- Train loss: 2.685488224029541, Train Acc: 0.120849609375 Test Acc: 0.126953125\n",
      "Step 43 -- Train loss: 2.6825127601623535, Train Acc: 0.120849609375 Test Acc: 0.115234375\n",
      "Step 44 -- Train loss: 2.6792943477630615, Train Acc: 0.119140625 Test Acc: 0.125\n",
      "Step 45 -- Train loss: 2.6717116832733154, Train Acc: 0.12451171875 Test Acc: 0.111328125\n",
      "Step 46 -- Train loss: 2.675116777420044, Train Acc: 0.11474609375 Test Acc: 0.1396484375\n",
      "Step 47 -- Train loss: 2.674469232559204, Train Acc: 0.119873046875 Test Acc: 0.1103515625\n",
      "Step 48 -- Train loss: 2.671973705291748, Train Acc: 0.119873046875 Test Acc: 0.1181640625\n",
      "Step 49 -- Train loss: 2.666914224624634, Train Acc: 0.119140625 Test Acc: 0.1142578125\n",
      "Step 50 -- Train loss: 2.6634016036987305, Train Acc: 0.11767578125 Test Acc: 0.130859375\n",
      "Step 51 -- Train loss: 2.6669187545776367, Train Acc: 0.117919921875 Test Acc: 0.125\n",
      "Step 52 -- Train loss: 2.6651415824890137, Train Acc: 0.11669921875 Test Acc: 0.1201171875\n",
      "Step 53 -- Train loss: 2.656761407852173, Train Acc: 0.124755859375 Test Acc: 0.11328125\n",
      "Step 54 -- Train loss: 2.65714430809021, Train Acc: 0.124755859375 Test Acc: 0.126953125\n",
      "Step 55 -- Train loss: 2.663754940032959, Train Acc: 0.1171875 Test Acc: 0.119140625\n",
      "Step 56 -- Train loss: 2.6558427810668945, Train Acc: 0.118408203125 Test Acc: 0.12109375\n",
      "Step 57 -- Train loss: 2.653404474258423, Train Acc: 0.127197265625 Test Acc: 0.126953125\n",
      "Step 58 -- Train loss: 2.65450119972229, Train Acc: 0.11572265625 Test Acc: 0.130859375\n",
      "Step 59 -- Train loss: 2.656804323196411, Train Acc: 0.118896484375 Test Acc: 0.1201171875\n",
      "Step 60 -- Train loss: 2.6545138359069824, Train Acc: 0.12353515625 Test Acc: 0.1240234375\n",
      "Step 61 -- Train loss: 2.6575891971588135, Train Acc: 0.112548828125 Test Acc: 0.1083984375\n",
      "Step 62 -- Train loss: 2.6475908756256104, Train Acc: 0.12353515625 Test Acc: 0.1162109375\n",
      "Step 63 -- Train loss: 2.653703451156616, Train Acc: 0.126708984375 Test Acc: 0.107421875\n",
      "Step 64 -- Train loss: 2.6481995582580566, Train Acc: 0.123046875 Test Acc: 0.1259765625\n",
      "Step 65 -- Train loss: 2.647143840789795, Train Acc: 0.121826171875 Test Acc: 0.11328125\n",
      "Step 66 -- Train loss: 2.6440937519073486, Train Acc: 0.122314453125 Test Acc: 0.1279296875\n",
      "Step 67 -- Train loss: 2.648238182067871, Train Acc: 0.12841796875 Test Acc: 0.123046875\n",
      "Step 68 -- Train loss: 2.6461260318756104, Train Acc: 0.126708984375 Test Acc: 0.1279296875\n",
      "Step 69 -- Train loss: 2.643289566040039, Train Acc: 0.12451171875 Test Acc: 0.123046875\n",
      "Step 70 -- Train loss: 2.641965627670288, Train Acc: 0.116455078125 Test Acc: 0.130859375\n",
      "Step 71 -- Train loss: 2.641577959060669, Train Acc: 0.127685546875 Test Acc: 0.12109375\n",
      "Step 72 -- Train loss: 2.6416144371032715, Train Acc: 0.12841796875 Test Acc: 0.1162109375\n",
      "Step 73 -- Train loss: 2.64089298248291, Train Acc: 0.1201171875 Test Acc: 0.119140625\n",
      "Step 74 -- Train loss: 2.638550281524658, Train Acc: 0.11962890625 Test Acc: 0.1201171875\n",
      "Step 75 -- Train loss: 2.639286994934082, Train Acc: 0.1220703125 Test Acc: 0.111328125\n",
      "Step 76 -- Train loss: 2.6440634727478027, Train Acc: 0.118408203125 Test Acc: 0.1171875\n",
      "Step 77 -- Train loss: 2.6429049968719482, Train Acc: 0.12353515625 Test Acc: 0.130859375\n",
      "Step 78 -- Train loss: 2.6405131816864014, Train Acc: 0.125 Test Acc: 0.119140625\n",
      "Step 79 -- Train loss: 2.6405699253082275, Train Acc: 0.115234375 Test Acc: 0.126953125\n",
      "Step 80 -- Train loss: 2.636338949203491, Train Acc: 0.119384765625 Test Acc: 0.1201171875\n",
      "Step 81 -- Train loss: 2.6401026248931885, Train Acc: 0.116943359375 Test Acc: 0.1328125\n",
      "Step 82 -- Train loss: 2.6373062133789062, Train Acc: 0.124267578125 Test Acc: 0.1064453125\n",
      "Step 83 -- Train loss: 2.631739854812622, Train Acc: 0.12646484375 Test Acc: 0.1103515625\n",
      "Step 84 -- Train loss: 2.6340701580047607, Train Acc: 0.128662109375 Test Acc: 0.1337890625\n",
      "Step 85 -- Train loss: 2.6357691287994385, Train Acc: 0.123291015625 Test Acc: 0.1220703125\n",
      "Step 86 -- Train loss: 2.638570547103882, Train Acc: 0.1162109375 Test Acc: 0.1220703125\n",
      "Step 87 -- Train loss: 2.635714292526245, Train Acc: 0.126708984375 Test Acc: 0.12109375\n",
      "Step 88 -- Train loss: 2.6367974281311035, Train Acc: 0.117919921875 Test Acc: 0.1298828125\n",
      "Step 89 -- Train loss: 2.6343419551849365, Train Acc: 0.11962890625 Test Acc: 0.1220703125\n",
      "Step 90 -- Train loss: 2.6341934204101562, Train Acc: 0.125732421875 Test Acc: 0.1171875\n",
      "Step 91 -- Train loss: 2.6329634189605713, Train Acc: 0.122314453125 Test Acc: 0.111328125\n",
      "Step 92 -- Train loss: 2.6327710151672363, Train Acc: 0.1259765625 Test Acc: 0.123046875\n",
      "Step 93 -- Train loss: 2.6345324516296387, Train Acc: 0.129638671875 Test Acc: 0.1162109375\n",
      "Step 94 -- Train loss: 2.6324515342712402, Train Acc: 0.125244140625 Test Acc: 0.12890625\n",
      "Step 95 -- Train loss: 2.635349988937378, Train Acc: 0.12353515625 Test Acc: 0.1181640625\n",
      "Step 96 -- Train loss: 2.6351373195648193, Train Acc: 0.11865234375 Test Acc: 0.123046875\n",
      "Step 97 -- Train loss: 2.636655569076538, Train Acc: 0.117919921875 Test Acc: 0.1416015625\n",
      "Step 98 -- Train loss: 2.6313934326171875, Train Acc: 0.118896484375 Test Acc: 0.115234375\n",
      "Step 99 -- Train loss: 2.633859872817993, Train Acc: 0.12158203125 Test Acc: 0.1220703125\n",
      "Step 100 -- Train loss: 2.6324639320373535, Train Acc: 0.12841796875 Test Acc: 0.11328125\n",
      "Step 101 -- Train loss: 2.6331191062927246, Train Acc: 0.118408203125 Test Acc: 0.126953125\n",
      "Step 102 -- Train loss: 2.630605936050415, Train Acc: 0.122314453125 Test Acc: 0.1201171875\n",
      "Step 103 -- Train loss: 2.6329402923583984, Train Acc: 0.12451171875 Test Acc: 0.1201171875\n",
      "Step 104 -- Train loss: 2.6315884590148926, Train Acc: 0.120849609375 Test Acc: 0.125\n",
      "Step 105 -- Train loss: 2.6294844150543213, Train Acc: 0.1220703125 Test Acc: 0.126953125\n",
      "Step 106 -- Train loss: 2.627230167388916, Train Acc: 0.126708984375 Test Acc: 0.1181640625\n",
      "Step 107 -- Train loss: 2.6271896362304688, Train Acc: 0.12451171875 Test Acc: 0.123046875\n",
      "Step 108 -- Train loss: 2.6273651123046875, Train Acc: 0.12646484375 Test Acc: 0.12109375\n",
      "Step 109 -- Train loss: 2.626554012298584, Train Acc: 0.120849609375 Test Acc: 0.1240234375\n",
      "Step 110 -- Train loss: 2.627068281173706, Train Acc: 0.130859375 Test Acc: 0.1220703125\n",
      "Step 111 -- Train loss: 2.6286158561706543, Train Acc: 0.121826171875 Test Acc: 0.125\n",
      "Step 112 -- Train loss: 2.628048896789551, Train Acc: 0.124267578125 Test Acc: 0.115234375\n",
      "Step 113 -- Train loss: 2.628533363342285, Train Acc: 0.125244140625 Test Acc: 0.1171875\n",
      "Step 114 -- Train loss: 2.628049850463867, Train Acc: 0.120849609375 Test Acc: 0.1220703125\n",
      "Step 115 -- Train loss: 2.62445068359375, Train Acc: 0.124755859375 Test Acc: 0.1259765625\n",
      "Step 116 -- Train loss: 2.6249256134033203, Train Acc: 0.11962890625 Test Acc: 0.1201171875\n",
      "Step 117 -- Train loss: 2.6273105144500732, Train Acc: 0.127197265625 Test Acc: 0.119140625\n",
      "Step 118 -- Train loss: 2.626568078994751, Train Acc: 0.12353515625 Test Acc: 0.111328125\n",
      "Step 119 -- Train loss: 2.6251206398010254, Train Acc: 0.126220703125 Test Acc: 0.1298828125\n",
      "Step 120 -- Train loss: 2.620394468307495, Train Acc: 0.116455078125 Test Acc: 0.123046875\n",
      "Step 121 -- Train loss: 2.6248340606689453, Train Acc: 0.1279296875 Test Acc: 0.12109375\n",
      "Step 122 -- Train loss: 2.6215827465057373, Train Acc: 0.12890625 Test Acc: 0.1181640625\n",
      "Step 123 -- Train loss: 2.618513584136963, Train Acc: 0.1220703125 Test Acc: 0.1337890625\n",
      "Step 124 -- Train loss: 2.6222774982452393, Train Acc: 0.125732421875 Test Acc: 0.125\n",
      "Step 125 -- Train loss: 2.622992992401123, Train Acc: 0.126708984375 Test Acc: 0.125\n",
      "Step 126 -- Train loss: 2.6231961250305176, Train Acc: 0.126953125 Test Acc: 0.1142578125\n",
      "Step 127 -- Train loss: 2.619166612625122, Train Acc: 0.1318359375 Test Acc: 0.123046875\n",
      "Step 128 -- Train loss: 2.618488073348999, Train Acc: 0.130859375 Test Acc: 0.1376953125\n",
      "Step 129 -- Train loss: 2.619382858276367, Train Acc: 0.12646484375 Test Acc: 0.1279296875\n",
      "Step 130 -- Train loss: 2.6207058429718018, Train Acc: 0.125 Test Acc: 0.119140625\n",
      "Step 131 -- Train loss: 2.619274854660034, Train Acc: 0.12841796875 Test Acc: 0.1328125\n",
      "Step 132 -- Train loss: 2.6112287044525146, Train Acc: 0.1259765625 Test Acc: 0.1357421875\n",
      "Step 133 -- Train loss: 2.6134932041168213, Train Acc: 0.134033203125 Test Acc: 0.1259765625\n",
      "Step 134 -- Train loss: 2.6124024391174316, Train Acc: 0.129638671875 Test Acc: 0.140625\n",
      "Step 135 -- Train loss: 2.6161653995513916, Train Acc: 0.123291015625 Test Acc: 0.125\n",
      "Step 136 -- Train loss: 2.61698317527771, Train Acc: 0.12939453125 Test Acc: 0.1396484375\n",
      "Step 137 -- Train loss: 2.6143882274627686, Train Acc: 0.12939453125 Test Acc: 0.125\n",
      "Step 138 -- Train loss: 2.6142656803131104, Train Acc: 0.1201171875 Test Acc: 0.12890625\n",
      "Step 139 -- Train loss: 2.6076231002807617, Train Acc: 0.1318359375 Test Acc: 0.1240234375\n",
      "Step 140 -- Train loss: 2.6123878955841064, Train Acc: 0.137451171875 Test Acc: 0.134765625\n",
      "Step 141 -- Train loss: 2.6046016216278076, Train Acc: 0.127197265625 Test Acc: 0.1240234375\n",
      "Step 142 -- Train loss: 2.6098527908325195, Train Acc: 0.13427734375 Test Acc: 0.1318359375\n",
      "Step 143 -- Train loss: 2.609403133392334, Train Acc: 0.12939453125 Test Acc: 0.1201171875\n",
      "Step 144 -- Train loss: 2.6068246364593506, Train Acc: 0.130126953125 Test Acc: 0.1171875\n",
      "Step 145 -- Train loss: 2.604098320007324, Train Acc: 0.13720703125 Test Acc: 0.13671875\n",
      "Step 146 -- Train loss: 2.6025516986846924, Train Acc: 0.12353515625 Test Acc: 0.1298828125\n",
      "Step 147 -- Train loss: 2.600825786590576, Train Acc: 0.13427734375 Test Acc: 0.130859375\n",
      "Step 148 -- Train loss: 2.60560941696167, Train Acc: 0.12451171875 Test Acc: 0.1318359375\n",
      "Step 149 -- Train loss: 2.5975968837738037, Train Acc: 0.132568359375 Test Acc: 0.142578125\n",
      "Step 150 -- Train loss: 2.597172737121582, Train Acc: 0.1357421875 Test Acc: 0.146484375\n",
      "Step 151 -- Train loss: 2.5931055545806885, Train Acc: 0.135498046875 Test Acc: 0.12890625\n",
      "Step 152 -- Train loss: 2.5903401374816895, Train Acc: 0.13671875 Test Acc: 0.142578125\n",
      "Step 153 -- Train loss: 2.590585947036743, Train Acc: 0.141845703125 Test Acc: 0.1494140625\n",
      "Step 154 -- Train loss: 2.580956220626831, Train Acc: 0.14501953125 Test Acc: 0.142578125\n",
      "Step 155 -- Train loss: 2.5846827030181885, Train Acc: 0.137939453125 Test Acc: 0.15234375\n",
      "Step 156 -- Train loss: 2.5768227577209473, Train Acc: 0.13916015625 Test Acc: 0.142578125\n",
      "Step 157 -- Train loss: 2.56628155708313, Train Acc: 0.142578125 Test Acc: 0.14453125\n",
      "Step 158 -- Train loss: 2.576543092727661, Train Acc: 0.142333984375 Test Acc: 0.1318359375\n",
      "Step 159 -- Train loss: 2.5620827674865723, Train Acc: 0.147216796875 Test Acc: 0.1513671875\n",
      "Step 160 -- Train loss: 2.567436456680298, Train Acc: 0.148681640625 Test Acc: 0.146484375\n",
      "Step 161 -- Train loss: 2.5575602054595947, Train Acc: 0.162109375 Test Acc: 0.1533203125\n",
      "Step 162 -- Train loss: 2.5497889518737793, Train Acc: 0.1611328125 Test Acc: 0.150390625\n",
      "Step 163 -- Train loss: 2.539066791534424, Train Acc: 0.1611328125 Test Acc: 0.1591796875\n",
      "Step 164 -- Train loss: 2.5275046825408936, Train Acc: 0.164306640625 Test Acc: 0.16796875\n",
      "Step 165 -- Train loss: 2.5260863304138184, Train Acc: 0.1640625 Test Acc: 0.166015625\n",
      "Step 166 -- Train loss: 2.5262489318847656, Train Acc: 0.170166015625 Test Acc: 0.173828125\n",
      "Step 167 -- Train loss: 2.507319927215576, Train Acc: 0.1845703125 Test Acc: 0.1875\n",
      "Step 168 -- Train loss: 2.4928078651428223, Train Acc: 0.19189453125 Test Acc: 0.2099609375\n",
      "Step 169 -- Train loss: 2.473832368850708, Train Acc: 0.1943359375 Test Acc: 0.1904296875\n",
      "Step 170 -- Train loss: 2.4684650897979736, Train Acc: 0.211181640625 Test Acc: 0.2158203125\n",
      "Step 171 -- Train loss: 2.459712266921997, Train Acc: 0.224609375 Test Acc: 0.232421875\n",
      "Step 172 -- Train loss: 2.4288370609283447, Train Acc: 0.252197265625 Test Acc: 0.251953125\n",
      "Step 173 -- Train loss: 2.4071381092071533, Train Acc: 0.284423828125 Test Acc: 0.259765625\n",
      "Step 174 -- Train loss: 2.3839311599731445, Train Acc: 0.28955078125 Test Acc: 0.287109375\n",
      "Step 175 -- Train loss: 2.3668606281280518, Train Acc: 0.3056640625 Test Acc: 0.3193359375\n",
      "Step 176 -- Train loss: 2.3334805965423584, Train Acc: 0.34912109375 Test Acc: 0.3583984375\n",
      "Step 177 -- Train loss: 2.3025460243225098, Train Acc: 0.3720703125 Test Acc: 0.380859375\n",
      "Step 178 -- Train loss: 2.255232572555542, Train Acc: 0.404052734375 Test Acc: 0.380859375\n",
      "Step 179 -- Train loss: 2.2155373096466064, Train Acc: 0.43408203125 Test Acc: 0.4384765625\n",
      "Step 180 -- Train loss: 2.169949531555176, Train Acc: 0.452880859375 Test Acc: 0.4482421875\n",
      "Step 181 -- Train loss: 2.1248018741607666, Train Acc: 0.4716796875 Test Acc: 0.4677734375\n",
      "Step 182 -- Train loss: 2.053591012954712, Train Acc: 0.494384765625 Test Acc: 0.478515625\n",
      "Step 183 -- Train loss: 2.0002288818359375, Train Acc: 0.50732421875 Test Acc: 0.513671875\n",
      "Step 184 -- Train loss: 1.93937349319458, Train Acc: 0.523193359375 Test Acc: 0.5283203125\n",
      "Step 185 -- Train loss: 1.870322823524475, Train Acc: 0.54150390625 Test Acc: 0.541015625\n",
      "Step 186 -- Train loss: 1.825850248336792, Train Acc: 0.537841796875 Test Acc: 0.5244140625\n",
      "Step 187 -- Train loss: 1.72543203830719, Train Acc: 0.55615234375 Test Acc: 0.5537109375\n",
      "Step 188 -- Train loss: 1.676946997642517, Train Acc: 0.56005859375 Test Acc: 0.5576171875\n",
      "Step 189 -- Train loss: 1.6154447793960571, Train Acc: 0.564697265625 Test Acc: 0.552734375\n",
      "Step 190 -- Train loss: 1.547811508178711, Train Acc: 0.55810546875 Test Acc: 0.564453125\n",
      "Step 191 -- Train loss: 1.4768688678741455, Train Acc: 0.572998046875 Test Acc: 0.55078125\n",
      "Step 192 -- Train loss: 1.4229916334152222, Train Acc: 0.56689453125 Test Acc: 0.5556640625\n",
      "Step 193 -- Train loss: 1.3839908838272095, Train Acc: 0.565673828125 Test Acc: 0.5615234375\n",
      "Step 194 -- Train loss: 1.3251123428344727, Train Acc: 0.561279296875 Test Acc: 0.5693359375\n",
      "Step 195 -- Train loss: 1.2626519203186035, Train Acc: 0.57177734375 Test Acc: 0.5732421875\n",
      "Step 196 -- Train loss: 1.2151989936828613, Train Acc: 0.5751953125 Test Acc: 0.5615234375\n",
      "Step 197 -- Train loss: 1.186554193496704, Train Acc: 0.584716796875 Test Acc: 0.552734375\n",
      "Step 198 -- Train loss: 1.1451977491378784, Train Acc: 0.583740234375 Test Acc: 0.5703125\n",
      "Step 199 -- Train loss: 1.100981593132019, Train Acc: 0.591552734375 Test Acc: 0.5869140625\n",
      "Step 200 -- Train loss: 1.0744787454605103, Train Acc: 0.59814453125 Test Acc: 0.58984375\n",
      "Step 201 -- Train loss: 1.0427227020263672, Train Acc: 0.609375 Test Acc: 0.58984375\n",
      "Step 202 -- Train loss: 1.0152031183242798, Train Acc: 0.6015625 Test Acc: 0.6259765625\n",
      "Step 203 -- Train loss: 0.974419355392456, Train Acc: 0.6162109375 Test Acc: 0.619140625\n",
      "Step 204 -- Train loss: 0.9441884160041809, Train Acc: 0.642578125 Test Acc: 0.607421875\n",
      "Step 205 -- Train loss: 0.936593770980835, Train Acc: 0.624755859375 Test Acc: 0.62109375\n",
      "Step 206 -- Train loss: 0.9149165749549866, Train Acc: 0.635498046875 Test Acc: 0.615234375\n",
      "Step 207 -- Train loss: 0.9004990458488464, Train Acc: 0.6181640625 Test Acc: 0.5751953125\n",
      "Step 208 -- Train loss: 0.8779780268669128, Train Acc: 0.6513671875 Test Acc: 0.623046875\n",
      "Step 209 -- Train loss: 0.865665853023529, Train Acc: 0.65185546875 Test Acc: 0.6142578125\n",
      "Step 210 -- Train loss: 0.8416599631309509, Train Acc: 0.6591796875 Test Acc: 0.6171875\n",
      "Step 211 -- Train loss: 0.8514869213104248, Train Acc: 0.632080078125 Test Acc: 0.6416015625\n",
      "Step 212 -- Train loss: 0.8311666250228882, Train Acc: 0.65625 Test Acc: 0.6328125\n",
      "Step 213 -- Train loss: 0.8142732977867126, Train Acc: 0.64794921875 Test Acc: 0.619140625\n",
      "Step 214 -- Train loss: 0.7998483180999756, Train Acc: 0.663330078125 Test Acc: 0.6708984375\n",
      "Step 215 -- Train loss: 0.7890777587890625, Train Acc: 0.669677734375 Test Acc: 0.62109375\n",
      "Step 216 -- Train loss: 0.7852174043655396, Train Acc: 0.662353515625 Test Acc: 0.642578125\n",
      "Step 217 -- Train loss: 0.7864751815795898, Train Acc: 0.65869140625 Test Acc: 0.64453125\n",
      "Step 218 -- Train loss: 0.7664616703987122, Train Acc: 0.677978515625 Test Acc: 0.6474609375\n",
      "Step 219 -- Train loss: 0.7601085901260376, Train Acc: 0.680908203125 Test Acc: 0.689453125\n",
      "Step 220 -- Train loss: 0.7473110556602478, Train Acc: 0.691162109375 Test Acc: 0.685546875\n",
      "Step 221 -- Train loss: 0.7508577704429626, Train Acc: 0.6826171875 Test Acc: 0.673828125\n",
      "Step 222 -- Train loss: 0.7352745532989502, Train Acc: 0.694580078125 Test Acc: 0.6826171875\n",
      "Step 223 -- Train loss: 0.7326789498329163, Train Acc: 0.691650390625 Test Acc: 0.685546875\n",
      "Step 224 -- Train loss: 0.7169880867004395, Train Acc: 0.6943359375 Test Acc: 0.6787109375\n",
      "Step 225 -- Train loss: 0.7215551733970642, Train Acc: 0.681884765625 Test Acc: 0.67578125\n",
      "Step 226 -- Train loss: 0.7099747657775879, Train Acc: 0.701171875 Test Acc: 0.6708984375\n",
      "Step 227 -- Train loss: 0.689002275466919, Train Acc: 0.72412109375 Test Acc: 0.6669921875\n",
      "Step 228 -- Train loss: 0.7003664374351501, Train Acc: 0.703369140625 Test Acc: 0.693359375\n",
      "Step 229 -- Train loss: 0.6809419989585876, Train Acc: 0.7060546875 Test Acc: 0.68359375\n",
      "Step 230 -- Train loss: 0.6918553113937378, Train Acc: 0.7041015625 Test Acc: 0.6962890625\n",
      "Step 231 -- Train loss: 0.6888865828514099, Train Acc: 0.70751953125 Test Acc: 0.7333984375\n",
      "Step 232 -- Train loss: 0.6699954271316528, Train Acc: 0.732666015625 Test Acc: 0.7216796875\n",
      "Step 233 -- Train loss: 0.6728935241699219, Train Acc: 0.73046875 Test Acc: 0.7412109375\n",
      "Step 234 -- Train loss: 0.6623267531394958, Train Acc: 0.7412109375 Test Acc: 0.724609375\n",
      "Step 235 -- Train loss: 0.6657310724258423, Train Acc: 0.728515625 Test Acc: 0.703125\n",
      "Step 236 -- Train loss: 0.6560357809066772, Train Acc: 0.736328125 Test Acc: 0.724609375\n",
      "Step 237 -- Train loss: 0.6518243551254272, Train Acc: 0.7529296875 Test Acc: 0.72265625\n",
      "Step 238 -- Train loss: 0.6535782814025879, Train Acc: 0.744873046875 Test Acc: 0.74609375\n",
      "Step 239 -- Train loss: 0.629120409488678, Train Acc: 0.777099609375 Test Acc: 0.7412109375\n",
      "Step 240 -- Train loss: 0.6391708254814148, Train Acc: 0.74462890625 Test Acc: 0.74609375\n",
      "Step 241 -- Train loss: 0.636127233505249, Train Acc: 0.748779296875 Test Acc: 0.728515625\n",
      "Step 242 -- Train loss: 0.623836100101471, Train Acc: 0.75830078125 Test Acc: 0.7353515625\n",
      "Step 243 -- Train loss: 0.6214982867240906, Train Acc: 0.7587890625 Test Acc: 0.7509765625\n",
      "Step 244 -- Train loss: 0.6243968605995178, Train Acc: 0.765625 Test Acc: 0.7724609375\n",
      "Step 245 -- Train loss: 0.617560863494873, Train Acc: 0.7734375 Test Acc: 0.7734375\n",
      "Step 246 -- Train loss: 0.6164376139640808, Train Acc: 0.770263671875 Test Acc: 0.7685546875\n",
      "Step 247 -- Train loss: 0.601092517375946, Train Acc: 0.77685546875 Test Acc: 0.75390625\n",
      "Step 248 -- Train loss: 0.6107928156852722, Train Acc: 0.767822265625 Test Acc: 0.7431640625\n",
      "Step 249 -- Train loss: 0.5986329317092896, Train Acc: 0.78759765625 Test Acc: 0.7666015625\n",
      "Step 250 -- Train loss: 0.6020817756652832, Train Acc: 0.778564453125 Test Acc: 0.7783203125\n",
      "Step 251 -- Train loss: 0.5966208577156067, Train Acc: 0.78955078125 Test Acc: 0.779296875\n",
      "Step 252 -- Train loss: 0.5844414234161377, Train Acc: 0.787353515625 Test Acc: 0.779296875\n",
      "Step 253 -- Train loss: 0.5752541422843933, Train Acc: 0.786865234375 Test Acc: 0.767578125\n",
      "Step 254 -- Train loss: 0.582854151725769, Train Acc: 0.782470703125 Test Acc: 0.7626953125\n",
      "Step 255 -- Train loss: 0.5668604969978333, Train Acc: 0.8037109375 Test Acc: 0.77734375\n",
      "Step 256 -- Train loss: 0.5709848999977112, Train Acc: 0.797607421875 Test Acc: 0.791015625\n",
      "Step 257 -- Train loss: 0.5622329115867615, Train Acc: 0.807373046875 Test Acc: 0.8046875\n",
      "Step 258 -- Train loss: 0.566015899181366, Train Acc: 0.794677734375 Test Acc: 0.7841796875\n",
      "Step 259 -- Train loss: 0.5527869462966919, Train Acc: 0.802734375 Test Acc: 0.802734375\n",
      "Step 260 -- Train loss: 0.5559177994728088, Train Acc: 0.80419921875 Test Acc: 0.8017578125\n",
      "Step 261 -- Train loss: 0.5521596670150757, Train Acc: 0.8193359375 Test Acc: 0.7998046875\n",
      "Step 262 -- Train loss: 0.5467396974563599, Train Acc: 0.83056640625 Test Acc: 0.8017578125\n",
      "Step 263 -- Train loss: 0.5366097688674927, Train Acc: 0.833251953125 Test Acc: 0.8193359375\n",
      "Step 264 -- Train loss: 0.5463671088218689, Train Acc: 0.820556640625 Test Acc: 0.8115234375\n",
      "Step 265 -- Train loss: 0.5357294082641602, Train Acc: 0.835205078125 Test Acc: 0.81640625\n",
      "Step 266 -- Train loss: 0.5366197228431702, Train Acc: 0.824951171875 Test Acc: 0.80859375\n",
      "Step 267 -- Train loss: 0.5375605821609497, Train Acc: 0.8291015625 Test Acc: 0.8134765625\n",
      "Step 268 -- Train loss: 0.5345891714096069, Train Acc: 0.82470703125 Test Acc: 0.814453125\n",
      "Step 269 -- Train loss: 0.5273438692092896, Train Acc: 0.834228515625 Test Acc: 0.8115234375\n",
      "Step 270 -- Train loss: 0.5184321999549866, Train Acc: 0.83837890625 Test Acc: 0.8125\n",
      "Step 271 -- Train loss: 0.5054997205734253, Train Acc: 0.843994140625 Test Acc: 0.8251953125\n",
      "Step 272 -- Train loss: 0.5199635624885559, Train Acc: 0.8330078125 Test Acc: 0.833984375\n",
      "Step 273 -- Train loss: 0.5127206444740295, Train Acc: 0.842529296875 Test Acc: 0.8076171875\n",
      "Step 274 -- Train loss: 0.5116971731185913, Train Acc: 0.847900390625 Test Acc: 0.8349609375\n",
      "Step 275 -- Train loss: 0.5121404528617859, Train Acc: 0.8388671875 Test Acc: 0.8525390625\n",
      "Step 276 -- Train loss: 0.5110136270523071, Train Acc: 0.839111328125 Test Acc: 0.8037109375\n",
      "Step 277 -- Train loss: 0.5038663148880005, Train Acc: 0.85107421875 Test Acc: 0.8359375\n",
      "Step 278 -- Train loss: 0.5031038522720337, Train Acc: 0.843505859375 Test Acc: 0.859375\n",
      "Step 279 -- Train loss: 0.49117210507392883, Train Acc: 0.853515625 Test Acc: 0.8408203125\n",
      "Step 280 -- Train loss: 0.4865700900554657, Train Acc: 0.85400390625 Test Acc: 0.8564453125\n",
      "Step 281 -- Train loss: 0.48324474692344666, Train Acc: 0.861083984375 Test Acc: 0.8525390625\n",
      "Step 282 -- Train loss: 0.48371344804763794, Train Acc: 0.8583984375 Test Acc: 0.841796875\n",
      "Step 283 -- Train loss: 0.4776946008205414, Train Acc: 0.869873046875 Test Acc: 0.85546875\n",
      "Step 284 -- Train loss: 0.4764847159385681, Train Acc: 0.87255859375 Test Acc: 0.8466796875\n",
      "Step 285 -- Train loss: 0.4752200245857239, Train Acc: 0.8701171875 Test Acc: 0.87109375\n",
      "Step 286 -- Train loss: 0.47359126806259155, Train Acc: 0.87646484375 Test Acc: 0.873046875\n",
      "Step 287 -- Train loss: 0.45633792877197266, Train Acc: 0.887451171875 Test Acc: 0.8779296875\n",
      "Step 288 -- Train loss: 0.46717333793640137, Train Acc: 0.86669921875 Test Acc: 0.861328125\n",
      "Step 289 -- Train loss: 0.4542657732963562, Train Acc: 0.871337890625 Test Acc: 0.85546875\n",
      "Step 290 -- Train loss: 0.450078547000885, Train Acc: 0.890625 Test Acc: 0.8583984375\n",
      "Step 291 -- Train loss: 0.445636510848999, Train Acc: 0.896728515625 Test Acc: 0.8740234375\n",
      "Step 292 -- Train loss: 0.4400579035282135, Train Acc: 0.89990234375 Test Acc: 0.8994140625\n",
      "Step 293 -- Train loss: 0.4357167184352875, Train Acc: 0.897705078125 Test Acc: 0.900390625\n",
      "Step 294 -- Train loss: 0.43775632977485657, Train Acc: 0.90234375 Test Acc: 0.892578125\n",
      "Step 295 -- Train loss: 0.4301280975341797, Train Acc: 0.900390625 Test Acc: 0.8798828125\n",
      "Step 296 -- Train loss: 0.4333644211292267, Train Acc: 0.8994140625 Test Acc: 0.9013671875\n",
      "Step 297 -- Train loss: 0.42288804054260254, Train Acc: 0.91357421875 Test Acc: 0.9091796875\n",
      "Step 298 -- Train loss: 0.4225013256072998, Train Acc: 0.91552734375 Test Acc: 0.9130859375\n",
      "Step 299 -- Train loss: 0.410616010427475, Train Acc: 0.915283203125 Test Acc: 0.9033203125\n",
      "Step 300 -- Train loss: 0.4002687633037567, Train Acc: 0.93115234375 Test Acc: 0.9287109375\n",
      "Step 301 -- Train loss: 0.3992493152618408, Train Acc: 0.930908203125 Test Acc: 0.93359375\n",
      "Step 302 -- Train loss: 0.39998137950897217, Train Acc: 0.9189453125 Test Acc: 0.923828125\n",
      "Step 303 -- Train loss: 0.38280734419822693, Train Acc: 0.931884765625 Test Acc: 0.921875\n",
      "Step 304 -- Train loss: 0.39113834500312805, Train Acc: 0.9296875 Test Acc: 0.9345703125\n",
      "Step 305 -- Train loss: 0.38821470737457275, Train Acc: 0.93115234375 Test Acc: 0.927734375\n",
      "Step 306 -- Train loss: 0.38059020042419434, Train Acc: 0.92919921875 Test Acc: 0.931640625\n",
      "Step 307 -- Train loss: 0.38339826464653015, Train Acc: 0.922607421875 Test Acc: 0.9208984375\n",
      "Step 308 -- Train loss: 0.3864550292491913, Train Acc: 0.92578125 Test Acc: 0.927734375\n",
      "Step 309 -- Train loss: 0.36535578966140747, Train Acc: 0.948974609375 Test Acc: 0.943359375\n",
      "Step 310 -- Train loss: 0.3680063784122467, Train Acc: 0.94091796875 Test Acc: 0.9423828125\n",
      "Step 311 -- Train loss: 0.36363065242767334, Train Acc: 0.939697265625 Test Acc: 0.947265625\n",
      "Step 312 -- Train loss: 0.35468703508377075, Train Acc: 0.94580078125 Test Acc: 0.939453125\n",
      "Step 313 -- Train loss: 0.3445677161216736, Train Acc: 0.9462890625 Test Acc: 0.94921875\n",
      "Step 314 -- Train loss: 0.3404141068458557, Train Acc: 0.954833984375 Test Acc: 0.9453125\n",
      "Step 315 -- Train loss: 0.34418749809265137, Train Acc: 0.948486328125 Test Acc: 0.951171875\n",
      "Step 316 -- Train loss: 0.33598148822784424, Train Acc: 0.95361328125 Test Acc: 0.9658203125\n",
      "Step 317 -- Train loss: 0.3293003439903259, Train Acc: 0.96240234375 Test Acc: 0.9609375\n",
      "Step 318 -- Train loss: 0.31976786255836487, Train Acc: 0.970458984375 Test Acc: 0.9658203125\n",
      "Step 319 -- Train loss: 0.317959725856781, Train Acc: 0.964111328125 Test Acc: 0.9755859375\n",
      "Step 320 -- Train loss: 0.31489911675453186, Train Acc: 0.970458984375 Test Acc: 0.96484375\n",
      "Step 321 -- Train loss: 0.30890968441963196, Train Acc: 0.972900390625 Test Acc: 0.9775390625\n",
      "Step 322 -- Train loss: 0.306075781583786, Train Acc: 0.97412109375 Test Acc: 0.966796875\n",
      "Step 323 -- Train loss: 0.29280734062194824, Train Acc: 0.97509765625 Test Acc: 0.9765625\n",
      "Step 324 -- Train loss: 0.29252034425735474, Train Acc: 0.9775390625 Test Acc: 0.98046875\n",
      "Step 325 -- Train loss: 0.2929605543613434, Train Acc: 0.97900390625 Test Acc: 0.9716796875\n",
      "Step 326 -- Train loss: 0.28015729784965515, Train Acc: 0.981201171875 Test Acc: 0.9853515625\n",
      "Step 327 -- Train loss: 0.27445167303085327, Train Acc: 0.98291015625 Test Acc: 0.9775390625\n",
      "Step 328 -- Train loss: 0.2745719850063324, Train Acc: 0.982666015625 Test Acc: 0.978515625\n",
      "Step 329 -- Train loss: 0.26897552609443665, Train Acc: 0.986572265625 Test Acc: 0.9833984375\n",
      "Step 330 -- Train loss: 0.2583691477775574, Train Acc: 0.98876953125 Test Acc: 0.9853515625\n",
      "Step 331 -- Train loss: 0.2526088356971741, Train Acc: 0.98974609375 Test Acc: 0.9873046875\n",
      "Step 332 -- Train loss: 0.2555139362812042, Train Acc: 0.983154296875 Test Acc: 0.9892578125\n",
      "Step 333 -- Train loss: 0.24698898196220398, Train Acc: 0.987060546875 Test Acc: 0.98828125\n",
      "Step 334 -- Train loss: 0.23237496614456177, Train Acc: 0.99267578125 Test Acc: 0.9912109375\n",
      "Step 335 -- Train loss: 0.23070882260799408, Train Acc: 0.990966796875 Test Acc: 0.990234375\n",
      "Step 336 -- Train loss: 0.23474377393722534, Train Acc: 0.98974609375 Test Acc: 0.9912109375\n",
      "Step 337 -- Train loss: 0.22475065290927887, Train Acc: 0.990234375 Test Acc: 0.990234375\n",
      "Step 338 -- Train loss: 0.21415798366069794, Train Acc: 0.99365234375 Test Acc: 0.9921875\n",
      "Step 339 -- Train loss: 0.2129453867673874, Train Acc: 0.9921875 Test Acc: 0.9951171875\n",
      "Step 340 -- Train loss: 0.2112492471933365, Train Acc: 0.993408203125 Test Acc: 0.994140625\n",
      "Step 341 -- Train loss: 0.20382611453533173, Train Acc: 0.99560546875 Test Acc: 1.0\n",
      "Step 342 -- Train loss: 0.19968943297863007, Train Acc: 0.9970703125 Test Acc: 0.994140625\n",
      "Step 343 -- Train loss: 0.1918221414089203, Train Acc: 0.99658203125 Test Acc: 0.9931640625\n",
      "Step 344 -- Train loss: 0.19042746722698212, Train Acc: 0.992919921875 Test Acc: 0.9921875\n",
      "Step 345 -- Train loss: 0.1859685778617859, Train Acc: 0.994873046875 Test Acc: 0.9931640625\n",
      "Step 346 -- Train loss: 0.18187645077705383, Train Acc: 0.994140625 Test Acc: 0.998046875\n",
      "Step 347 -- Train loss: 0.17589247226715088, Train Acc: 0.995849609375 Test Acc: 0.9931640625\n",
      "Step 348 -- Train loss: 0.1684613823890686, Train Acc: 0.9970703125 Test Acc: 0.9990234375\n",
      "Step 349 -- Train loss: 0.16713298857212067, Train Acc: 0.9951171875 Test Acc: 0.9931640625\n",
      "Step 350 -- Train loss: 0.1617434322834015, Train Acc: 0.995849609375 Test Acc: 0.9951171875\n",
      "Step 351 -- Train loss: 0.16087281703948975, Train Acc: 0.992431640625 Test Acc: 0.9951171875\n",
      "Step 352 -- Train loss: 0.15080951154232025, Train Acc: 0.995849609375 Test Acc: 0.9951171875\n",
      "Step 353 -- Train loss: 0.14925667643547058, Train Acc: 0.99755859375 Test Acc: 0.9970703125\n",
      "Step 354 -- Train loss: 0.14762429893016815, Train Acc: 0.997802734375 Test Acc: 0.998046875\n",
      "Step 355 -- Train loss: 0.1408892720937729, Train Acc: 0.99853515625 Test Acc: 1.0\n",
      "Step 356 -- Train loss: 0.13995398581027985, Train Acc: 0.997314453125 Test Acc: 0.9990234375\n",
      "Step 357 -- Train loss: 0.13582843542099, Train Acc: 0.998046875 Test Acc: 0.9990234375\n",
      "Step 358 -- Train loss: 0.1268777847290039, Train Acc: 0.998779296875 Test Acc: 0.9990234375\n",
      "Step 359 -- Train loss: 0.12558422982692719, Train Acc: 0.998291015625 Test Acc: 0.9990234375\n",
      "Step 360 -- Train loss: 0.12516309320926666, Train Acc: 0.999267578125 Test Acc: 0.998046875\n",
      "Step 361 -- Train loss: 0.12161749601364136, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 362 -- Train loss: 0.11654847860336304, Train Acc: 0.999267578125 Test Acc: 0.9990234375\n",
      "Step 363 -- Train loss: 0.11467146873474121, Train Acc: 0.9990234375 Test Acc: 1.0\n",
      "Step 364 -- Train loss: 0.10959227383136749, Train Acc: 0.999267578125 Test Acc: 0.9990234375\n",
      "Step 365 -- Train loss: 0.10836954414844513, Train Acc: 0.99853515625 Test Acc: 0.9990234375\n",
      "Step 366 -- Train loss: 0.10594762861728668, Train Acc: 0.999267578125 Test Acc: 0.998046875\n",
      "Step 367 -- Train loss: 0.10204645991325378, Train Acc: 0.9990234375 Test Acc: 0.99609375\n",
      "Step 368 -- Train loss: 0.09905970096588135, Train Acc: 0.9990234375 Test Acc: 0.9990234375\n",
      "Step 369 -- Train loss: 0.09656287729740143, Train Acc: 0.99951171875 Test Acc: 1.0\n",
      "Step 370 -- Train loss: 0.09665552526712418, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 371 -- Train loss: 0.09369444102048874, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 372 -- Train loss: 0.09017537534236908, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 373 -- Train loss: 0.08870327472686768, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 374 -- Train loss: 0.08623036742210388, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 375 -- Train loss: 0.08320270478725433, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 376 -- Train loss: 0.08160324394702911, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 377 -- Train loss: 0.07940907776355743, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 378 -- Train loss: 0.07771077752113342, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 379 -- Train loss: 0.07552669942378998, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 380 -- Train loss: 0.07279231399297714, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 381 -- Train loss: 0.07311289757490158, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 382 -- Train loss: 0.0708829015493393, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 383 -- Train loss: 0.06789413094520569, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 384 -- Train loss: 0.06706907600164413, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 385 -- Train loss: 0.06618083268404007, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 386 -- Train loss: 0.0641820877790451, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 387 -- Train loss: 0.06157100945711136, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 388 -- Train loss: 0.06098145991563797, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 389 -- Train loss: 0.060069113969802856, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 390 -- Train loss: 0.05956273525953293, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 391 -- Train loss: 0.05909688398241997, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 392 -- Train loss: 0.056558530777692795, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 393 -- Train loss: 0.05603363364934921, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 394 -- Train loss: 0.053815703839063644, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 395 -- Train loss: 0.05226130411028862, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 396 -- Train loss: 0.0519220232963562, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 397 -- Train loss: 0.05132622644305229, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 398 -- Train loss: 0.049582429230213165, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 399 -- Train loss: 0.04875153675675392, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 400 -- Train loss: 0.048722345381975174, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 401 -- Train loss: 0.04837372899055481, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 402 -- Train loss: 0.04662762209773064, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 403 -- Train loss: 0.045602478086948395, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 404 -- Train loss: 0.04546185955405235, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 405 -- Train loss: 0.04441390559077263, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 406 -- Train loss: 0.04391544312238693, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 407 -- Train loss: 0.042827997356653214, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 408 -- Train loss: 0.04203301668167114, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 409 -- Train loss: 0.04222291335463524, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 410 -- Train loss: 0.04055488854646683, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 411 -- Train loss: 0.040058404207229614, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 412 -- Train loss: 0.040024757385253906, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 413 -- Train loss: 0.03814452886581421, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 414 -- Train loss: 0.03820724040269852, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 415 -- Train loss: 0.03779005631804466, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 416 -- Train loss: 0.03720054030418396, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 417 -- Train loss: 0.03627016767859459, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 418 -- Train loss: 0.03673408553004265, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 419 -- Train loss: 0.0353858545422554, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 420 -- Train loss: 0.03514547273516655, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 421 -- Train loss: 0.03476428985595703, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 422 -- Train loss: 0.03384434059262276, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 423 -- Train loss: 0.033739637583494186, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 424 -- Train loss: 0.03353780880570412, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 425 -- Train loss: 0.032906949520111084, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 426 -- Train loss: 0.032509416341781616, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 427 -- Train loss: 0.0329873152077198, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 428 -- Train loss: 0.031290024518966675, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 429 -- Train loss: 0.031188271939754486, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 430 -- Train loss: 0.030583186075091362, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 431 -- Train loss: 0.03071701154112816, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 432 -- Train loss: 0.030030155554413795, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 433 -- Train loss: 0.029878417029976845, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 434 -- Train loss: 0.029155820608139038, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 435 -- Train loss: 0.029037147760391235, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 436 -- Train loss: 0.028741544112563133, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 437 -- Train loss: 0.027934715151786804, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 438 -- Train loss: 0.028080381453037262, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 439 -- Train loss: 0.02746647596359253, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 440 -- Train loss: 0.027692316100001335, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 441 -- Train loss: 0.026602819561958313, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 442 -- Train loss: 0.026584897190332413, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 443 -- Train loss: 0.02613060735166073, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 444 -- Train loss: 0.026117801666259766, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 445 -- Train loss: 0.02551950141787529, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 446 -- Train loss: 0.026054680347442627, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 447 -- Train loss: 0.025401491671800613, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 448 -- Train loss: 0.025280427187681198, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 449 -- Train loss: 0.0254005566239357, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 450 -- Train loss: 0.024877015501260757, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 451 -- Train loss: 0.02434219792485237, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 452 -- Train loss: 0.02405410259962082, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 453 -- Train loss: 0.023889033123850822, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 454 -- Train loss: 0.023539189249277115, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 455 -- Train loss: 0.023520195856690407, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 456 -- Train loss: 0.023388870060443878, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 457 -- Train loss: 0.02295185625553131, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 458 -- Train loss: 0.022669592872262, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 459 -- Train loss: 0.022499239072203636, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 460 -- Train loss: 0.021981829777359962, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 461 -- Train loss: 0.022200699895620346, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 462 -- Train loss: 0.021917805075645447, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 463 -- Train loss: 0.021295109763741493, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 464 -- Train loss: 0.0216421689838171, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 465 -- Train loss: 0.02136491984128952, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 466 -- Train loss: 0.020637936890125275, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 467 -- Train loss: 0.02067277766764164, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 468 -- Train loss: 0.020720159634947777, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 469 -- Train loss: 0.020488843321800232, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 470 -- Train loss: 0.020130427554249763, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 471 -- Train loss: 0.01970801316201687, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 472 -- Train loss: 0.02016006037592888, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 473 -- Train loss: 0.01994933933019638, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 474 -- Train loss: 0.01964385062456131, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 475 -- Train loss: 0.01963583007454872, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 476 -- Train loss: 0.019389642402529716, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 477 -- Train loss: 0.01903114840388298, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 478 -- Train loss: 0.018973954021930695, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 479 -- Train loss: 0.018814971670508385, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 480 -- Train loss: 0.018391327932476997, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 481 -- Train loss: 0.018498389050364494, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 482 -- Train loss: 0.018306700512766838, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 483 -- Train loss: 0.018196100369095802, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 484 -- Train loss: 0.018034178763628006, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 485 -- Train loss: 0.01810191385447979, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 486 -- Train loss: 0.0178680382668972, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 487 -- Train loss: 0.017422087490558624, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 488 -- Train loss: 0.017516003921628, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 489 -- Train loss: 0.017499269917607307, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 490 -- Train loss: 0.017442582175135612, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 491 -- Train loss: 0.016785213723778725, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 492 -- Train loss: 0.016961373388767242, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 493 -- Train loss: 0.01680680178105831, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 494 -- Train loss: 0.01686233840882778, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 495 -- Train loss: 0.016461290419101715, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 496 -- Train loss: 0.01656796969473362, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 497 -- Train loss: 0.016343338415026665, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 498 -- Train loss: 0.016535820439457893, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 499 -- Train loss: 0.01602831482887268, Train Acc: 1.0 Test Acc: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>att_prog_measure</td><td></td></tr><tr><td>data_repeat_frac</td><td></td></tr><tr><td>idx0_check</td><td></td></tr><tr><td>idx10_check</td><td></td></tr><tr><td>idx11_check</td><td></td></tr><tr><td>idx12_check</td><td></td></tr><tr><td>idx13_check</td><td></td></tr><tr><td>idx14_check</td><td></td></tr><tr><td>idx15_check</td><td></td></tr><tr><td>idx1_check</td><td></td></tr><tr><td>idx2_check</td><td></td></tr><tr><td>idx3_check</td><td></td></tr><tr><td>idx4_check</td><td></td></tr><tr><td>idx5_check</td><td></td></tr><tr><td>idx6_check</td><td></td></tr><tr><td>idx7_check</td><td></td></tr><tr><td>idx8_check</td><td></td></tr><tr><td>idx9_check</td><td></td></tr><tr><td>mean_cosine_sim</td><td></td></tr><tr><td>mean_cosine_sim_0</td><td></td></tr><tr><td>mean_cosine_sim_1</td><td></td></tr><tr><td>mean_cosine_sim_10</td><td></td></tr><tr><td>mean_cosine_sim_11</td><td></td></tr><tr><td>mean_cosine_sim_12</td><td></td></tr><tr><td>mean_cosine_sim_13</td><td></td></tr><tr><td>mean_cosine_sim_14</td><td></td></tr><tr><td>mean_cosine_sim_2</td><td></td></tr><tr><td>mean_cosine_sim_3</td><td></td></tr><tr><td>mean_cosine_sim_4</td><td></td></tr><tr><td>mean_cosine_sim_5</td><td></td></tr><tr><td>mean_cosine_sim_6</td><td></td></tr><tr><td>mean_cosine_sim_7</td><td></td></tr><tr><td>mean_cosine_sim_8</td><td></td></tr><tr><td>mean_cosine_sim_9</td><td></td></tr><tr><td>model_repeat_frac</td><td></td></tr><tr><td>test_acc</td><td></td></tr><tr><td>train_acc</td><td></td></tr><tr><td>train_loss</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>att_prog_measure</td><td>0.73029</td></tr><tr><td>data_repeat_frac</td><td>0.05</td></tr><tr><td>idx0_check</td><td>1.0</td></tr><tr><td>idx10_check</td><td>1.0</td></tr><tr><td>idx11_check</td><td>1.0</td></tr><tr><td>idx12_check</td><td>1.0</td></tr><tr><td>idx13_check</td><td>1.0</td></tr><tr><td>idx14_check</td><td>1.0</td></tr><tr><td>idx15_check</td><td>1.0</td></tr><tr><td>idx1_check</td><td>1.0</td></tr><tr><td>idx2_check</td><td>1.0</td></tr><tr><td>idx3_check</td><td>1.0</td></tr><tr><td>idx4_check</td><td>1.0</td></tr><tr><td>idx5_check</td><td>1.0</td></tr><tr><td>idx6_check</td><td>1.0</td></tr><tr><td>idx7_check</td><td>1.0</td></tr><tr><td>idx8_check</td><td>1.0</td></tr><tr><td>idx9_check</td><td>1.0</td></tr><tr><td>mean_cosine_sim</td><td>0.01072</td></tr><tr><td>mean_cosine_sim_0</td><td>0.00047</td></tr><tr><td>mean_cosine_sim_1</td><td>0.0092</td></tr><tr><td>mean_cosine_sim_10</td><td>0.01397</td></tr><tr><td>mean_cosine_sim_11</td><td>0.01297</td></tr><tr><td>mean_cosine_sim_12</td><td>0.00767</td></tr><tr><td>mean_cosine_sim_13</td><td>0.01699</td></tr><tr><td>mean_cosine_sim_14</td><td>0.02485</td></tr><tr><td>mean_cosine_sim_2</td><td>0.008</td></tr><tr><td>mean_cosine_sim_3</td><td>0.0102</td></tr><tr><td>mean_cosine_sim_4</td><td>0.00502</td></tr><tr><td>mean_cosine_sim_5</td><td>0.01263</td></tr><tr><td>mean_cosine_sim_6</td><td>0.00559</td></tr><tr><td>mean_cosine_sim_7</td><td>0.00814</td></tr><tr><td>mean_cosine_sim_8</td><td>0.02096</td></tr><tr><td>mean_cosine_sim_9</td><td>0.02047</td></tr><tr><td>model_repeat_frac</td><td>0.05</td></tr><tr><td>test_acc</td><td>1.0</td></tr><tr><td>train_acc</td><td>1.0</td></tr><tr><td>train_loss</td><td>0.01603</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">mws_mwp_linear_frozen_embedding</strong> at: <a href='https://wandb.ai/wth_ucsd/loss_plateau_tf/runs/d2mhf1co' target=\"_blank\">https://wandb.ai/wth_ucsd/loss_plateau_tf/runs/d2mhf1co</a><br/>Synced 6 W&B file(s), 500 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251107_013333-d2mhf1co/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_samplers = {}\n",
    "data_samplers['mws'] = data.MovingWindowSum(\n",
    "    min_num=config.data.min_num,\n",
    "    max_num=config.data.max_num,\n",
    "    k=config.data.k,\n",
    "    p=config.data.p,\n",
    "    sep = 17,\n",
    ")\n",
    "data_samplers['mwp'] = data.MovingWindowProduct(\n",
    "    min_num=config.data.min_num,\n",
    "    max_num=config.data.max_num,\n",
    "    k=config.data.k,\n",
    "    p=config.data.p,\n",
    "    sep = 0,\n",
    ")\n",
    "config.model.n_head = 1\n",
    "model = GPTLinear(config.model, return_att=True).to(device)\n",
    "\n",
    "## Freeze embedding layer weights\n",
    "for param in model.transformer.wte.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in model.transformer.wpe.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "optim = Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=config.train.lr)\n",
    "\n",
    "\n",
    "if config.train.wandb:\n",
    "    wandb_run_name = 'mws_mwp_linear_frozen_embedding'\n",
    "    wandb.login(key=\"\")\n",
    "    wandb.init(project=\"loss_plateau_tf\", name=wandb_run_name, config=config)\n",
    "    wandb.watch(model)\n",
    "\n",
    "for step in range(config.train.num_steps):\n",
    "    train_step(\n",
    "        model=model,\n",
    "        optim=optim,\n",
    "        data_samplers=data_samplers,\n",
    "        step=step,\n",
    "        config=config,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "if config.train.wandb:\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
