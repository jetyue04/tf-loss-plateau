{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "YiF5Vq1LGhEw"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import yaml\n",
    "import argparse\n",
    "from dotmap import DotMap\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.optim import Adam\n",
    "from torch.nn.functional import cosine_similarity\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"./src\")  # make sure Python can find src/\n",
    "from model_minimal import GPTLinear, GPTSoftmax\n",
    "from data import MovingWindowSum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "\n",
    "class MovingWindowSumNoMod:\n",
    "    def __init__(self, min_num=1, max_num=16, k=2, p=17, sep=17, device=\"cuda\"):\n",
    "        self.min_num = min_num\n",
    "        self.max_num = max_num\n",
    "        self.k = k\n",
    "        self.p = p\n",
    "        self.sep = sep\n",
    "        self.device = device\n",
    "        assert self.p > self.max_num\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def sample(\n",
    "        self,\n",
    "        num_samples,\n",
    "        num_tokens,\n",
    "    ):\n",
    "        random_ints = torch.randint(\n",
    "            low=self.min_num, high=self.max_num + 1, size=(num_samples, num_tokens)\n",
    "        ).to(self.device)\n",
    "\n",
    "        random_ints_np = random_ints.detach().cpu().numpy()\n",
    "        convolution = torch.stack(\n",
    "            [\n",
    "                torch.from_numpy(\n",
    "                    np.convolve(\n",
    "                        random_ints_np[i],\n",
    "                        np.ones(self.k),\n",
    "                        mode=\"valid\",\n",
    "                    )\n",
    "                )\n",
    "                for i in range(random_ints.shape[0])\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        moving_sum = random_ints.clone().detach()\n",
    "        moving_sum[:, self.k - 1 :] = convolution\n",
    "\n",
    "        # for i in range(num_samples):\n",
    "        #     for j in range(0, self.k - 1):\n",
    "        #         if moving_sum[i, j] != random_ints[i, j]:\n",
    "        #             print(f\"ERROR! {i} {j}\")\n",
    "        #     for j in range(self.k - 1, num_tokens):\n",
    "        #         if moving_sum[i, j] != torch.sum(random_ints[i, j-self.k+1:j+1]):\n",
    "        #             print(f\"ERROR! {i} {j}\")\n",
    "\n",
    "        # exit()\n",
    "        samples = (\n",
    "            torch.cat(\n",
    "                [\n",
    "                    random_ints,\n",
    "                    self.sep * torch.ones(size=(num_samples, 1)).to(self.device),\n",
    "                    moving_sum,\n",
    "                ],\n",
    "                axis=-1,\n",
    "            )\n",
    "            .to(int)\n",
    "            .detach()\n",
    "        )\n",
    "\n",
    "        return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "def train_step(\n",
    "    model,\n",
    "    optim,\n",
    "    data_sampler,\n",
    "    step,\n",
    "    config,\n",
    "):\n",
    "    n_train, n_test, num_tokens = (\n",
    "        config.data.n_train,\n",
    "        config.data.n_test,\n",
    "        config.data.num_tokens,\n",
    "    )\n",
    "\n",
    "    data = data_sampler.sample(\n",
    "        num_samples=n_train + n_test,\n",
    "        num_tokens=num_tokens,\n",
    "    )\n",
    "\n",
    "    train_data = data[:n_train, :]\n",
    "    test_data = data[n_train:, :]\n",
    "\n",
    "    prompt_len = num_tokens + 1\n",
    "    gen_len = num_tokens\n",
    "    acc_start = num_tokens + 1\n",
    "\n",
    "    model.train()\n",
    "    optim.zero_grad(set_to_none=True)\n",
    "\n",
    "    _, _, _, loss = model(\n",
    "        train_data[:, :-1], targets=train_data[:, 1:], prompt_len =prompt_len,\n",
    "    )\n",
    "    loss.backward()\n",
    "\n",
    "    if config.train.grad_clip > 0:\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), config.train.grad_clip)\n",
    "\n",
    "    optim.step()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Log train loss, train / test acc, repetition frequency\n",
    "        attn_map, pre_lm_h, _, train_loss = model(train_data[:, :-1], targets=train_data[:, 1:], prompt_len =prompt_len,)\n",
    "\n",
    "        train_pred = model.generate(\n",
    "            idx=train_data[:, :prompt_len],\n",
    "            max_new_tokens=gen_len,\n",
    "            prompt_len =prompt_len,\n",
    "        )\n",
    "        test_pred = model.generate(\n",
    "            idx=test_data[:, :prompt_len],\n",
    "            max_new_tokens=gen_len,\n",
    "            prompt_len =prompt_len,\n",
    "        )\n",
    "\n",
    "        train_acc = torch.mean(\n",
    "            (train_pred[:, acc_start:] == train_data[:, acc_start:]).to(float)\n",
    "        ).item()\n",
    "        test_acc = torch.mean(\n",
    "            (test_pred[:, acc_start:] == test_data[:, acc_start:]).to(float)\n",
    "        ).item()\n",
    "\n",
    "        data_repeat_frac = torch.mean((test_data[:, acc_start:-1] == test_data[:, acc_start+1:]).to(float))\n",
    "        model_repeat_frac = torch.mean((test_pred[:, acc_start:-1] == test_pred[:, acc_start+1:]).to(float))\n",
    "\n",
    "        # Log attention progress measure\n",
    "        attn_map_output_seq = attn_map[:, :, acc_start-1:]\n",
    "        att_mask = torch.zeros_like(attn_map_output_seq).to(device)\n",
    "\n",
    "        att_mask[:, :, 0, 0] = 1\n",
    "        for i in range(num_tokens - 1):\n",
    "            att_mask[:, :, i + 1, i : i + 2] = 1\n",
    "\n",
    "        att_prog_measure = torch.mean(\n",
    "            torch.sum(torch.abs(attn_map_output_seq) * att_mask, dim=(-3, -2, -1)) /\n",
    "            torch.sum(torch.abs(attn_map_output_seq), dim=(-3, -2, -1)),\n",
    "            dim=0\n",
    "        )\n",
    "\n",
    "        # Log pair-wise cosine similarity between hidden states\n",
    "        embed_start = acc_start - 1\n",
    "        embed_len = gen_len\n",
    "\n",
    "        logit_cs = torch.zeros((embed_len, embed_len))\n",
    "\n",
    "        for i_1 in range(embed_start, embed_start + embed_len):\n",
    "            for i_2 in range(embed_start, i_1):\n",
    "                logit_cs[i_1 - embed_start, i_2 - embed_start] = torch.mean(\n",
    "                    (\n",
    "                        cosine_similarity(\n",
    "                            pre_lm_h[:, i_1, :], pre_lm_h[:, i_2, :], dim=-1\n",
    "                        )\n",
    "                    ), dim=0\n",
    "                )\n",
    "\n",
    "        # Log plots for cosine similarity, attention map\n",
    "        logit_fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(30, 15))\n",
    "\n",
    "        im1 = ax[0].imshow(logit_cs)\n",
    "        ax[0].set_title(\"avg pre_lm_h cosine sim\")\n",
    "        cb1 = logit_fig.colorbar(im1, location=\"right\", shrink=0.99, pad=0.02, ax=ax[0])\n",
    "\n",
    "        avg_attn_map = torch.mean(attn_map, dim=0).squeeze().detach().cpu().numpy()\n",
    "\n",
    "        im2 = ax[1].imshow(avg_attn_map)\n",
    "        ax[1].set_title(\"att map\")\n",
    "        cb4 = logit_fig.colorbar(im2, location=\"right\", shrink=0.99, pad=0.02, ax=ax[1])\n",
    "        ax[1].set_xticks(range(avg_attn_map.shape[-1]))\n",
    "        ax[1].set_yticks(range(avg_attn_map.shape[-2]))\n",
    "\n",
    "        for i1 in range(embed_len):\n",
    "            for i2 in range(embed_len):\n",
    "                text1 = ax[0].text(\n",
    "                    i2,\n",
    "                    i1,\n",
    "                    round(logit_cs[i1, i2].item(), 2),\n",
    "                    ha=\"center\",\n",
    "                    va=\"center\",\n",
    "                    color=\"w\",\n",
    "                )\n",
    "\n",
    "\n",
    "        print(\n",
    "            f\"Step {step} -- Train loss: {train_loss}, Train Acc: {train_acc} Test Acc: {test_acc}\"\n",
    "        )\n",
    "        # print(f\"input: {test_data[0]} \\n predicted:{test_pred[0]}\")\n",
    "\n",
    "        if config.train.wandb:\n",
    "\n",
    "            log_data = {\n",
    "                \"train_loss\": train_loss,\n",
    "                \"train_acc\": train_acc,\n",
    "                \"test_acc\": test_acc,\n",
    "                \"data_repeat_frac\": data_repeat_frac,\n",
    "                \"model_repeat_frac\": model_repeat_frac,\n",
    "                \"att_prog_measure\": att_prog_measure,\n",
    "                \"pre_lm_h_cosine_sim\": logit_fig,\n",
    "                \"mean_cosine_sim\": torch.sum(logit_cs[:, 1:]) / (0.5 * (gen_len-1) * (gen_len-2))\n",
    "            }\n",
    "\n",
    "            for output_pos in range(gen_len):\n",
    "                log_data.update(\n",
    "                    {\n",
    "                        f\"idx{output_pos}_check\": torch.mean(\n",
    "                            (train_pred[:, acc_start + output_pos] == train_data[:, acc_start + output_pos]).to(float)\n",
    "                        ).item()\n",
    "                    }\n",
    "                )\n",
    "\n",
    "                if output_pos < gen_len-1:\n",
    "                    log_data.update(\n",
    "                        {\n",
    "                            f\"mean_cosine_sim_{output_pos}\": torch.sum(logit_cs[:, output_pos]) / (gen_len-1-output_pos)\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "            wandb.log(log_data)\n",
    "\n",
    "        plt.close()\n",
    "        del (\n",
    "            logit_fig,\n",
    "            ax,\n",
    "            logit_cs,\n",
    "        )\n",
    "\n",
    "        if config.train.save_ckpt:\n",
    "            if (step == 0) or ((step + 1) % config.train.ckpt_freq == 0):\n",
    "                model.train()\n",
    "                torch.save(\n",
    "                    {\n",
    "                        \"epoch\": step,\n",
    "                        \"model\": model.state_dict(),\n",
    "                        \"optim\": optim.state_dict(),\n",
    "                        \"train_loss\": train_loss,\n",
    "                        \"test_acc\": test_acc,\n",
    "                    },\n",
    "                    \"./mws_k2_l1_h1_a16_n16.tar\",\n",
    "                )\n",
    "                print(f\"saved state at epoch {step} to {f'./mws_k2_l1_h1_a16_n16.tar'}\")\n",
    "\n",
    "                if config.train.wandb:\n",
    "                    model_wandb = wandb.Artifact(\n",
    "                        f\"model_step{step}\", type=\"model\"\n",
    "                    )\n",
    "                    model_wandb.add_file(f\"./mws_k2_l1_h1_a16_n16.tar\")\n",
    "                    wandb.log_artifact(model_wandb)\n",
    "                    print(\"model uploaded to wandb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "config = {\n",
    "'model':\n",
    "  {\n",
    "    'n_layer': 1,\n",
    "    'n_head': 1,\n",
    "    'n_embd': 256,\n",
    "    'linear': True,\n",
    "  },\n",
    "\n",
    "'data':\n",
    "  {\n",
    "    'name': 'window',\n",
    "    'min_num': 1,\n",
    "    'max_num': 16,\n",
    "    'k': 2,\n",
    "    'p': 17,\n",
    "    'sep': 17,\n",
    "    'cot': False,\n",
    "    'num_tokens': 16,\n",
    "    'n_train': 256,\n",
    "    'n_test': 64,\n",
    "    'fixed_len': True,\n",
    "  },\n",
    "\n",
    "'train':\n",
    "  {\n",
    "    'lr': 0.0001,\n",
    "    'grad_clip': -1,\n",
    "    'num_steps': 500,\n",
    "    'norm_type': \"none_rank\",\n",
    "    'wandb': True,\n",
    "    'save_ckpt': False,\n",
    "    'ckpt_freq': 20,\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.22.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jyue/private/tf-loss-plateau/wandb/run-20251104_002438-qsc08oxb</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/wth_ucsd/loss_plateau_tf/runs/qsc08oxb' target=\"_blank\">mws_linear_noMLP_noMod_1000steps</a></strong> to <a href='https://wandb.ai/wth_ucsd/loss_plateau_tf' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/wth_ucsd/loss_plateau_tf' target=\"_blank\">https://wandb.ai/wth_ucsd/loss_plateau_tf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/wth_ucsd/loss_plateau_tf/runs/qsc08oxb' target=\"_blank\">https://wandb.ai/wth_ucsd/loss_plateau_tf/runs/qsc08oxb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0 -- Train loss: 3.5780763626098633, Train Acc: 0.032470703125 Test Acc: 0.03515625\n",
      "Step 1 -- Train loss: 3.570441246032715, Train Acc: 0.03759765625 Test Acc: 0.0341796875\n",
      "Step 2 -- Train loss: 3.5452446937561035, Train Acc: 0.039306640625 Test Acc: 0.0390625\n",
      "Step 3 -- Train loss: 3.5041933059692383, Train Acc: 0.042236328125 Test Acc: 0.03125\n",
      "Step 4 -- Train loss: 3.476463794708252, Train Acc: 0.048095703125 Test Acc: 0.046875\n",
      "Step 5 -- Train loss: 3.4347543716430664, Train Acc: 0.04931640625 Test Acc: 0.0673828125\n",
      "Step 6 -- Train loss: 3.416731357574463, Train Acc: 0.05419921875 Test Acc: 0.0478515625\n",
      "Step 7 -- Train loss: 3.3794291019439697, Train Acc: 0.052001953125 Test Acc: 0.048828125\n",
      "Step 8 -- Train loss: 3.3569788932800293, Train Acc: 0.06591796875 Test Acc: 0.0576171875\n",
      "Step 9 -- Train loss: 3.344305992126465, Train Acc: 0.057373046875 Test Acc: 0.056640625\n",
      "Step 10 -- Train loss: 3.336804151535034, Train Acc: 0.053955078125 Test Acc: 0.0576171875\n",
      "Step 11 -- Train loss: 3.315269947052002, Train Acc: 0.060546875 Test Acc: 0.07421875\n",
      "Step 12 -- Train loss: 3.302884101867676, Train Acc: 0.0517578125 Test Acc: 0.0546875\n",
      "Step 13 -- Train loss: 3.2941455841064453, Train Acc: 0.060302734375 Test Acc: 0.0576171875\n",
      "Step 14 -- Train loss: 3.2786705493927, Train Acc: 0.0576171875 Test Acc: 0.056640625\n",
      "Step 15 -- Train loss: 3.273989200592041, Train Acc: 0.062255859375 Test Acc: 0.048828125\n",
      "Step 16 -- Train loss: 3.2485158443450928, Train Acc: 0.063232421875 Test Acc: 0.060546875\n",
      "Step 17 -- Train loss: 3.260653018951416, Train Acc: 0.061279296875 Test Acc: 0.060546875\n",
      "Step 18 -- Train loss: 3.252464771270752, Train Acc: 0.055908203125 Test Acc: 0.064453125\n",
      "Step 19 -- Train loss: 3.2449920177459717, Train Acc: 0.0595703125 Test Acc: 0.0634765625\n",
      "Step 20 -- Train loss: 3.241060733795166, Train Acc: 0.061767578125 Test Acc: 0.041015625\n",
      "Step 21 -- Train loss: 3.2236392498016357, Train Acc: 0.0537109375 Test Acc: 0.0498046875\n",
      "Step 22 -- Train loss: 3.2214369773864746, Train Acc: 0.056640625 Test Acc: 0.068359375\n",
      "Step 23 -- Train loss: 3.2047345638275146, Train Acc: 0.0615234375 Test Acc: 0.0673828125\n",
      "Step 24 -- Train loss: 3.202745199203491, Train Acc: 0.05908203125 Test Acc: 0.0654296875\n",
      "Step 25 -- Train loss: 3.1942882537841797, Train Acc: 0.066162109375 Test Acc: 0.056640625\n",
      "Step 26 -- Train loss: 3.198293924331665, Train Acc: 0.057861328125 Test Acc: 0.0673828125\n",
      "Step 27 -- Train loss: 3.188016176223755, Train Acc: 0.06201171875 Test Acc: 0.0517578125\n",
      "Step 28 -- Train loss: 3.185466766357422, Train Acc: 0.068115234375 Test Acc: 0.0712890625\n",
      "Step 29 -- Train loss: 3.1869122982025146, Train Acc: 0.064208984375 Test Acc: 0.0576171875\n",
      "Step 30 -- Train loss: 3.166994571685791, Train Acc: 0.063232421875 Test Acc: 0.0625\n",
      "Step 31 -- Train loss: 3.1695315837860107, Train Acc: 0.065673828125 Test Acc: 0.060546875\n",
      "Step 32 -- Train loss: 3.1554298400878906, Train Acc: 0.0673828125 Test Acc: 0.0712890625\n",
      "Step 33 -- Train loss: 3.1547324657440186, Train Acc: 0.0673828125 Test Acc: 0.072265625\n",
      "Step 34 -- Train loss: 3.1651699542999268, Train Acc: 0.068359375 Test Acc: 0.06640625\n",
      "Step 35 -- Train loss: 3.1515347957611084, Train Acc: 0.069091796875 Test Acc: 0.0810546875\n",
      "Step 36 -- Train loss: 3.152895450592041, Train Acc: 0.06787109375 Test Acc: 0.05078125\n",
      "Step 37 -- Train loss: 3.1403086185455322, Train Acc: 0.070068359375 Test Acc: 0.0576171875\n",
      "Step 38 -- Train loss: 3.1518590450286865, Train Acc: 0.075439453125 Test Acc: 0.0634765625\n",
      "Step 39 -- Train loss: 3.1405105590820312, Train Acc: 0.0693359375 Test Acc: 0.0732421875\n",
      "Step 40 -- Train loss: 3.1342921257019043, Train Acc: 0.072021484375 Test Acc: 0.08203125\n",
      "Step 41 -- Train loss: 3.124558210372925, Train Acc: 0.080810546875 Test Acc: 0.0830078125\n",
      "Step 42 -- Train loss: 3.133236885070801, Train Acc: 0.072265625 Test Acc: 0.078125\n",
      "Step 43 -- Train loss: 3.136695623397827, Train Acc: 0.077392578125 Test Acc: 0.068359375\n",
      "Step 44 -- Train loss: 3.128558397293091, Train Acc: 0.077880859375 Test Acc: 0.08203125\n",
      "Step 45 -- Train loss: 3.115492105484009, Train Acc: 0.083740234375 Test Acc: 0.0888671875\n",
      "Step 46 -- Train loss: 3.110826253890991, Train Acc: 0.0771484375 Test Acc: 0.0791015625\n",
      "Step 47 -- Train loss: 3.117795705795288, Train Acc: 0.083251953125 Test Acc: 0.0703125\n",
      "Step 48 -- Train loss: 3.1099493503570557, Train Acc: 0.0771484375 Test Acc: 0.0947265625\n",
      "Step 49 -- Train loss: 3.1111652851104736, Train Acc: 0.08154296875 Test Acc: 0.09375\n",
      "Step 50 -- Train loss: 3.0943572521209717, Train Acc: 0.08740234375 Test Acc: 0.080078125\n",
      "Step 51 -- Train loss: 3.101881504058838, Train Acc: 0.091064453125 Test Acc: 0.0791015625\n",
      "Step 52 -- Train loss: 3.0774667263031006, Train Acc: 0.096923828125 Test Acc: 0.095703125\n",
      "Step 53 -- Train loss: 3.079582691192627, Train Acc: 0.08935546875 Test Acc: 0.0869140625\n",
      "Step 54 -- Train loss: 3.067044734954834, Train Acc: 0.095458984375 Test Acc: 0.109375\n",
      "Step 55 -- Train loss: 3.0792880058288574, Train Acc: 0.093994140625 Test Acc: 0.1025390625\n",
      "Step 56 -- Train loss: 3.065768241882324, Train Acc: 0.102294921875 Test Acc: 0.1005859375\n",
      "Step 57 -- Train loss: 3.0633482933044434, Train Acc: 0.099365234375 Test Acc: 0.1005859375\n",
      "Step 58 -- Train loss: 3.054471015930176, Train Acc: 0.101318359375 Test Acc: 0.0986328125\n",
      "Step 59 -- Train loss: 3.057194232940674, Train Acc: 0.1015625 Test Acc: 0.109375\n",
      "Step 60 -- Train loss: 3.039470672607422, Train Acc: 0.1103515625 Test Acc: 0.1083984375\n",
      "Step 61 -- Train loss: 3.038074016571045, Train Acc: 0.116455078125 Test Acc: 0.111328125\n",
      "Step 62 -- Train loss: 3.030212640762329, Train Acc: 0.10546875 Test Acc: 0.1142578125\n",
      "Step 63 -- Train loss: 3.0347752571105957, Train Acc: 0.112060546875 Test Acc: 0.0908203125\n",
      "Step 64 -- Train loss: 3.0064003467559814, Train Acc: 0.11083984375 Test Acc: 0.109375\n",
      "Step 65 -- Train loss: 3.012251615524292, Train Acc: 0.11572265625 Test Acc: 0.1044921875\n",
      "Step 66 -- Train loss: 3.007833242416382, Train Acc: 0.11181640625 Test Acc: 0.109375\n",
      "Step 67 -- Train loss: 3.014634847640991, Train Acc: 0.10986328125 Test Acc: 0.1044921875\n",
      "Step 68 -- Train loss: 3.001459836959839, Train Acc: 0.119140625 Test Acc: 0.109375\n",
      "Step 69 -- Train loss: 3.002070665359497, Train Acc: 0.110107421875 Test Acc: 0.111328125\n",
      "Step 70 -- Train loss: 2.9940874576568604, Train Acc: 0.11083984375 Test Acc: 0.119140625\n",
      "Step 71 -- Train loss: 2.9927895069122314, Train Acc: 0.1142578125 Test Acc: 0.1181640625\n",
      "Step 72 -- Train loss: 2.9876933097839355, Train Acc: 0.1103515625 Test Acc: 0.1171875\n",
      "Step 73 -- Train loss: 2.987898826599121, Train Acc: 0.11279296875 Test Acc: 0.1103515625\n",
      "Step 74 -- Train loss: 2.96679425239563, Train Acc: 0.11767578125 Test Acc: 0.1103515625\n",
      "Step 75 -- Train loss: 2.988952398300171, Train Acc: 0.114013671875 Test Acc: 0.1123046875\n",
      "Step 76 -- Train loss: 2.9747025966644287, Train Acc: 0.11474609375 Test Acc: 0.1064453125\n",
      "Step 77 -- Train loss: 2.976198673248291, Train Acc: 0.112548828125 Test Acc: 0.1220703125\n",
      "Step 78 -- Train loss: 2.9754927158355713, Train Acc: 0.108642578125 Test Acc: 0.1142578125\n",
      "Step 79 -- Train loss: 2.948354482650757, Train Acc: 0.11083984375 Test Acc: 0.1201171875\n",
      "Step 80 -- Train loss: 2.9552793502807617, Train Acc: 0.10986328125 Test Acc: 0.1123046875\n",
      "Step 81 -- Train loss: 2.9549262523651123, Train Acc: 0.11083984375 Test Acc: 0.115234375\n",
      "Step 82 -- Train loss: 2.938786268234253, Train Acc: 0.112060546875 Test Acc: 0.1015625\n",
      "Step 83 -- Train loss: 2.940199375152588, Train Acc: 0.11376953125 Test Acc: 0.1162109375\n",
      "Step 84 -- Train loss: 2.9296984672546387, Train Acc: 0.112060546875 Test Acc: 0.103515625\n",
      "Step 85 -- Train loss: 2.941605567932129, Train Acc: 0.10986328125 Test Acc: 0.1123046875\n",
      "Step 86 -- Train loss: 2.931957721710205, Train Acc: 0.110107421875 Test Acc: 0.1103515625\n",
      "Step 87 -- Train loss: 2.9383468627929688, Train Acc: 0.104248046875 Test Acc: 0.1123046875\n",
      "Step 88 -- Train loss: 2.9057939052581787, Train Acc: 0.111328125 Test Acc: 0.1005859375\n",
      "Step 89 -- Train loss: 2.902552604675293, Train Acc: 0.1201171875 Test Acc: 0.1181640625\n",
      "Step 90 -- Train loss: 2.8972156047821045, Train Acc: 0.110107421875 Test Acc: 0.115234375\n",
      "Step 91 -- Train loss: 2.893346071243286, Train Acc: 0.112060546875 Test Acc: 0.1103515625\n",
      "Step 92 -- Train loss: 2.8941471576690674, Train Acc: 0.114990234375 Test Acc: 0.109375\n",
      "Step 93 -- Train loss: 2.880286693572998, Train Acc: 0.120361328125 Test Acc: 0.115234375\n",
      "Step 94 -- Train loss: 2.8750815391540527, Train Acc: 0.1162109375 Test Acc: 0.1220703125\n",
      "Step 95 -- Train loss: 2.8724467754364014, Train Acc: 0.1181640625 Test Acc: 0.1240234375\n",
      "Step 96 -- Train loss: 2.86376690864563, Train Acc: 0.115234375 Test Acc: 0.1162109375\n",
      "Step 97 -- Train loss: 2.8297836780548096, Train Acc: 0.129150390625 Test Acc: 0.119140625\n",
      "Step 98 -- Train loss: 2.8346710205078125, Train Acc: 0.120849609375 Test Acc: 0.109375\n",
      "Step 99 -- Train loss: 2.8247225284576416, Train Acc: 0.115966796875 Test Acc: 0.1123046875\n",
      "Step 100 -- Train loss: 2.8080036640167236, Train Acc: 0.12060546875 Test Acc: 0.130859375\n",
      "Step 101 -- Train loss: 2.7947394847869873, Train Acc: 0.126708984375 Test Acc: 0.126953125\n",
      "Step 102 -- Train loss: 2.7652857303619385, Train Acc: 0.131103515625 Test Acc: 0.1376953125\n",
      "Step 103 -- Train loss: 2.755732774734497, Train Acc: 0.130859375 Test Acc: 0.1240234375\n",
      "Step 104 -- Train loss: 2.733354330062866, Train Acc: 0.13330078125 Test Acc: 0.1337890625\n",
      "Step 105 -- Train loss: 2.719271659851074, Train Acc: 0.143798828125 Test Acc: 0.140625\n",
      "Step 106 -- Train loss: 2.7019076347351074, Train Acc: 0.141845703125 Test Acc: 0.130859375\n",
      "Step 107 -- Train loss: 2.683521270751953, Train Acc: 0.140380859375 Test Acc: 0.1357421875\n",
      "Step 108 -- Train loss: 2.650892972946167, Train Acc: 0.158203125 Test Acc: 0.158203125\n",
      "Step 109 -- Train loss: 2.638085126876831, Train Acc: 0.157470703125 Test Acc: 0.1533203125\n",
      "Step 110 -- Train loss: 2.6306698322296143, Train Acc: 0.154052734375 Test Acc: 0.1552734375\n",
      "Step 111 -- Train loss: 2.6003506183624268, Train Acc: 0.165283203125 Test Acc: 0.1572265625\n",
      "Step 112 -- Train loss: 2.5935378074645996, Train Acc: 0.1728515625 Test Acc: 0.173828125\n",
      "Step 113 -- Train loss: 2.559983968734741, Train Acc: 0.16748046875 Test Acc: 0.1708984375\n",
      "Step 114 -- Train loss: 2.537658929824829, Train Acc: 0.18212890625 Test Acc: 0.18359375\n",
      "Step 115 -- Train loss: 2.5253610610961914, Train Acc: 0.188232421875 Test Acc: 0.1748046875\n",
      "Step 116 -- Train loss: 2.504720687866211, Train Acc: 0.191162109375 Test Acc: 0.2080078125\n",
      "Step 117 -- Train loss: 2.5008645057678223, Train Acc: 0.1923828125 Test Acc: 0.20703125\n",
      "Step 118 -- Train loss: 2.468961000442505, Train Acc: 0.1953125 Test Acc: 0.2099609375\n",
      "Step 119 -- Train loss: 2.452273368835449, Train Acc: 0.204833984375 Test Acc: 0.1953125\n",
      "Step 120 -- Train loss: 2.436728000640869, Train Acc: 0.21484375 Test Acc: 0.185546875\n",
      "Step 121 -- Train loss: 2.425603151321411, Train Acc: 0.218017578125 Test Acc: 0.2197265625\n",
      "Step 122 -- Train loss: 2.4000244140625, Train Acc: 0.22021484375 Test Acc: 0.208984375\n",
      "Step 123 -- Train loss: 2.3925273418426514, Train Acc: 0.2255859375 Test Acc: 0.2197265625\n",
      "Step 124 -- Train loss: 2.3683862686157227, Train Acc: 0.23681640625 Test Acc: 0.224609375\n",
      "Step 125 -- Train loss: 2.3635854721069336, Train Acc: 0.242919921875 Test Acc: 0.2353515625\n",
      "Step 126 -- Train loss: 2.3409361839294434, Train Acc: 0.244140625 Test Acc: 0.228515625\n",
      "Step 127 -- Train loss: 2.3423774242401123, Train Acc: 0.23583984375 Test Acc: 0.2431640625\n",
      "Step 128 -- Train loss: 2.3221490383148193, Train Acc: 0.24462890625 Test Acc: 0.232421875\n",
      "Step 129 -- Train loss: 2.3158514499664307, Train Acc: 0.2509765625 Test Acc: 0.2724609375\n",
      "Step 130 -- Train loss: 2.3030126094818115, Train Acc: 0.2509765625 Test Acc: 0.2431640625\n",
      "Step 131 -- Train loss: 2.291348457336426, Train Acc: 0.261474609375 Test Acc: 0.251953125\n",
      "Step 132 -- Train loss: 2.2876992225646973, Train Acc: 0.253662109375 Test Acc: 0.234375\n",
      "Step 133 -- Train loss: 2.2658889293670654, Train Acc: 0.26025390625 Test Acc: 0.25\n",
      "Step 134 -- Train loss: 2.258518695831299, Train Acc: 0.255126953125 Test Acc: 0.2861328125\n",
      "Step 135 -- Train loss: 2.251575469970703, Train Acc: 0.2529296875 Test Acc: 0.24609375\n",
      "Step 136 -- Train loss: 2.229851007461548, Train Acc: 0.269775390625 Test Acc: 0.2548828125\n",
      "Step 137 -- Train loss: 2.2374770641326904, Train Acc: 0.26806640625 Test Acc: 0.2802734375\n",
      "Step 138 -- Train loss: 2.214189052581787, Train Acc: 0.26171875 Test Acc: 0.263671875\n",
      "Step 139 -- Train loss: 2.217874526977539, Train Acc: 0.268798828125 Test Acc: 0.2353515625\n",
      "Step 140 -- Train loss: 2.204535961151123, Train Acc: 0.2724609375 Test Acc: 0.2607421875\n",
      "Step 141 -- Train loss: 2.199009418487549, Train Acc: 0.279052734375 Test Acc: 0.28515625\n",
      "Step 142 -- Train loss: 2.194047451019287, Train Acc: 0.27001953125 Test Acc: 0.2890625\n",
      "Step 143 -- Train loss: 2.190873146057129, Train Acc: 0.267822265625 Test Acc: 0.25390625\n",
      "Step 144 -- Train loss: 2.178520679473877, Train Acc: 0.27001953125 Test Acc: 0.271484375\n",
      "Step 145 -- Train loss: 2.172808885574341, Train Acc: 0.276611328125 Test Acc: 0.26171875\n",
      "Step 146 -- Train loss: 2.1526565551757812, Train Acc: 0.28955078125 Test Acc: 0.28125\n",
      "Step 147 -- Train loss: 2.1581315994262695, Train Acc: 0.2890625 Test Acc: 0.2587890625\n",
      "Step 148 -- Train loss: 2.149322032928467, Train Acc: 0.27490234375 Test Acc: 0.2724609375\n",
      "Step 149 -- Train loss: 2.1420199871063232, Train Acc: 0.28076171875 Test Acc: 0.2783203125\n",
      "Step 150 -- Train loss: 2.139582872390747, Train Acc: 0.293701171875 Test Acc: 0.28515625\n",
      "Step 151 -- Train loss: 2.1494712829589844, Train Acc: 0.277099609375 Test Acc: 0.2822265625\n",
      "Step 152 -- Train loss: 2.1417596340179443, Train Acc: 0.28759765625 Test Acc: 0.310546875\n",
      "Step 153 -- Train loss: 2.1177072525024414, Train Acc: 0.309814453125 Test Acc: 0.3349609375\n",
      "Step 154 -- Train loss: 2.1247284412384033, Train Acc: 0.2978515625 Test Acc: 0.310546875\n",
      "Step 155 -- Train loss: 2.1065196990966797, Train Acc: 0.313232421875 Test Acc: 0.279296875\n",
      "Step 156 -- Train loss: 2.1162145137786865, Train Acc: 0.30517578125 Test Acc: 0.2958984375\n",
      "Step 157 -- Train loss: 2.095036268234253, Train Acc: 0.31201171875 Test Acc: 0.3115234375\n",
      "Step 158 -- Train loss: 2.105741500854492, Train Acc: 0.30322265625 Test Acc: 0.26953125\n",
      "Step 159 -- Train loss: 2.0828542709350586, Train Acc: 0.299560546875 Test Acc: 0.2900390625\n",
      "Step 160 -- Train loss: 2.0929996967315674, Train Acc: 0.3046875 Test Acc: 0.3046875\n",
      "Step 161 -- Train loss: 2.10105299949646, Train Acc: 0.3095703125 Test Acc: 0.3154296875\n",
      "Step 162 -- Train loss: 2.0828545093536377, Train Acc: 0.298828125 Test Acc: 0.2939453125\n",
      "Step 163 -- Train loss: 2.081544876098633, Train Acc: 0.31103515625 Test Acc: 0.3037109375\n",
      "Step 164 -- Train loss: 2.0728518962860107, Train Acc: 0.302734375 Test Acc: 0.2744140625\n",
      "Step 165 -- Train loss: 2.073791742324829, Train Acc: 0.301513671875 Test Acc: 0.2724609375\n",
      "Step 166 -- Train loss: 2.0720953941345215, Train Acc: 0.314208984375 Test Acc: 0.30859375\n",
      "Step 167 -- Train loss: 2.064201831817627, Train Acc: 0.316650390625 Test Acc: 0.314453125\n",
      "Step 168 -- Train loss: 2.0602469444274902, Train Acc: 0.319580078125 Test Acc: 0.3046875\n",
      "Step 169 -- Train loss: 2.069908618927002, Train Acc: 0.29541015625 Test Acc: 0.28125\n",
      "Step 170 -- Train loss: 2.030219316482544, Train Acc: 0.318603515625 Test Acc: 0.3115234375\n",
      "Step 171 -- Train loss: 2.041867971420288, Train Acc: 0.322998046875 Test Acc: 0.3046875\n",
      "Step 172 -- Train loss: 2.046452283859253, Train Acc: 0.328125 Test Acc: 0.30859375\n",
      "Step 173 -- Train loss: 2.034468650817871, Train Acc: 0.32080078125 Test Acc: 0.2978515625\n",
      "Step 174 -- Train loss: 2.042275905609131, Train Acc: 0.313720703125 Test Acc: 0.33203125\n",
      "Step 175 -- Train loss: 2.030167818069458, Train Acc: 0.323486328125 Test Acc: 0.2939453125\n",
      "Step 176 -- Train loss: 2.0372238159179688, Train Acc: 0.322265625 Test Acc: 0.3154296875\n",
      "Step 177 -- Train loss: 2.0164811611175537, Train Acc: 0.33251953125 Test Acc: 0.3310546875\n",
      "Step 178 -- Train loss: 2.030367612838745, Train Acc: 0.31884765625 Test Acc: 0.3349609375\n",
      "Step 179 -- Train loss: 2.0158252716064453, Train Acc: 0.32275390625 Test Acc: 0.3154296875\n",
      "Step 180 -- Train loss: 2.0162065029144287, Train Acc: 0.325439453125 Test Acc: 0.328125\n",
      "Step 181 -- Train loss: 2.0155258178710938, Train Acc: 0.340576171875 Test Acc: 0.3232421875\n",
      "Step 182 -- Train loss: 2.02355694770813, Train Acc: 0.3310546875 Test Acc: 0.3330078125\n",
      "Step 183 -- Train loss: 2.006945848464966, Train Acc: 0.337158203125 Test Acc: 0.3271484375\n",
      "Step 184 -- Train loss: 2.01353120803833, Train Acc: 0.32666015625 Test Acc: 0.3193359375\n",
      "Step 185 -- Train loss: 2.0096585750579834, Train Acc: 0.341796875 Test Acc: 0.3291015625\n",
      "Step 186 -- Train loss: 2.0154616832733154, Train Acc: 0.32568359375 Test Acc: 0.3232421875\n",
      "Step 187 -- Train loss: 1.989345669746399, Train Acc: 0.341552734375 Test Acc: 0.3583984375\n",
      "Step 188 -- Train loss: 2.001371145248413, Train Acc: 0.343505859375 Test Acc: 0.3466796875\n",
      "Step 189 -- Train loss: 2.0045604705810547, Train Acc: 0.3271484375 Test Acc: 0.318359375\n",
      "Step 190 -- Train loss: 1.9897825717926025, Train Acc: 0.3427734375 Test Acc: 0.33984375\n",
      "Step 191 -- Train loss: 1.9795104265213013, Train Acc: 0.34765625 Test Acc: 0.3740234375\n",
      "Step 192 -- Train loss: 1.9842021465301514, Train Acc: 0.35302734375 Test Acc: 0.35546875\n",
      "Step 193 -- Train loss: 1.9744340181350708, Train Acc: 0.354736328125 Test Acc: 0.36328125\n",
      "Step 194 -- Train loss: 1.9856219291687012, Train Acc: 0.359375 Test Acc: 0.365234375\n",
      "Step 195 -- Train loss: 1.9984534978866577, Train Acc: 0.322021484375 Test Acc: 0.3310546875\n",
      "Step 196 -- Train loss: 1.9797649383544922, Train Acc: 0.355224609375 Test Acc: 0.345703125\n",
      "Step 197 -- Train loss: 2.0346782207489014, Train Acc: 0.32666015625 Test Acc: 0.3359375\n",
      "Step 198 -- Train loss: 2.0119121074676514, Train Acc: 0.33935546875 Test Acc: 0.361328125\n",
      "Step 199 -- Train loss: 2.015937328338623, Train Acc: 0.3310546875 Test Acc: 0.3388671875\n",
      "Step 200 -- Train loss: 1.9795763492584229, Train Acc: 0.365234375 Test Acc: 0.3466796875\n",
      "Step 201 -- Train loss: 1.9801181554794312, Train Acc: 0.3681640625 Test Acc: 0.3408203125\n",
      "Step 202 -- Train loss: 1.9877828359603882, Train Acc: 0.355712890625 Test Acc: 0.34375\n",
      "Step 203 -- Train loss: 1.9806594848632812, Train Acc: 0.351318359375 Test Acc: 0.345703125\n",
      "Step 204 -- Train loss: 1.9860063791275024, Train Acc: 0.3505859375 Test Acc: 0.359375\n",
      "Step 205 -- Train loss: 1.9657609462738037, Train Acc: 0.368408203125 Test Acc: 0.3642578125\n",
      "Step 206 -- Train loss: 1.9622586965560913, Train Acc: 0.3671875 Test Acc: 0.3544921875\n",
      "Step 207 -- Train loss: 1.9763799905776978, Train Acc: 0.35693359375 Test Acc: 0.3447265625\n",
      "Step 208 -- Train loss: 1.953292727470398, Train Acc: 0.34521484375 Test Acc: 0.318359375\n",
      "Step 209 -- Train loss: 1.9539728164672852, Train Acc: 0.33203125 Test Acc: 0.3359375\n",
      "Step 210 -- Train loss: 1.9474337100982666, Train Acc: 0.36376953125 Test Acc: 0.3359375\n",
      "Step 211 -- Train loss: 1.9515880346298218, Train Acc: 0.368408203125 Test Acc: 0.3359375\n",
      "Step 212 -- Train loss: 1.9494558572769165, Train Acc: 0.36181640625 Test Acc: 0.361328125\n",
      "Step 213 -- Train loss: 1.9436670541763306, Train Acc: 0.35302734375 Test Acc: 0.3466796875\n",
      "Step 214 -- Train loss: 1.9437952041625977, Train Acc: 0.36474609375 Test Acc: 0.361328125\n",
      "Step 215 -- Train loss: 1.9252033233642578, Train Acc: 0.374755859375 Test Acc: 0.3935546875\n",
      "Step 216 -- Train loss: 1.9443247318267822, Train Acc: 0.363525390625 Test Acc: 0.3427734375\n",
      "Step 217 -- Train loss: 1.9356566667556763, Train Acc: 0.3818359375 Test Acc: 0.357421875\n",
      "Step 218 -- Train loss: 1.9242912530899048, Train Acc: 0.37841796875 Test Acc: 0.3642578125\n",
      "Step 219 -- Train loss: 1.931058645248413, Train Acc: 0.355712890625 Test Acc: 0.37109375\n",
      "Step 220 -- Train loss: 1.9360160827636719, Train Acc: 0.356201171875 Test Acc: 0.3388671875\n",
      "Step 221 -- Train loss: 1.925338864326477, Train Acc: 0.389404296875 Test Acc: 0.3798828125\n",
      "Step 222 -- Train loss: 1.916926622390747, Train Acc: 0.385498046875 Test Acc: 0.373046875\n",
      "Step 223 -- Train loss: 1.9216185808181763, Train Acc: 0.360107421875 Test Acc: 0.3779296875\n",
      "Step 224 -- Train loss: 1.9220552444458008, Train Acc: 0.383544921875 Test Acc: 0.3837890625\n",
      "Step 225 -- Train loss: 1.9255092144012451, Train Acc: 0.37890625 Test Acc: 0.36328125\n",
      "Step 226 -- Train loss: 1.9181249141693115, Train Acc: 0.377197265625 Test Acc: 0.3701171875\n",
      "Step 227 -- Train loss: 1.9188894033432007, Train Acc: 0.363037109375 Test Acc: 0.3662109375\n",
      "Step 228 -- Train loss: 1.8992801904678345, Train Acc: 0.4052734375 Test Acc: 0.37890625\n",
      "Step 229 -- Train loss: 1.902522325515747, Train Acc: 0.397216796875 Test Acc: 0.380859375\n",
      "Step 230 -- Train loss: 1.912739634513855, Train Acc: 0.38525390625 Test Acc: 0.3740234375\n",
      "Step 231 -- Train loss: 1.9117525815963745, Train Acc: 0.390869140625 Test Acc: 0.3828125\n",
      "Step 232 -- Train loss: 1.898885726928711, Train Acc: 0.385009765625 Test Acc: 0.384765625\n",
      "Step 233 -- Train loss: 1.9054958820343018, Train Acc: 0.3896484375 Test Acc: 0.3681640625\n",
      "Step 234 -- Train loss: 1.892228364944458, Train Acc: 0.3837890625 Test Acc: 0.3896484375\n",
      "Step 235 -- Train loss: 1.8920295238494873, Train Acc: 0.39697265625 Test Acc: 0.3818359375\n",
      "Step 236 -- Train loss: 1.8915315866470337, Train Acc: 0.396728515625 Test Acc: 0.408203125\n",
      "Step 237 -- Train loss: 1.890586018562317, Train Acc: 0.385986328125 Test Acc: 0.37890625\n",
      "Step 238 -- Train loss: 1.8930931091308594, Train Acc: 0.397216796875 Test Acc: 0.4033203125\n",
      "Step 239 -- Train loss: 1.8761227130889893, Train Acc: 0.410888671875 Test Acc: 0.3994140625\n",
      "Step 240 -- Train loss: 1.8972499370574951, Train Acc: 0.388671875 Test Acc: 0.384765625\n",
      "Step 241 -- Train loss: 1.8969922065734863, Train Acc: 0.39794921875 Test Acc: 0.40234375\n",
      "Step 242 -- Train loss: 1.8918445110321045, Train Acc: 0.399658203125 Test Acc: 0.3876953125\n",
      "Step 243 -- Train loss: 1.8934190273284912, Train Acc: 0.39599609375 Test Acc: 0.4248046875\n",
      "Step 244 -- Train loss: 1.8805015087127686, Train Acc: 0.411376953125 Test Acc: 0.40234375\n",
      "Step 245 -- Train loss: 1.8899314403533936, Train Acc: 0.40380859375 Test Acc: 0.423828125\n",
      "Step 246 -- Train loss: 1.8802359104156494, Train Acc: 0.398681640625 Test Acc: 0.3974609375\n",
      "Step 247 -- Train loss: 1.8871222734451294, Train Acc: 0.39794921875 Test Acc: 0.4365234375\n",
      "Step 248 -- Train loss: 1.8861805200576782, Train Acc: 0.408935546875 Test Acc: 0.4033203125\n",
      "Step 249 -- Train loss: 1.8710079193115234, Train Acc: 0.418701171875 Test Acc: 0.4296875\n",
      "Step 250 -- Train loss: 1.8782449960708618, Train Acc: 0.402587890625 Test Acc: 0.3798828125\n",
      "Step 251 -- Train loss: 1.8814339637756348, Train Acc: 0.396484375 Test Acc: 0.41796875\n",
      "Step 252 -- Train loss: 1.869971752166748, Train Acc: 0.419189453125 Test Acc: 0.4052734375\n",
      "Step 253 -- Train loss: 1.8644778728485107, Train Acc: 0.413330078125 Test Acc: 0.40234375\n",
      "Step 254 -- Train loss: 1.8790931701660156, Train Acc: 0.4140625 Test Acc: 0.4013671875\n",
      "Step 255 -- Train loss: 1.8743329048156738, Train Acc: 0.405029296875 Test Acc: 0.4404296875\n",
      "Step 256 -- Train loss: 1.8694941997528076, Train Acc: 0.411376953125 Test Acc: 0.4140625\n",
      "Step 257 -- Train loss: 1.8537659645080566, Train Acc: 0.44384765625 Test Acc: 0.4013671875\n",
      "Step 258 -- Train loss: 1.868194818496704, Train Acc: 0.413818359375 Test Acc: 0.4052734375\n",
      "Step 259 -- Train loss: 1.8435862064361572, Train Acc: 0.43896484375 Test Acc: 0.4130859375\n",
      "Step 260 -- Train loss: 1.8670668601989746, Train Acc: 0.42822265625 Test Acc: 0.423828125\n",
      "Step 261 -- Train loss: 1.8864320516586304, Train Acc: 0.410888671875 Test Acc: 0.4072265625\n",
      "Step 262 -- Train loss: 1.9128220081329346, Train Acc: 0.406494140625 Test Acc: 0.4130859375\n",
      "Step 263 -- Train loss: 1.8994206190109253, Train Acc: 0.41650390625 Test Acc: 0.45703125\n",
      "Step 264 -- Train loss: 1.8628900051116943, Train Acc: 0.436767578125 Test Acc: 0.4677734375\n",
      "Step 265 -- Train loss: 1.920507550239563, Train Acc: 0.4033203125 Test Acc: 0.3876953125\n",
      "Step 266 -- Train loss: 1.872269868850708, Train Acc: 0.404052734375 Test Acc: 0.408203125\n",
      "Step 267 -- Train loss: 1.890892505645752, Train Acc: 0.429931640625 Test Acc: 0.40234375\n",
      "Step 268 -- Train loss: 1.9081549644470215, Train Acc: 0.41748046875 Test Acc: 0.416015625\n",
      "Step 269 -- Train loss: 1.889211893081665, Train Acc: 0.41357421875 Test Acc: 0.44140625\n",
      "Step 270 -- Train loss: 1.8619420528411865, Train Acc: 0.423095703125 Test Acc: 0.4345703125\n",
      "Step 271 -- Train loss: 1.8679111003875732, Train Acc: 0.424072265625 Test Acc: 0.4111328125\n",
      "Step 272 -- Train loss: 1.8918882608413696, Train Acc: 0.37451171875 Test Acc: 0.4072265625\n",
      "Step 273 -- Train loss: 1.839336633682251, Train Acc: 0.4169921875 Test Acc: 0.3798828125\n",
      "Step 274 -- Train loss: 1.8576929569244385, Train Acc: 0.425048828125 Test Acc: 0.4306640625\n",
      "Step 275 -- Train loss: 1.863992691040039, Train Acc: 0.42138671875 Test Acc: 0.412109375\n",
      "Step 276 -- Train loss: 1.848081350326538, Train Acc: 0.441650390625 Test Acc: 0.41796875\n",
      "Step 277 -- Train loss: 1.8666839599609375, Train Acc: 0.417236328125 Test Acc: 0.4111328125\n",
      "Step 278 -- Train loss: 1.851583480834961, Train Acc: 0.43310546875 Test Acc: 0.3955078125\n",
      "Step 279 -- Train loss: 1.8592126369476318, Train Acc: 0.429931640625 Test Acc: 0.384765625\n",
      "Step 280 -- Train loss: 1.8272597789764404, Train Acc: 0.421875 Test Acc: 0.41796875\n",
      "Step 281 -- Train loss: 1.8381597995758057, Train Acc: 0.4267578125 Test Acc: 0.4140625\n",
      "Step 282 -- Train loss: 1.8438475131988525, Train Acc: 0.422119140625 Test Acc: 0.4248046875\n",
      "Step 283 -- Train loss: 1.8353354930877686, Train Acc: 0.427978515625 Test Acc: 0.4453125\n",
      "Step 284 -- Train loss: 1.8376235961914062, Train Acc: 0.422607421875 Test Acc: 0.4052734375\n",
      "Step 285 -- Train loss: 1.814807415008545, Train Acc: 0.4248046875 Test Acc: 0.3984375\n",
      "Step 286 -- Train loss: 1.823114037513733, Train Acc: 0.421630859375 Test Acc: 0.4150390625\n",
      "Step 287 -- Train loss: 1.8264915943145752, Train Acc: 0.4287109375 Test Acc: 0.4013671875\n",
      "Step 288 -- Train loss: 1.824592113494873, Train Acc: 0.419189453125 Test Acc: 0.4423828125\n",
      "Step 289 -- Train loss: 1.8223422765731812, Train Acc: 0.4306640625 Test Acc: 0.4169921875\n",
      "Step 290 -- Train loss: 1.8110603094100952, Train Acc: 0.432373046875 Test Acc: 0.4189453125\n",
      "Step 291 -- Train loss: 1.838234305381775, Train Acc: 0.419189453125 Test Acc: 0.4208984375\n",
      "Step 292 -- Train loss: 1.814124584197998, Train Acc: 0.422119140625 Test Acc: 0.3896484375\n",
      "Step 293 -- Train loss: 1.7996466159820557, Train Acc: 0.42822265625 Test Acc: 0.439453125\n",
      "Step 294 -- Train loss: 1.8238695859909058, Train Acc: 0.428466796875 Test Acc: 0.4365234375\n",
      "Step 295 -- Train loss: 1.8013370037078857, Train Acc: 0.444091796875 Test Acc: 0.451171875\n",
      "Step 296 -- Train loss: 1.809574007987976, Train Acc: 0.432861328125 Test Acc: 0.42578125\n",
      "Step 297 -- Train loss: 1.819161295890808, Train Acc: 0.416015625 Test Acc: 0.4296875\n",
      "Step 298 -- Train loss: 1.8115191459655762, Train Acc: 0.425537109375 Test Acc: 0.408203125\n",
      "Step 299 -- Train loss: 1.8078640699386597, Train Acc: 0.434814453125 Test Acc: 0.419921875\n",
      "Step 300 -- Train loss: 1.799682378768921, Train Acc: 0.43896484375 Test Acc: 0.4697265625\n",
      "Step 301 -- Train loss: 1.8005019426345825, Train Acc: 0.454345703125 Test Acc: 0.4453125\n",
      "Step 302 -- Train loss: 1.7892003059387207, Train Acc: 0.4462890625 Test Acc: 0.482421875\n",
      "Step 303 -- Train loss: 1.799078106880188, Train Acc: 0.442138671875 Test Acc: 0.4150390625\n",
      "Step 304 -- Train loss: 1.8228111267089844, Train Acc: 0.43505859375 Test Acc: 0.4697265625\n",
      "Step 305 -- Train loss: 1.8085860013961792, Train Acc: 0.451171875 Test Acc: 0.4658203125\n",
      "Step 306 -- Train loss: 1.79922354221344, Train Acc: 0.468994140625 Test Acc: 0.458984375\n",
      "Step 307 -- Train loss: 1.7916878461837769, Train Acc: 0.453857421875 Test Acc: 0.48046875\n",
      "Step 308 -- Train loss: 1.79460871219635, Train Acc: 0.435546875 Test Acc: 0.4365234375\n",
      "Step 309 -- Train loss: 1.7946702241897583, Train Acc: 0.425537109375 Test Acc: 0.4609375\n",
      "Step 310 -- Train loss: 1.7849867343902588, Train Acc: 0.453125 Test Acc: 0.4580078125\n",
      "Step 311 -- Train loss: 1.7858368158340454, Train Acc: 0.461181640625 Test Acc: 0.453125\n",
      "Step 312 -- Train loss: 1.7836707830429077, Train Acc: 0.458984375 Test Acc: 0.4365234375\n",
      "Step 313 -- Train loss: 1.7940564155578613, Train Acc: 0.460693359375 Test Acc: 0.4716796875\n",
      "Step 314 -- Train loss: 1.7773677110671997, Train Acc: 0.469970703125 Test Acc: 0.462890625\n",
      "Step 315 -- Train loss: 1.7810165882110596, Train Acc: 0.460205078125 Test Acc: 0.4326171875\n",
      "Step 316 -- Train loss: 1.7810983657836914, Train Acc: 0.4677734375 Test Acc: 0.4853515625\n",
      "Step 317 -- Train loss: 1.7775914669036865, Train Acc: 0.464599609375 Test Acc: 0.4638671875\n",
      "Step 318 -- Train loss: 1.7810920476913452, Train Acc: 0.45654296875 Test Acc: 0.4921875\n",
      "Step 319 -- Train loss: 1.7693668603897095, Train Acc: 0.468994140625 Test Acc: 0.4716796875\n",
      "Step 320 -- Train loss: 1.7736384868621826, Train Acc: 0.47119140625 Test Acc: 0.4619140625\n",
      "Step 321 -- Train loss: 1.7609223127365112, Train Acc: 0.489013671875 Test Acc: 0.4677734375\n",
      "Step 322 -- Train loss: 1.7698099613189697, Train Acc: 0.46875 Test Acc: 0.4794921875\n",
      "Step 323 -- Train loss: 1.765799880027771, Train Acc: 0.469482421875 Test Acc: 0.4580078125\n",
      "Step 324 -- Train loss: 1.762616515159607, Train Acc: 0.46728515625 Test Acc: 0.451171875\n",
      "Step 325 -- Train loss: 1.7701876163482666, Train Acc: 0.46240234375 Test Acc: 0.4541015625\n",
      "Step 326 -- Train loss: 1.7550265789031982, Train Acc: 0.4638671875 Test Acc: 0.458984375\n",
      "Step 327 -- Train loss: 1.7571605443954468, Train Acc: 0.46826171875 Test Acc: 0.4716796875\n",
      "Step 328 -- Train loss: 1.756317138671875, Train Acc: 0.470458984375 Test Acc: 0.4541015625\n",
      "Step 329 -- Train loss: 1.7577182054519653, Train Acc: 0.455810546875 Test Acc: 0.4599609375\n",
      "Step 330 -- Train loss: 1.7635265588760376, Train Acc: 0.4580078125 Test Acc: 0.4609375\n",
      "Step 331 -- Train loss: 1.7548280954360962, Train Acc: 0.460205078125 Test Acc: 0.4326171875\n",
      "Step 332 -- Train loss: 1.7701847553253174, Train Acc: 0.4580078125 Test Acc: 0.4677734375\n",
      "Step 333 -- Train loss: 1.7557204961776733, Train Acc: 0.4697265625 Test Acc: 0.4306640625\n",
      "Step 334 -- Train loss: 1.7596431970596313, Train Acc: 0.461669921875 Test Acc: 0.427734375\n",
      "Step 335 -- Train loss: 1.751180648803711, Train Acc: 0.461181640625 Test Acc: 0.490234375\n",
      "Step 336 -- Train loss: 1.7504465579986572, Train Acc: 0.461181640625 Test Acc: 0.470703125\n",
      "Step 337 -- Train loss: 1.7519104480743408, Train Acc: 0.48388671875 Test Acc: 0.4912109375\n",
      "Step 338 -- Train loss: 1.7483625411987305, Train Acc: 0.47998046875 Test Acc: 0.4677734375\n",
      "Step 339 -- Train loss: 1.7562428712844849, Train Acc: 0.467041015625 Test Acc: 0.4619140625\n",
      "Step 340 -- Train loss: 1.7588424682617188, Train Acc: 0.469970703125 Test Acc: 0.484375\n",
      "Step 341 -- Train loss: 1.7595977783203125, Train Acc: 0.48828125 Test Acc: 0.478515625\n",
      "Step 342 -- Train loss: 1.7454642057418823, Train Acc: 0.474365234375 Test Acc: 0.466796875\n",
      "Step 343 -- Train loss: 1.731282114982605, Train Acc: 0.495361328125 Test Acc: 0.4794921875\n",
      "Step 344 -- Train loss: 1.743957281112671, Train Acc: 0.482177734375 Test Acc: 0.4755859375\n",
      "Step 345 -- Train loss: 1.7535849809646606, Train Acc: 0.48095703125 Test Acc: 0.455078125\n",
      "Step 346 -- Train loss: 1.7401931285858154, Train Acc: 0.458251953125 Test Acc: 0.4580078125\n",
      "Step 347 -- Train loss: 1.7503160238265991, Train Acc: 0.453125 Test Acc: 0.4736328125\n",
      "Step 348 -- Train loss: 1.7455836534500122, Train Acc: 0.47314453125 Test Acc: 0.4609375\n",
      "Step 349 -- Train loss: 1.736037254333496, Train Acc: 0.476318359375 Test Acc: 0.451171875\n",
      "Step 350 -- Train loss: 1.7313017845153809, Train Acc: 0.47509765625 Test Acc: 0.4765625\n",
      "Step 351 -- Train loss: 1.7277889251708984, Train Acc: 0.489013671875 Test Acc: 0.48046875\n",
      "Step 352 -- Train loss: 1.7202470302581787, Train Acc: 0.4814453125 Test Acc: 0.439453125\n",
      "Step 353 -- Train loss: 1.7333736419677734, Train Acc: 0.48779296875 Test Acc: 0.4580078125\n",
      "Step 354 -- Train loss: 1.7297285795211792, Train Acc: 0.47607421875 Test Acc: 0.49609375\n",
      "Step 355 -- Train loss: 1.721095085144043, Train Acc: 0.475830078125 Test Acc: 0.498046875\n",
      "Step 356 -- Train loss: 1.7151561975479126, Train Acc: 0.468017578125 Test Acc: 0.490234375\n",
      "Step 357 -- Train loss: 1.7357505559921265, Train Acc: 0.45751953125 Test Acc: 0.4541015625\n",
      "Step 358 -- Train loss: 1.7148770093917847, Train Acc: 0.462646484375 Test Acc: 0.447265625\n",
      "Step 359 -- Train loss: 1.7212255001068115, Train Acc: 0.468505859375 Test Acc: 0.4697265625\n",
      "Step 360 -- Train loss: 1.7223620414733887, Train Acc: 0.483154296875 Test Acc: 0.466796875\n",
      "Step 361 -- Train loss: 1.7340140342712402, Train Acc: 0.46826171875 Test Acc: 0.466796875\n",
      "Step 362 -- Train loss: 1.7163128852844238, Train Acc: 0.462890625 Test Acc: 0.4619140625\n",
      "Step 363 -- Train loss: 1.7272337675094604, Train Acc: 0.443359375 Test Acc: 0.4443359375\n",
      "Step 364 -- Train loss: 1.7249988317489624, Train Acc: 0.45556640625 Test Acc: 0.4501953125\n",
      "Step 365 -- Train loss: 1.723259687423706, Train Acc: 0.46484375 Test Acc: 0.4794921875\n",
      "Step 366 -- Train loss: 1.7120656967163086, Train Acc: 0.479736328125 Test Acc: 0.4775390625\n",
      "Step 367 -- Train loss: 1.7098623514175415, Train Acc: 0.48095703125 Test Acc: 0.4560546875\n",
      "Step 368 -- Train loss: 1.7032661437988281, Train Acc: 0.489990234375 Test Acc: 0.453125\n",
      "Step 369 -- Train loss: 1.7044930458068848, Train Acc: 0.476318359375 Test Acc: 0.4482421875\n",
      "Step 370 -- Train loss: 1.721972107887268, Train Acc: 0.47412109375 Test Acc: 0.4736328125\n",
      "Step 371 -- Train loss: 1.7266889810562134, Train Acc: 0.46337890625 Test Acc: 0.4794921875\n",
      "Step 372 -- Train loss: 1.7376097440719604, Train Acc: 0.473876953125 Test Acc: 0.4462890625\n",
      "Step 373 -- Train loss: 1.7336152791976929, Train Acc: 0.459228515625 Test Acc: 0.4697265625\n",
      "Step 374 -- Train loss: 1.7151046991348267, Train Acc: 0.487060546875 Test Acc: 0.482421875\n",
      "Step 375 -- Train loss: 1.7567577362060547, Train Acc: 0.471435546875 Test Acc: 0.4443359375\n",
      "Step 376 -- Train loss: 1.7350146770477295, Train Acc: 0.47412109375 Test Acc: 0.470703125\n",
      "Step 377 -- Train loss: 1.7401565313339233, Train Acc: 0.478759765625 Test Acc: 0.4619140625\n",
      "Step 378 -- Train loss: 1.7804973125457764, Train Acc: 0.4541015625 Test Acc: 0.462890625\n",
      "Step 379 -- Train loss: 1.7581572532653809, Train Acc: 0.4775390625 Test Acc: 0.5009765625\n",
      "Step 380 -- Train loss: 1.7498865127563477, Train Acc: 0.470458984375 Test Acc: 0.4970703125\n",
      "Step 381 -- Train loss: 1.7305794954299927, Train Acc: 0.488525390625 Test Acc: 0.482421875\n",
      "Step 382 -- Train loss: 1.7237650156021118, Train Acc: 0.504150390625 Test Acc: 0.5068359375\n",
      "Step 383 -- Train loss: 1.7388324737548828, Train Acc: 0.47900390625 Test Acc: 0.48828125\n",
      "Step 384 -- Train loss: 1.7181373834609985, Train Acc: 0.48388671875 Test Acc: 0.474609375\n",
      "Step 385 -- Train loss: 1.7233877182006836, Train Acc: 0.48876953125 Test Acc: 0.4873046875\n",
      "Step 386 -- Train loss: 1.7254376411437988, Train Acc: 0.47119140625 Test Acc: 0.4736328125\n",
      "Step 387 -- Train loss: 1.72030508518219, Train Acc: 0.473876953125 Test Acc: 0.4765625\n",
      "Step 388 -- Train loss: 1.7187353372573853, Train Acc: 0.476806640625 Test Acc: 0.46875\n",
      "Step 389 -- Train loss: 1.6900132894515991, Train Acc: 0.496826171875 Test Acc: 0.4599609375\n",
      "Step 390 -- Train loss: 1.7076053619384766, Train Acc: 0.48046875 Test Acc: 0.501953125\n",
      "Step 391 -- Train loss: 1.7046215534210205, Train Acc: 0.498779296875 Test Acc: 0.466796875\n",
      "Step 392 -- Train loss: 1.697213888168335, Train Acc: 0.5029296875 Test Acc: 0.498046875\n",
      "Step 393 -- Train loss: 1.6957671642303467, Train Acc: 0.50244140625 Test Acc: 0.478515625\n",
      "Step 394 -- Train loss: 1.6880601644515991, Train Acc: 0.50146484375 Test Acc: 0.4921875\n",
      "Step 395 -- Train loss: 1.6926257610321045, Train Acc: 0.486328125 Test Acc: 0.4541015625\n",
      "Step 396 -- Train loss: 1.6968275308609009, Train Acc: 0.479248046875 Test Acc: 0.4599609375\n",
      "Step 397 -- Train loss: 1.6844933032989502, Train Acc: 0.488037109375 Test Acc: 0.505859375\n",
      "Step 398 -- Train loss: 1.6918971538543701, Train Acc: 0.496826171875 Test Acc: 0.4990234375\n",
      "Step 399 -- Train loss: 1.6917496919631958, Train Acc: 0.485595703125 Test Acc: 0.4853515625\n",
      "Step 400 -- Train loss: 1.6767771244049072, Train Acc: 0.4921875 Test Acc: 0.4814453125\n",
      "Step 401 -- Train loss: 1.6919987201690674, Train Acc: 0.4921875 Test Acc: 0.4775390625\n",
      "Step 402 -- Train loss: 1.6780502796173096, Train Acc: 0.49853515625 Test Acc: 0.4677734375\n",
      "Step 403 -- Train loss: 1.6837129592895508, Train Acc: 0.515625 Test Acc: 0.48828125\n",
      "Step 404 -- Train loss: 1.678867220878601, Train Acc: 0.503173828125 Test Acc: 0.4931640625\n",
      "Step 405 -- Train loss: 1.6848939657211304, Train Acc: 0.50048828125 Test Acc: 0.4794921875\n",
      "Step 406 -- Train loss: 1.679962396621704, Train Acc: 0.493408203125 Test Acc: 0.4736328125\n",
      "Step 407 -- Train loss: 1.666410207748413, Train Acc: 0.50390625 Test Acc: 0.490234375\n",
      "Step 408 -- Train loss: 1.6752283573150635, Train Acc: 0.50048828125 Test Acc: 0.525390625\n",
      "Step 409 -- Train loss: 1.6724839210510254, Train Acc: 0.49169921875 Test Acc: 0.5009765625\n",
      "Step 410 -- Train loss: 1.6658437252044678, Train Acc: 0.510009765625 Test Acc: 0.494140625\n",
      "Step 411 -- Train loss: 1.682654857635498, Train Acc: 0.4873046875 Test Acc: 0.5244140625\n",
      "Step 412 -- Train loss: 1.6842402219772339, Train Acc: 0.49755859375 Test Acc: 0.505859375\n",
      "Step 413 -- Train loss: 1.6624021530151367, Train Acc: 0.512939453125 Test Acc: 0.4990234375\n",
      "Step 414 -- Train loss: 1.6725146770477295, Train Acc: 0.485595703125 Test Acc: 0.4677734375\n",
      "Step 415 -- Train loss: 1.6701215505599976, Train Acc: 0.4833984375 Test Acc: 0.4658203125\n",
      "Step 416 -- Train loss: 1.666490912437439, Train Acc: 0.49560546875 Test Acc: 0.4931640625\n",
      "Step 417 -- Train loss: 1.667057991027832, Train Acc: 0.47705078125 Test Acc: 0.5009765625\n",
      "Step 418 -- Train loss: 1.6708191633224487, Train Acc: 0.474365234375 Test Acc: 0.4755859375\n",
      "Step 419 -- Train loss: 1.672640323638916, Train Acc: 0.49755859375 Test Acc: 0.4697265625\n",
      "Step 420 -- Train loss: 1.661279320716858, Train Acc: 0.486572265625 Test Acc: 0.494140625\n",
      "Step 421 -- Train loss: 1.670408010482788, Train Acc: 0.470703125 Test Acc: 0.4853515625\n",
      "Step 422 -- Train loss: 1.6712666749954224, Train Acc: 0.496826171875 Test Acc: 0.5029296875\n",
      "Step 423 -- Train loss: 1.6425222158432007, Train Acc: 0.509521484375 Test Acc: 0.5283203125\n",
      "Step 424 -- Train loss: 1.662035346031189, Train Acc: 0.5107421875 Test Acc: 0.5107421875\n",
      "Step 425 -- Train loss: 1.6570872068405151, Train Acc: 0.499267578125 Test Acc: 0.5224609375\n",
      "Step 426 -- Train loss: 1.667447566986084, Train Acc: 0.494384765625 Test Acc: 0.52734375\n",
      "Step 427 -- Train loss: 1.654941439628601, Train Acc: 0.515380859375 Test Acc: 0.509765625\n",
      "Step 428 -- Train loss: 1.645235538482666, Train Acc: 0.51708984375 Test Acc: 0.4951171875\n",
      "Step 429 -- Train loss: 1.645415186882019, Train Acc: 0.5283203125 Test Acc: 0.4990234375\n",
      "Step 430 -- Train loss: 1.652953863143921, Train Acc: 0.51953125 Test Acc: 0.51171875\n",
      "Step 431 -- Train loss: 1.653414249420166, Train Acc: 0.522705078125 Test Acc: 0.5263671875\n",
      "Step 432 -- Train loss: 1.6517908573150635, Train Acc: 0.53662109375 Test Acc: 0.5234375\n",
      "Step 433 -- Train loss: 1.6522732973098755, Train Acc: 0.53076171875 Test Acc: 0.5224609375\n",
      "Step 434 -- Train loss: 1.6454476118087769, Train Acc: 0.54248046875 Test Acc: 0.5322265625\n",
      "Step 435 -- Train loss: 1.656064510345459, Train Acc: 0.52197265625 Test Acc: 0.5498046875\n",
      "Step 436 -- Train loss: 1.6492869853973389, Train Acc: 0.54345703125 Test Acc: 0.51171875\n",
      "Step 437 -- Train loss: 1.6423901319503784, Train Acc: 0.527587890625 Test Acc: 0.5205078125\n",
      "Step 438 -- Train loss: 1.6380010843276978, Train Acc: 0.53369140625 Test Acc: 0.5068359375\n",
      "Step 439 -- Train loss: 1.645575761795044, Train Acc: 0.521728515625 Test Acc: 0.513671875\n",
      "Step 440 -- Train loss: 1.6297699213027954, Train Acc: 0.525634765625 Test Acc: 0.49609375\n",
      "Step 441 -- Train loss: 1.6373955011367798, Train Acc: 0.51513671875 Test Acc: 0.5283203125\n",
      "Step 442 -- Train loss: 1.6374562978744507, Train Acc: 0.523193359375 Test Acc: 0.5107421875\n",
      "Step 443 -- Train loss: 1.6454392671585083, Train Acc: 0.510986328125 Test Acc: 0.5546875\n",
      "Step 444 -- Train loss: 1.6432113647460938, Train Acc: 0.514404296875 Test Acc: 0.513671875\n",
      "Step 445 -- Train loss: 1.6386172771453857, Train Acc: 0.509521484375 Test Acc: 0.484375\n",
      "Step 446 -- Train loss: 1.645311951637268, Train Acc: 0.498291015625 Test Acc: 0.5029296875\n",
      "Step 447 -- Train loss: 1.6415255069732666, Train Acc: 0.509521484375 Test Acc: 0.490234375\n",
      "Step 448 -- Train loss: 1.6313050985336304, Train Acc: 0.492431640625 Test Acc: 0.5\n",
      "Step 449 -- Train loss: 1.6284452676773071, Train Acc: 0.507080078125 Test Acc: 0.5009765625\n",
      "Step 450 -- Train loss: 1.6244440078735352, Train Acc: 0.5009765625 Test Acc: 0.5078125\n",
      "Step 451 -- Train loss: 1.6408401727676392, Train Acc: 0.511962890625 Test Acc: 0.49609375\n",
      "Step 452 -- Train loss: 1.6293030977249146, Train Acc: 0.520751953125 Test Acc: 0.5146484375\n",
      "Step 453 -- Train loss: 1.625590205192566, Train Acc: 0.53125 Test Acc: 0.5078125\n",
      "Step 454 -- Train loss: 1.637735366821289, Train Acc: 0.51953125 Test Acc: 0.5068359375\n",
      "Step 455 -- Train loss: 1.6272037029266357, Train Acc: 0.50439453125 Test Acc: 0.5234375\n",
      "Step 456 -- Train loss: 1.622806191444397, Train Acc: 0.520751953125 Test Acc: 0.5400390625\n",
      "Step 457 -- Train loss: 1.6189452409744263, Train Acc: 0.537841796875 Test Acc: 0.5419921875\n",
      "Step 458 -- Train loss: 1.6318408250808716, Train Acc: 0.517578125 Test Acc: 0.529296875\n",
      "Step 459 -- Train loss: 1.6179955005645752, Train Acc: 0.532470703125 Test Acc: 0.490234375\n",
      "Step 460 -- Train loss: 1.6303101778030396, Train Acc: 0.51806640625 Test Acc: 0.525390625\n",
      "Step 461 -- Train loss: 1.6159439086914062, Train Acc: 0.512939453125 Test Acc: 0.513671875\n",
      "Step 462 -- Train loss: 1.6298524141311646, Train Acc: 0.501953125 Test Acc: 0.51953125\n",
      "Step 463 -- Train loss: 1.6203540563583374, Train Acc: 0.507080078125 Test Acc: 0.51953125\n",
      "Step 464 -- Train loss: 1.622231364250183, Train Acc: 0.511474609375 Test Acc: 0.5244140625\n",
      "Step 465 -- Train loss: 1.6215945482254028, Train Acc: 0.5302734375 Test Acc: 0.4833984375\n",
      "Step 466 -- Train loss: 1.6042938232421875, Train Acc: 0.549072265625 Test Acc: 0.5263671875\n",
      "Step 467 -- Train loss: 1.6208518743515015, Train Acc: 0.53759765625 Test Acc: 0.5244140625\n",
      "Step 468 -- Train loss: 1.6141108274459839, Train Acc: 0.5390625 Test Acc: 0.5068359375\n",
      "Step 469 -- Train loss: 1.6184557676315308, Train Acc: 0.521728515625 Test Acc: 0.5361328125\n",
      "Step 470 -- Train loss: 1.6069939136505127, Train Acc: 0.529052734375 Test Acc: 0.525390625\n",
      "Step 471 -- Train loss: 1.6119133234024048, Train Acc: 0.538330078125 Test Acc: 0.5283203125\n",
      "Step 472 -- Train loss: 1.613623023033142, Train Acc: 0.535888671875 Test Acc: 0.541015625\n",
      "Step 473 -- Train loss: 1.610030174255371, Train Acc: 0.531982421875 Test Acc: 0.541015625\n",
      "Step 474 -- Train loss: 1.595674991607666, Train Acc: 0.540283203125 Test Acc: 0.5322265625\n",
      "Step 475 -- Train loss: 1.6172176599502563, Train Acc: 0.523681640625 Test Acc: 0.5078125\n",
      "Step 476 -- Train loss: 1.6003425121307373, Train Acc: 0.54150390625 Test Acc: 0.5078125\n",
      "Step 477 -- Train loss: 1.6152108907699585, Train Acc: 0.505615234375 Test Acc: 0.5263671875\n",
      "Step 478 -- Train loss: 1.6106626987457275, Train Acc: 0.53125 Test Acc: 0.5205078125\n",
      "Step 479 -- Train loss: 1.6124945878982544, Train Acc: 0.5439453125 Test Acc: 0.5400390625\n",
      "Step 480 -- Train loss: 1.6248767375946045, Train Acc: 0.53662109375 Test Acc: 0.5576171875\n",
      "Step 481 -- Train loss: 1.617271065711975, Train Acc: 0.527099609375 Test Acc: 0.5185546875\n",
      "Step 482 -- Train loss: 1.6092860698699951, Train Acc: 0.540771484375 Test Acc: 0.546875\n",
      "Step 483 -- Train loss: 1.6442656517028809, Train Acc: 0.524658203125 Test Acc: 0.5048828125\n",
      "Step 484 -- Train loss: 1.665366291999817, Train Acc: 0.508056640625 Test Acc: 0.505859375\n",
      "Step 485 -- Train loss: 1.6719354391098022, Train Acc: 0.507080078125 Test Acc: 0.5068359375\n",
      "Step 486 -- Train loss: 1.6789703369140625, Train Acc: 0.5087890625 Test Acc: 0.5087890625\n",
      "Step 487 -- Train loss: 1.6609606742858887, Train Acc: 0.521728515625 Test Acc: 0.509765625\n",
      "Step 488 -- Train loss: 1.634453296661377, Train Acc: 0.531982421875 Test Acc: 0.509765625\n",
      "Step 489 -- Train loss: 1.659436583518982, Train Acc: 0.523193359375 Test Acc: 0.48046875\n",
      "Step 490 -- Train loss: 1.626820683479309, Train Acc: 0.521728515625 Test Acc: 0.5283203125\n",
      "Step 491 -- Train loss: 1.6169941425323486, Train Acc: 0.512939453125 Test Acc: 0.5390625\n",
      "Step 492 -- Train loss: 1.6348627805709839, Train Acc: 0.514404296875 Test Acc: 0.5283203125\n",
      "Step 493 -- Train loss: 1.62091863155365, Train Acc: 0.528076171875 Test Acc: 0.5126953125\n",
      "Step 494 -- Train loss: 1.6157641410827637, Train Acc: 0.53369140625 Test Acc: 0.5390625\n",
      "Step 495 -- Train loss: 1.621580719947815, Train Acc: 0.511962890625 Test Acc: 0.5439453125\n",
      "Step 496 -- Train loss: 1.6135838031768799, Train Acc: 0.5185546875 Test Acc: 0.505859375\n",
      "Step 497 -- Train loss: 1.6203525066375732, Train Acc: 0.5146484375 Test Acc: 0.533203125\n",
      "Step 498 -- Train loss: 1.6193876266479492, Train Acc: 0.53173828125 Test Acc: 0.5224609375\n",
      "Step 499 -- Train loss: 1.6100940704345703, Train Acc: 0.543701171875 Test Acc: 0.509765625\n",
      "Step 500 -- Train loss: 1.620592474937439, Train Acc: 0.538818359375 Test Acc: 0.5107421875\n",
      "Step 501 -- Train loss: 1.6078130006790161, Train Acc: 0.552001953125 Test Acc: 0.548828125\n",
      "Step 502 -- Train loss: 1.6020208597183228, Train Acc: 0.539306640625 Test Acc: 0.5380859375\n",
      "Step 503 -- Train loss: 1.5949475765228271, Train Acc: 0.549560546875 Test Acc: 0.5126953125\n",
      "Step 504 -- Train loss: 1.6000146865844727, Train Acc: 0.544189453125 Test Acc: 0.541015625\n",
      "Step 505 -- Train loss: 1.6048797369003296, Train Acc: 0.54541015625 Test Acc: 0.560546875\n",
      "Step 506 -- Train loss: 1.5906637907028198, Train Acc: 0.546875 Test Acc: 0.5380859375\n",
      "Step 507 -- Train loss: 1.5978705883026123, Train Acc: 0.556396484375 Test Acc: 0.58984375\n",
      "Step 508 -- Train loss: 1.5977810621261597, Train Acc: 0.548583984375 Test Acc: 0.5615234375\n",
      "Step 509 -- Train loss: 1.586569905281067, Train Acc: 0.561767578125 Test Acc: 0.5615234375\n",
      "Step 510 -- Train loss: 1.5933350324630737, Train Acc: 0.548095703125 Test Acc: 0.5234375\n",
      "Step 511 -- Train loss: 1.5770654678344727, Train Acc: 0.53662109375 Test Acc: 0.54296875\n",
      "Step 512 -- Train loss: 1.5850731134414673, Train Acc: 0.548095703125 Test Acc: 0.529296875\n",
      "Step 513 -- Train loss: 1.5940823554992676, Train Acc: 0.559326171875 Test Acc: 0.5546875\n",
      "Step 514 -- Train loss: 1.5816553831100464, Train Acc: 0.558837890625 Test Acc: 0.560546875\n",
      "Step 515 -- Train loss: 1.5832374095916748, Train Acc: 0.548828125 Test Acc: 0.513671875\n",
      "Step 516 -- Train loss: 1.5810267925262451, Train Acc: 0.54541015625 Test Acc: 0.521484375\n",
      "Step 517 -- Train loss: 1.5746618509292603, Train Acc: 0.55859375 Test Acc: 0.5732421875\n",
      "Step 518 -- Train loss: 1.58656907081604, Train Acc: 0.538818359375 Test Acc: 0.5625\n",
      "Step 519 -- Train loss: 1.572877049446106, Train Acc: 0.546142578125 Test Acc: 0.548828125\n",
      "Step 520 -- Train loss: 1.5770665407180786, Train Acc: 0.548583984375 Test Acc: 0.5625\n",
      "Step 521 -- Train loss: 1.5825245380401611, Train Acc: 0.546875 Test Acc: 0.5634765625\n",
      "Step 522 -- Train loss: 1.5741896629333496, Train Acc: 0.556396484375 Test Acc: 0.548828125\n",
      "Step 523 -- Train loss: 1.560586929321289, Train Acc: 0.568603515625 Test Acc: 0.5576171875\n",
      "Step 524 -- Train loss: 1.5667026042938232, Train Acc: 0.563232421875 Test Acc: 0.5654296875\n",
      "Step 525 -- Train loss: 1.5817914009094238, Train Acc: 0.55322265625 Test Acc: 0.564453125\n",
      "Step 526 -- Train loss: 1.5757843255996704, Train Acc: 0.54638671875 Test Acc: 0.544921875\n",
      "Step 527 -- Train loss: 1.5695723295211792, Train Acc: 0.546875 Test Acc: 0.5322265625\n",
      "Step 528 -- Train loss: 1.5826908349990845, Train Acc: 0.5458984375 Test Acc: 0.564453125\n",
      "Step 529 -- Train loss: 1.5740259885787964, Train Acc: 0.5478515625 Test Acc: 0.564453125\n",
      "Step 530 -- Train loss: 1.5646474361419678, Train Acc: 0.569580078125 Test Acc: 0.5830078125\n",
      "Step 531 -- Train loss: 1.5816419124603271, Train Acc: 0.571533203125 Test Acc: 0.5712890625\n",
      "Step 532 -- Train loss: 1.572087049484253, Train Acc: 0.592529296875 Test Acc: 0.5927734375\n",
      "Step 533 -- Train loss: 1.5714335441589355, Train Acc: 0.588623046875 Test Acc: 0.5732421875\n",
      "Step 534 -- Train loss: 1.578462839126587, Train Acc: 0.576904296875 Test Acc: 0.5625\n",
      "Step 535 -- Train loss: 1.5673191547393799, Train Acc: 0.57421875 Test Acc: 0.580078125\n",
      "Step 536 -- Train loss: 1.5626928806304932, Train Acc: 0.58349609375 Test Acc: 0.5654296875\n",
      "Step 537 -- Train loss: 1.5586656332015991, Train Acc: 0.562744140625 Test Acc: 0.568359375\n",
      "Step 538 -- Train loss: 1.558943748474121, Train Acc: 0.5625 Test Acc: 0.5302734375\n",
      "Step 539 -- Train loss: 1.5687241554260254, Train Acc: 0.5556640625 Test Acc: 0.54296875\n",
      "Step 540 -- Train loss: 1.5718200206756592, Train Acc: 0.542724609375 Test Acc: 0.5244140625\n",
      "Step 541 -- Train loss: 1.5729715824127197, Train Acc: 0.57080078125 Test Acc: 0.58203125\n",
      "Step 542 -- Train loss: 1.566040277481079, Train Acc: 0.567626953125 Test Acc: 0.5556640625\n",
      "Step 543 -- Train loss: 1.561352014541626, Train Acc: 0.573486328125 Test Acc: 0.576171875\n",
      "Step 544 -- Train loss: 1.567042589187622, Train Acc: 0.57763671875 Test Acc: 0.552734375\n",
      "Step 545 -- Train loss: 1.5614571571350098, Train Acc: 0.5849609375 Test Acc: 0.5537109375\n",
      "Step 546 -- Train loss: 1.556317687034607, Train Acc: 0.575439453125 Test Acc: 0.5703125\n",
      "Step 547 -- Train loss: 1.5542922019958496, Train Acc: 0.568603515625 Test Acc: 0.56640625\n",
      "Step 548 -- Train loss: 1.550953984260559, Train Acc: 0.567138671875 Test Acc: 0.5341796875\n",
      "Step 549 -- Train loss: 1.5570343732833862, Train Acc: 0.557373046875 Test Acc: 0.56640625\n",
      "Step 550 -- Train loss: 1.5629119873046875, Train Acc: 0.571533203125 Test Acc: 0.587890625\n",
      "Step 551 -- Train loss: 1.560290813446045, Train Acc: 0.572998046875 Test Acc: 0.5751953125\n",
      "Step 552 -- Train loss: 1.559633493423462, Train Acc: 0.564208984375 Test Acc: 0.5517578125\n",
      "Step 553 -- Train loss: 1.5553464889526367, Train Acc: 0.554443359375 Test Acc: 0.568359375\n",
      "Step 554 -- Train loss: 1.5554683208465576, Train Acc: 0.5673828125 Test Acc: 0.564453125\n",
      "Step 555 -- Train loss: 1.5752942562103271, Train Acc: 0.5546875 Test Acc: 0.5556640625\n",
      "Step 556 -- Train loss: 1.5526695251464844, Train Acc: 0.569580078125 Test Acc: 0.583984375\n",
      "Step 557 -- Train loss: 1.5447977781295776, Train Acc: 0.57421875 Test Acc: 0.576171875\n",
      "Step 558 -- Train loss: 1.5611306428909302, Train Acc: 0.576904296875 Test Acc: 0.5615234375\n",
      "Step 559 -- Train loss: 1.5565276145935059, Train Acc: 0.56787109375 Test Acc: 0.5712890625\n",
      "Step 560 -- Train loss: 1.5516470670700073, Train Acc: 0.574462890625 Test Acc: 0.5478515625\n",
      "Step 561 -- Train loss: 1.544991374015808, Train Acc: 0.578857421875 Test Acc: 0.560546875\n",
      "Step 562 -- Train loss: 1.5560818910598755, Train Acc: 0.562744140625 Test Acc: 0.55859375\n",
      "Step 563 -- Train loss: 1.551586627960205, Train Acc: 0.56689453125 Test Acc: 0.5634765625\n",
      "Step 564 -- Train loss: 1.5515278577804565, Train Acc: 0.591064453125 Test Acc: 0.6025390625\n",
      "Step 565 -- Train loss: 1.5560075044631958, Train Acc: 0.587890625 Test Acc: 0.55859375\n",
      "Step 566 -- Train loss: 1.5508989095687866, Train Acc: 0.594970703125 Test Acc: 0.5751953125\n",
      "Step 567 -- Train loss: 1.5506190061569214, Train Acc: 0.59326171875 Test Acc: 0.59375\n",
      "Step 568 -- Train loss: 1.551159381866455, Train Acc: 0.592529296875 Test Acc: 0.5791015625\n",
      "Step 569 -- Train loss: 1.5447801351547241, Train Acc: 0.58984375 Test Acc: 0.5986328125\n",
      "Step 570 -- Train loss: 1.5332006216049194, Train Acc: 0.57763671875 Test Acc: 0.5390625\n",
      "Step 571 -- Train loss: 1.5497039556503296, Train Acc: 0.5751953125 Test Acc: 0.587890625\n",
      "Step 572 -- Train loss: 1.5494914054870605, Train Acc: 0.579833984375 Test Acc: 0.56640625\n",
      "Step 573 -- Train loss: 1.5400563478469849, Train Acc: 0.599365234375 Test Acc: 0.6015625\n",
      "Step 574 -- Train loss: 1.5531491041183472, Train Acc: 0.600341796875 Test Acc: 0.576171875\n",
      "Step 575 -- Train loss: 1.542548656463623, Train Acc: 0.60009765625 Test Acc: 0.6015625\n",
      "Step 576 -- Train loss: 1.5465354919433594, Train Acc: 0.5830078125 Test Acc: 0.6044921875\n",
      "Step 577 -- Train loss: 1.5344234704971313, Train Acc: 0.611083984375 Test Acc: 0.591796875\n",
      "Step 578 -- Train loss: 1.5458548069000244, Train Acc: 0.595458984375 Test Acc: 0.5927734375\n",
      "Step 579 -- Train loss: 1.5290476083755493, Train Acc: 0.6015625 Test Acc: 0.595703125\n",
      "Step 580 -- Train loss: 1.5379695892333984, Train Acc: 0.587646484375 Test Acc: 0.560546875\n",
      "Step 581 -- Train loss: 1.5415277481079102, Train Acc: 0.5986328125 Test Acc: 0.5634765625\n",
      "Step 582 -- Train loss: 1.5481613874435425, Train Acc: 0.587890625 Test Acc: 0.5849609375\n",
      "Step 583 -- Train loss: 1.537032127380371, Train Acc: 0.6064453125 Test Acc: 0.587890625\n",
      "Step 584 -- Train loss: 1.5411382913589478, Train Acc: 0.58984375 Test Acc: 0.587890625\n",
      "Step 585 -- Train loss: 1.5373687744140625, Train Acc: 0.58935546875 Test Acc: 0.5947265625\n",
      "Step 586 -- Train loss: 1.5266183614730835, Train Acc: 0.58447265625 Test Acc: 0.5546875\n",
      "Step 587 -- Train loss: 1.5326331853866577, Train Acc: 0.589599609375 Test Acc: 0.5810546875\n",
      "Step 588 -- Train loss: 1.534382939338684, Train Acc: 0.58447265625 Test Acc: 0.6083984375\n",
      "Step 589 -- Train loss: 1.5329785346984863, Train Acc: 0.5830078125 Test Acc: 0.5712890625\n",
      "Step 590 -- Train loss: 1.5387197732925415, Train Acc: 0.57568359375 Test Acc: 0.560546875\n",
      "Step 591 -- Train loss: 1.53714120388031, Train Acc: 0.579345703125 Test Acc: 0.58984375\n",
      "Step 592 -- Train loss: 1.538482427597046, Train Acc: 0.581787109375 Test Acc: 0.5791015625\n",
      "Step 593 -- Train loss: 1.5371121168136597, Train Acc: 0.58935546875 Test Acc: 0.6044921875\n",
      "Step 594 -- Train loss: 1.5391860008239746, Train Acc: 0.591552734375 Test Acc: 0.5771484375\n",
      "Step 595 -- Train loss: 1.533349633216858, Train Acc: 0.59912109375 Test Acc: 0.607421875\n",
      "Step 596 -- Train loss: 1.52436101436615, Train Acc: 0.607666015625 Test Acc: 0.6123046875\n",
      "Step 597 -- Train loss: 1.5283160209655762, Train Acc: 0.600341796875 Test Acc: 0.595703125\n",
      "Step 598 -- Train loss: 1.532759189605713, Train Acc: 0.60107421875 Test Acc: 0.552734375\n",
      "Step 599 -- Train loss: 1.528573751449585, Train Acc: 0.602783203125 Test Acc: 0.5732421875\n",
      "Step 600 -- Train loss: 1.5183905363082886, Train Acc: 0.608154296875 Test Acc: 0.6298828125\n",
      "Step 601 -- Train loss: 1.5281007289886475, Train Acc: 0.59814453125 Test Acc: 0.62109375\n",
      "Step 602 -- Train loss: 1.518040418624878, Train Acc: 0.609619140625 Test Acc: 0.5947265625\n",
      "Step 603 -- Train loss: 1.5409622192382812, Train Acc: 0.589599609375 Test Acc: 0.615234375\n",
      "Step 604 -- Train loss: 1.512477993965149, Train Acc: 0.607177734375 Test Acc: 0.5966796875\n",
      "Step 605 -- Train loss: 1.52346932888031, Train Acc: 0.580810546875 Test Acc: 0.603515625\n",
      "Step 606 -- Train loss: 1.517946720123291, Train Acc: 0.590576171875 Test Acc: 0.5830078125\n",
      "Step 607 -- Train loss: 1.5177278518676758, Train Acc: 0.594482421875 Test Acc: 0.5859375\n",
      "Step 608 -- Train loss: 1.5235275030136108, Train Acc: 0.59765625 Test Acc: 0.5908203125\n",
      "Step 609 -- Train loss: 1.5188575983047485, Train Acc: 0.591064453125 Test Acc: 0.587890625\n",
      "Step 610 -- Train loss: 1.528022050857544, Train Acc: 0.600830078125 Test Acc: 0.5830078125\n",
      "Step 611 -- Train loss: 1.51791512966156, Train Acc: 0.618896484375 Test Acc: 0.5947265625\n",
      "Step 612 -- Train loss: 1.5187395811080933, Train Acc: 0.6259765625 Test Acc: 0.615234375\n",
      "Step 613 -- Train loss: 1.5174278020858765, Train Acc: 0.603759765625 Test Acc: 0.58203125\n",
      "Step 614 -- Train loss: 1.5247265100479126, Train Acc: 0.599609375 Test Acc: 0.611328125\n",
      "Step 615 -- Train loss: 1.521329402923584, Train Acc: 0.59423828125 Test Acc: 0.599609375\n",
      "Step 616 -- Train loss: 1.5122559070587158, Train Acc: 0.594482421875 Test Acc: 0.6015625\n",
      "Step 617 -- Train loss: 1.5138386487960815, Train Acc: 0.578369140625 Test Acc: 0.58984375\n",
      "Step 618 -- Train loss: 1.5075913667678833, Train Acc: 0.593017578125 Test Acc: 0.5859375\n",
      "Step 619 -- Train loss: 1.521133542060852, Train Acc: 0.570556640625 Test Acc: 0.5888671875\n",
      "Step 620 -- Train loss: 1.503739595413208, Train Acc: 0.580078125 Test Acc: 0.5673828125\n",
      "Step 621 -- Train loss: 1.5079703330993652, Train Acc: 0.57275390625 Test Acc: 0.5810546875\n",
      "Step 622 -- Train loss: 1.515586256980896, Train Acc: 0.5498046875 Test Acc: 0.5517578125\n",
      "Step 623 -- Train loss: 1.520353078842163, Train Acc: 0.553955078125 Test Acc: 0.5673828125\n",
      "Step 624 -- Train loss: 1.5059072971343994, Train Acc: 0.558837890625 Test Acc: 0.5634765625\n",
      "Step 625 -- Train loss: 1.51579749584198, Train Acc: 0.561767578125 Test Acc: 0.572265625\n",
      "Step 626 -- Train loss: 1.5127527713775635, Train Acc: 0.587646484375 Test Acc: 0.5673828125\n",
      "Step 627 -- Train loss: 1.5121197700500488, Train Acc: 0.601806640625 Test Acc: 0.5654296875\n",
      "Step 628 -- Train loss: 1.5070891380310059, Train Acc: 0.60400390625 Test Acc: 0.5869140625\n",
      "Step 629 -- Train loss: 1.5229072570800781, Train Acc: 0.599609375 Test Acc: 0.5791015625\n",
      "Step 630 -- Train loss: 1.508908987045288, Train Acc: 0.600830078125 Test Acc: 0.603515625\n",
      "Step 631 -- Train loss: 1.5155909061431885, Train Acc: 0.597900390625 Test Acc: 0.580078125\n",
      "Step 632 -- Train loss: 1.5212106704711914, Train Acc: 0.59375 Test Acc: 0.6005859375\n",
      "Step 633 -- Train loss: 1.49805748462677, Train Acc: 0.61572265625 Test Acc: 0.5986328125\n",
      "Step 634 -- Train loss: 1.519981026649475, Train Acc: 0.60107421875 Test Acc: 0.595703125\n",
      "Step 635 -- Train loss: 1.500015377998352, Train Acc: 0.608154296875 Test Acc: 0.5771484375\n",
      "Step 636 -- Train loss: 1.5065687894821167, Train Acc: 0.6083984375 Test Acc: 0.61328125\n",
      "Step 637 -- Train loss: 1.5099380016326904, Train Acc: 0.596923828125 Test Acc: 0.5771484375\n",
      "Step 638 -- Train loss: 1.5085288286209106, Train Acc: 0.595947265625 Test Acc: 0.5966796875\n",
      "Step 639 -- Train loss: 1.5056495666503906, Train Acc: 0.59326171875 Test Acc: 0.5703125\n",
      "Step 640 -- Train loss: 1.500742793083191, Train Acc: 0.58349609375 Test Acc: 0.5927734375\n",
      "Step 641 -- Train loss: 1.4955767393112183, Train Acc: 0.591552734375 Test Acc: 0.5732421875\n",
      "Step 642 -- Train loss: 1.5126423835754395, Train Acc: 0.576904296875 Test Acc: 0.595703125\n",
      "Step 643 -- Train loss: 1.5070858001708984, Train Acc: 0.587646484375 Test Acc: 0.619140625\n",
      "Step 644 -- Train loss: 1.5108661651611328, Train Acc: 0.580078125 Test Acc: 0.595703125\n",
      "Step 645 -- Train loss: 1.5015170574188232, Train Acc: 0.594482421875 Test Acc: 0.57421875\n",
      "Step 646 -- Train loss: 1.5062586069107056, Train Acc: 0.591552734375 Test Acc: 0.576171875\n",
      "Step 647 -- Train loss: 1.4980655908584595, Train Acc: 0.588134765625 Test Acc: 0.5888671875\n",
      "Step 648 -- Train loss: 1.499137282371521, Train Acc: 0.594482421875 Test Acc: 0.5927734375\n",
      "Step 649 -- Train loss: 1.5046031475067139, Train Acc: 0.593505859375 Test Acc: 0.5947265625\n",
      "Step 650 -- Train loss: 1.4953190088272095, Train Acc: 0.5888671875 Test Acc: 0.5986328125\n",
      "Step 651 -- Train loss: 1.4856842756271362, Train Acc: 0.603515625 Test Acc: 0.578125\n",
      "Step 652 -- Train loss: 1.4987579584121704, Train Acc: 0.59814453125 Test Acc: 0.5908203125\n",
      "Step 653 -- Train loss: 1.4991085529327393, Train Acc: 0.603515625 Test Acc: 0.5732421875\n",
      "Step 654 -- Train loss: 1.4939525127410889, Train Acc: 0.613525390625 Test Acc: 0.6044921875\n",
      "Step 655 -- Train loss: 1.5023114681243896, Train Acc: 0.586669921875 Test Acc: 0.591796875\n",
      "Step 656 -- Train loss: 1.501448392868042, Train Acc: 0.59375 Test Acc: 0.5888671875\n",
      "Step 657 -- Train loss: 1.4893076419830322, Train Acc: 0.60986328125 Test Acc: 0.6220703125\n",
      "Step 658 -- Train loss: 1.487797737121582, Train Acc: 0.610107421875 Test Acc: 0.5966796875\n",
      "Step 659 -- Train loss: 1.5086506605148315, Train Acc: 0.58056640625 Test Acc: 0.5947265625\n",
      "Step 660 -- Train loss: 1.4939141273498535, Train Acc: 0.599853515625 Test Acc: 0.5751953125\n",
      "Step 661 -- Train loss: 1.4901039600372314, Train Acc: 0.61083984375 Test Acc: 0.6025390625\n",
      "Step 662 -- Train loss: 1.4932451248168945, Train Acc: 0.601318359375 Test Acc: 0.6064453125\n",
      "Step 663 -- Train loss: 1.5003466606140137, Train Acc: 0.60498046875 Test Acc: 0.6279296875\n",
      "Step 664 -- Train loss: 1.4879378080368042, Train Acc: 0.603759765625 Test Acc: 0.5986328125\n",
      "Step 665 -- Train loss: 1.4925827980041504, Train Acc: 0.603759765625 Test Acc: 0.5849609375\n",
      "Step 666 -- Train loss: 1.4783767461776733, Train Acc: 0.62548828125 Test Acc: 0.603515625\n",
      "Step 667 -- Train loss: 1.4911991357803345, Train Acc: 0.603759765625 Test Acc: 0.6005859375\n",
      "Step 668 -- Train loss: 1.487499475479126, Train Acc: 0.589599609375 Test Acc: 0.5908203125\n",
      "Step 669 -- Train loss: 1.4861693382263184, Train Acc: 0.6142578125 Test Acc: 0.5849609375\n",
      "Step 670 -- Train loss: 1.5007312297821045, Train Acc: 0.578857421875 Test Acc: 0.59375\n",
      "Step 671 -- Train loss: 1.4783836603164673, Train Acc: 0.596923828125 Test Acc: 0.611328125\n",
      "Step 672 -- Train loss: 1.4867875576019287, Train Acc: 0.591552734375 Test Acc: 0.583984375\n",
      "Step 673 -- Train loss: 1.4976950883865356, Train Acc: 0.579833984375 Test Acc: 0.591796875\n",
      "Step 674 -- Train loss: 1.4844505786895752, Train Acc: 0.577880859375 Test Acc: 0.546875\n",
      "Step 675 -- Train loss: 1.4738229513168335, Train Acc: 0.6005859375 Test Acc: 0.5654296875\n",
      "Step 676 -- Train loss: 1.4861927032470703, Train Acc: 0.591064453125 Test Acc: 0.5869140625\n",
      "Step 677 -- Train loss: 1.4840812683105469, Train Acc: 0.578857421875 Test Acc: 0.6083984375\n",
      "Step 678 -- Train loss: 1.4762191772460938, Train Acc: 0.58544921875 Test Acc: 0.6015625\n",
      "Step 679 -- Train loss: 1.4767279624938965, Train Acc: 0.591064453125 Test Acc: 0.57421875\n",
      "Step 680 -- Train loss: 1.4858006238937378, Train Acc: 0.578857421875 Test Acc: 0.5810546875\n",
      "Step 681 -- Train loss: 1.5287249088287354, Train Acc: 0.564208984375 Test Acc: 0.5947265625\n",
      "Step 682 -- Train loss: 1.5625561475753784, Train Acc: 0.577392578125 Test Acc: 0.5751953125\n",
      "Step 683 -- Train loss: 1.5856661796569824, Train Acc: 0.580322265625 Test Acc: 0.58203125\n",
      "Step 684 -- Train loss: 1.5534510612487793, Train Acc: 0.574462890625 Test Acc: 0.5869140625\n",
      "Step 685 -- Train loss: 1.5461697578430176, Train Acc: 0.58349609375 Test Acc: 0.564453125\n",
      "Step 686 -- Train loss: 1.5404287576675415, Train Acc: 0.5615234375 Test Acc: 0.5693359375\n",
      "Step 687 -- Train loss: 1.539501667022705, Train Acc: 0.563720703125 Test Acc: 0.5810546875\n",
      "Step 688 -- Train loss: 1.4970510005950928, Train Acc: 0.568359375 Test Acc: 0.611328125\n",
      "Step 689 -- Train loss: 1.5053333044052124, Train Acc: 0.57470703125 Test Acc: 0.5654296875\n",
      "Step 690 -- Train loss: 1.510818362236023, Train Acc: 0.571044921875 Test Acc: 0.58203125\n",
      "Step 691 -- Train loss: 1.5253020524978638, Train Acc: 0.558837890625 Test Acc: 0.564453125\n",
      "Step 692 -- Train loss: 1.5287991762161255, Train Acc: 0.5556640625 Test Acc: 0.5546875\n",
      "Step 693 -- Train loss: 1.5139729976654053, Train Acc: 0.580810546875 Test Acc: 0.556640625\n",
      "Step 694 -- Train loss: 1.5054879188537598, Train Acc: 0.59033203125 Test Acc: 0.5771484375\n",
      "Step 695 -- Train loss: 1.5115231275558472, Train Acc: 0.583984375 Test Acc: 0.59375\n",
      "Step 696 -- Train loss: 1.5076260566711426, Train Acc: 0.580810546875 Test Acc: 0.5966796875\n",
      "Step 697 -- Train loss: 1.5066457986831665, Train Acc: 0.591064453125 Test Acc: 0.58203125\n",
      "Step 698 -- Train loss: 1.5088269710540771, Train Acc: 0.588134765625 Test Acc: 0.576171875\n",
      "Step 699 -- Train loss: 1.5018936395645142, Train Acc: 0.58251953125 Test Acc: 0.580078125\n",
      "Step 700 -- Train loss: 1.4843212366104126, Train Acc: 0.593017578125 Test Acc: 0.5712890625\n",
      "Step 701 -- Train loss: 1.4972825050354004, Train Acc: 0.606689453125 Test Acc: 0.6064453125\n",
      "Step 702 -- Train loss: 1.4875823259353638, Train Acc: 0.61572265625 Test Acc: 0.5888671875\n",
      "Step 703 -- Train loss: 1.4769389629364014, Train Acc: 0.613525390625 Test Acc: 0.5966796875\n",
      "Step 704 -- Train loss: 1.4986248016357422, Train Acc: 0.607177734375 Test Acc: 0.5986328125\n",
      "Step 705 -- Train loss: 1.489283800125122, Train Acc: 0.609375 Test Acc: 0.6064453125\n",
      "Step 706 -- Train loss: 1.488134741783142, Train Acc: 0.5966796875 Test Acc: 0.6298828125\n",
      "Step 707 -- Train loss: 1.497084379196167, Train Acc: 0.583251953125 Test Acc: 0.626953125\n",
      "Step 708 -- Train loss: 1.4857985973358154, Train Acc: 0.599365234375 Test Acc: 0.59765625\n",
      "Step 709 -- Train loss: 1.4784897565841675, Train Acc: 0.600830078125 Test Acc: 0.5869140625\n",
      "Step 710 -- Train loss: 1.4746018648147583, Train Acc: 0.61376953125 Test Acc: 0.57421875\n",
      "Step 711 -- Train loss: 1.478611946105957, Train Acc: 0.604248046875 Test Acc: 0.6162109375\n",
      "Step 712 -- Train loss: 1.471858024597168, Train Acc: 0.597900390625 Test Acc: 0.5791015625\n",
      "Step 713 -- Train loss: 1.4840258359909058, Train Acc: 0.60302734375 Test Acc: 0.5810546875\n",
      "Step 714 -- Train loss: 1.4731677770614624, Train Acc: 0.596435546875 Test Acc: 0.5771484375\n",
      "Step 715 -- Train loss: 1.4812427759170532, Train Acc: 0.592041015625 Test Acc: 0.6123046875\n",
      "Step 716 -- Train loss: 1.4700546264648438, Train Acc: 0.59326171875 Test Acc: 0.583984375\n",
      "Step 717 -- Train loss: 1.4784294366836548, Train Acc: 0.591552734375 Test Acc: 0.572265625\n",
      "Step 718 -- Train loss: 1.4830644130706787, Train Acc: 0.557861328125 Test Acc: 0.5966796875\n",
      "Step 719 -- Train loss: 1.4672670364379883, Train Acc: 0.594970703125 Test Acc: 0.568359375\n",
      "Step 720 -- Train loss: 1.4724336862564087, Train Acc: 0.577880859375 Test Acc: 0.58984375\n",
      "Step 721 -- Train loss: 1.4634721279144287, Train Acc: 0.58642578125 Test Acc: 0.6005859375\n",
      "Step 722 -- Train loss: 1.4673031568527222, Train Acc: 0.585693359375 Test Acc: 0.595703125\n",
      "Step 723 -- Train loss: 1.4729152917861938, Train Acc: 0.58056640625 Test Acc: 0.595703125\n",
      "Step 724 -- Train loss: 1.4667259454727173, Train Acc: 0.580810546875 Test Acc: 0.6171875\n",
      "Step 725 -- Train loss: 1.4578027725219727, Train Acc: 0.588623046875 Test Acc: 0.6015625\n",
      "Step 726 -- Train loss: 1.4603838920593262, Train Acc: 0.59228515625 Test Acc: 0.57421875\n",
      "Step 727 -- Train loss: 1.4663782119750977, Train Acc: 0.601318359375 Test Acc: 0.5869140625\n",
      "Step 728 -- Train loss: 1.4699416160583496, Train Acc: 0.605224609375 Test Acc: 0.5888671875\n",
      "Step 729 -- Train loss: 1.4664753675460815, Train Acc: 0.61376953125 Test Acc: 0.59765625\n",
      "Step 730 -- Train loss: 1.4532461166381836, Train Acc: 0.61279296875 Test Acc: 0.646484375\n",
      "Step 731 -- Train loss: 1.4557530879974365, Train Acc: 0.6240234375 Test Acc: 0.6337890625\n",
      "Step 732 -- Train loss: 1.4622917175292969, Train Acc: 0.6123046875 Test Acc: 0.5966796875\n",
      "Step 733 -- Train loss: 1.468271255493164, Train Acc: 0.602783203125 Test Acc: 0.6064453125\n",
      "Step 734 -- Train loss: 1.459661602973938, Train Acc: 0.606689453125 Test Acc: 0.640625\n",
      "Step 735 -- Train loss: 1.45341956615448, Train Acc: 0.603515625 Test Acc: 0.58984375\n",
      "Step 736 -- Train loss: 1.4526199102401733, Train Acc: 0.600341796875 Test Acc: 0.6181640625\n",
      "Step 737 -- Train loss: 1.4489301443099976, Train Acc: 0.611328125 Test Acc: 0.5830078125\n",
      "Step 738 -- Train loss: 1.4579845666885376, Train Acc: 0.602783203125 Test Acc: 0.583984375\n",
      "Step 739 -- Train loss: 1.4606196880340576, Train Acc: 0.620361328125 Test Acc: 0.634765625\n",
      "Step 740 -- Train loss: 1.4614230394363403, Train Acc: 0.611572265625 Test Acc: 0.5859375\n",
      "Step 741 -- Train loss: 1.4453226327896118, Train Acc: 0.620849609375 Test Acc: 0.6298828125\n",
      "Step 742 -- Train loss: 1.4493690729141235, Train Acc: 0.624267578125 Test Acc: 0.62109375\n",
      "Step 743 -- Train loss: 1.4558801651000977, Train Acc: 0.6142578125 Test Acc: 0.583984375\n",
      "Step 744 -- Train loss: 1.4592244625091553, Train Acc: 0.605224609375 Test Acc: 0.599609375\n",
      "Step 745 -- Train loss: 1.4479376077651978, Train Acc: 0.62451171875 Test Acc: 0.62109375\n",
      "Step 746 -- Train loss: 1.469337821006775, Train Acc: 0.602294921875 Test Acc: 0.62109375\n",
      "Step 747 -- Train loss: 1.4563273191452026, Train Acc: 0.630859375 Test Acc: 0.6201171875\n",
      "Step 748 -- Train loss: 1.4435052871704102, Train Acc: 0.638916015625 Test Acc: 0.650390625\n",
      "Step 749 -- Train loss: 1.4568185806274414, Train Acc: 0.63525390625 Test Acc: 0.6240234375\n",
      "Step 750 -- Train loss: 1.4551905393600464, Train Acc: 0.624267578125 Test Acc: 0.6064453125\n",
      "Step 751 -- Train loss: 1.450444221496582, Train Acc: 0.627197265625 Test Acc: 0.6181640625\n",
      "Step 752 -- Train loss: 1.451888084411621, Train Acc: 0.62158203125 Test Acc: 0.6181640625\n",
      "Step 753 -- Train loss: 1.4556125402450562, Train Acc: 0.61181640625 Test Acc: 0.646484375\n",
      "Step 754 -- Train loss: 1.442595362663269, Train Acc: 0.617919921875 Test Acc: 0.6025390625\n",
      "Step 755 -- Train loss: 1.4450139999389648, Train Acc: 0.611572265625 Test Acc: 0.595703125\n",
      "Step 756 -- Train loss: 1.4432401657104492, Train Acc: 0.625732421875 Test Acc: 0.607421875\n",
      "Step 757 -- Train loss: 1.4535713195800781, Train Acc: 0.6220703125 Test Acc: 0.5966796875\n",
      "Step 758 -- Train loss: 1.4505091905593872, Train Acc: 0.607177734375 Test Acc: 0.6083984375\n",
      "Step 759 -- Train loss: 1.430297613143921, Train Acc: 0.59375 Test Acc: 0.59375\n",
      "Step 760 -- Train loss: 1.4445024728775024, Train Acc: 0.596923828125 Test Acc: 0.5947265625\n",
      "Step 761 -- Train loss: 1.4363149404525757, Train Acc: 0.60400390625 Test Acc: 0.6044921875\n",
      "Step 762 -- Train loss: 1.4395489692687988, Train Acc: 0.615966796875 Test Acc: 0.6064453125\n",
      "Step 763 -- Train loss: 1.4383221864700317, Train Acc: 0.619384765625 Test Acc: 0.595703125\n",
      "Step 764 -- Train loss: 1.4394030570983887, Train Acc: 0.630615234375 Test Acc: 0.61328125\n",
      "Step 765 -- Train loss: 1.4348002672195435, Train Acc: 0.634521484375 Test Acc: 0.611328125\n",
      "Step 766 -- Train loss: 1.4310897588729858, Train Acc: 0.6357421875 Test Acc: 0.626953125\n",
      "Step 767 -- Train loss: 1.434550166130066, Train Acc: 0.626953125 Test Acc: 0.623046875\n",
      "Step 768 -- Train loss: 1.4315102100372314, Train Acc: 0.645263671875 Test Acc: 0.63671875\n",
      "Step 769 -- Train loss: 1.4336485862731934, Train Acc: 0.6640625 Test Acc: 0.681640625\n",
      "Step 770 -- Train loss: 1.4451054334640503, Train Acc: 0.655029296875 Test Acc: 0.642578125\n",
      "Step 771 -- Train loss: 1.4318575859069824, Train Acc: 0.649658203125 Test Acc: 0.642578125\n",
      "Step 772 -- Train loss: 1.4389744997024536, Train Acc: 0.6376953125 Test Acc: 0.6396484375\n",
      "Step 773 -- Train loss: 1.4349696636199951, Train Acc: 0.65185546875 Test Acc: 0.6044921875\n",
      "Step 774 -- Train loss: 1.4423182010650635, Train Acc: 0.63232421875 Test Acc: 0.6181640625\n",
      "Step 775 -- Train loss: 1.4357597827911377, Train Acc: 0.633056640625 Test Acc: 0.6376953125\n",
      "Step 776 -- Train loss: 1.4357861280441284, Train Acc: 0.6259765625 Test Acc: 0.6298828125\n",
      "Step 777 -- Train loss: 1.4346811771392822, Train Acc: 0.620361328125 Test Acc: 0.595703125\n",
      "Step 778 -- Train loss: 1.4310635328292847, Train Acc: 0.619140625 Test Acc: 0.6015625\n",
      "Step 779 -- Train loss: 1.4357749223709106, Train Acc: 0.607666015625 Test Acc: 0.6396484375\n",
      "Step 780 -- Train loss: 1.4442468881607056, Train Acc: 0.59423828125 Test Acc: 0.5888671875\n",
      "Step 781 -- Train loss: 1.4338390827178955, Train Acc: 0.603515625 Test Acc: 0.6240234375\n",
      "Step 782 -- Train loss: 1.4279032945632935, Train Acc: 0.6220703125 Test Acc: 0.6484375\n",
      "Step 783 -- Train loss: 1.4393410682678223, Train Acc: 0.621826171875 Test Acc: 0.611328125\n",
      "Step 784 -- Train loss: 1.4381108283996582, Train Acc: 0.61474609375 Test Acc: 0.63671875\n",
      "Step 785 -- Train loss: 1.4296787977218628, Train Acc: 0.61865234375 Test Acc: 0.6083984375\n",
      "Step 786 -- Train loss: 1.4340131282806396, Train Acc: 0.632080078125 Test Acc: 0.6435546875\n",
      "Step 787 -- Train loss: 1.4468799829483032, Train Acc: 0.613037109375 Test Acc: 0.6025390625\n",
      "Step 788 -- Train loss: 1.4347819089889526, Train Acc: 0.633544921875 Test Acc: 0.6259765625\n",
      "Step 789 -- Train loss: 1.4362921714782715, Train Acc: 0.641845703125 Test Acc: 0.65234375\n",
      "Step 790 -- Train loss: 1.4264154434204102, Train Acc: 0.646728515625 Test Acc: 0.640625\n",
      "Step 791 -- Train loss: 1.4320975542068481, Train Acc: 0.650146484375 Test Acc: 0.6123046875\n",
      "Step 792 -- Train loss: 1.4298756122589111, Train Acc: 0.657958984375 Test Acc: 0.6298828125\n",
      "Step 793 -- Train loss: 1.4322377443313599, Train Acc: 0.636474609375 Test Acc: 0.6650390625\n",
      "Step 794 -- Train loss: 1.4357331991195679, Train Acc: 0.635009765625 Test Acc: 0.640625\n",
      "Step 795 -- Train loss: 1.4329619407653809, Train Acc: 0.62841796875 Test Acc: 0.630859375\n",
      "Step 796 -- Train loss: 1.4214919805526733, Train Acc: 0.63525390625 Test Acc: 0.642578125\n",
      "Step 797 -- Train loss: 1.4308913946151733, Train Acc: 0.64404296875 Test Acc: 0.6015625\n",
      "Step 798 -- Train loss: 1.42794668674469, Train Acc: 0.646728515625 Test Acc: 0.6513671875\n",
      "Step 799 -- Train loss: 1.4266877174377441, Train Acc: 0.635009765625 Test Acc: 0.6357421875\n",
      "Step 800 -- Train loss: 1.4319355487823486, Train Acc: 0.635986328125 Test Acc: 0.6474609375\n",
      "Step 801 -- Train loss: 1.4259247779846191, Train Acc: 0.641845703125 Test Acc: 0.65625\n",
      "Step 802 -- Train loss: 1.4236799478530884, Train Acc: 0.632080078125 Test Acc: 0.62109375\n",
      "Step 803 -- Train loss: 1.4285578727722168, Train Acc: 0.636962890625 Test Acc: 0.642578125\n",
      "Step 804 -- Train loss: 1.4290142059326172, Train Acc: 0.619140625 Test Acc: 0.6630859375\n",
      "Step 805 -- Train loss: 1.4129559993743896, Train Acc: 0.636474609375 Test Acc: 0.6328125\n",
      "Step 806 -- Train loss: 1.4179530143737793, Train Acc: 0.64306640625 Test Acc: 0.658203125\n",
      "Step 807 -- Train loss: 1.4289758205413818, Train Acc: 0.651611328125 Test Acc: 0.6669921875\n",
      "Step 808 -- Train loss: 1.4285542964935303, Train Acc: 0.642578125 Test Acc: 0.6416015625\n",
      "Step 809 -- Train loss: 1.431677222251892, Train Acc: 0.66162109375 Test Acc: 0.650390625\n",
      "Step 810 -- Train loss: 1.4235111474990845, Train Acc: 0.622314453125 Test Acc: 0.65234375\n",
      "Step 811 -- Train loss: 1.4247894287109375, Train Acc: 0.650634765625 Test Acc: 0.654296875\n",
      "Step 812 -- Train loss: 1.4216097593307495, Train Acc: 0.629150390625 Test Acc: 0.6123046875\n",
      "Step 813 -- Train loss: 1.424641728401184, Train Acc: 0.644775390625 Test Acc: 0.6435546875\n",
      "Step 814 -- Train loss: 1.425845742225647, Train Acc: 0.634033203125 Test Acc: 0.6279296875\n",
      "Step 815 -- Train loss: 1.41489577293396, Train Acc: 0.66455078125 Test Acc: 0.6484375\n",
      "Step 816 -- Train loss: 1.4070265293121338, Train Acc: 0.653564453125 Test Acc: 0.640625\n",
      "Step 817 -- Train loss: 1.4289880990982056, Train Acc: 0.633544921875 Test Acc: 0.6474609375\n",
      "Step 818 -- Train loss: 1.4150108098983765, Train Acc: 0.646728515625 Test Acc: 0.6376953125\n",
      "Step 819 -- Train loss: 1.416394829750061, Train Acc: 0.62841796875 Test Acc: 0.638671875\n",
      "Step 820 -- Train loss: 1.41904878616333, Train Acc: 0.633544921875 Test Acc: 0.623046875\n",
      "Step 821 -- Train loss: 1.4111533164978027, Train Acc: 0.637451171875 Test Acc: 0.6435546875\n",
      "Step 822 -- Train loss: 1.4261434078216553, Train Acc: 0.6318359375 Test Acc: 0.6162109375\n",
      "Step 823 -- Train loss: 1.4260809421539307, Train Acc: 0.6298828125 Test Acc: 0.626953125\n",
      "Step 824 -- Train loss: 1.4234075546264648, Train Acc: 0.645751953125 Test Acc: 0.63671875\n",
      "Step 825 -- Train loss: 1.4134751558303833, Train Acc: 0.662353515625 Test Acc: 0.6650390625\n",
      "Step 826 -- Train loss: 1.416990041732788, Train Acc: 0.659423828125 Test Acc: 0.6826171875\n",
      "Step 827 -- Train loss: 1.4231338500976562, Train Acc: 0.66552734375 Test Acc: 0.6982421875\n",
      "Step 828 -- Train loss: 1.430248737335205, Train Acc: 0.664306640625 Test Acc: 0.666015625\n",
      "Step 829 -- Train loss: 1.41509211063385, Train Acc: 0.6435546875 Test Acc: 0.609375\n",
      "Step 830 -- Train loss: 1.4174354076385498, Train Acc: 0.627197265625 Test Acc: 0.5888671875\n",
      "Step 831 -- Train loss: 1.4023023843765259, Train Acc: 0.63427734375 Test Acc: 0.634765625\n",
      "Step 832 -- Train loss: 1.4183251857757568, Train Acc: 0.631591796875 Test Acc: 0.642578125\n",
      "Step 833 -- Train loss: 1.414910912513733, Train Acc: 0.63623046875 Test Acc: 0.6279296875\n",
      "Step 834 -- Train loss: 1.4163157939910889, Train Acc: 0.624267578125 Test Acc: 0.6083984375\n",
      "Step 835 -- Train loss: 1.4182466268539429, Train Acc: 0.63330078125 Test Acc: 0.6337890625\n",
      "Step 836 -- Train loss: 1.4212350845336914, Train Acc: 0.640380859375 Test Acc: 0.6328125\n",
      "Step 837 -- Train loss: 1.412243127822876, Train Acc: 0.65576171875 Test Acc: 0.6416015625\n",
      "Step 838 -- Train loss: 1.4089012145996094, Train Acc: 0.648193359375 Test Acc: 0.6630859375\n",
      "Step 839 -- Train loss: 1.4164167642593384, Train Acc: 0.660400390625 Test Acc: 0.65625\n",
      "Step 840 -- Train loss: 1.4180711507797241, Train Acc: 0.659912109375 Test Acc: 0.650390625\n",
      "Step 841 -- Train loss: 1.4320507049560547, Train Acc: 0.660400390625 Test Acc: 0.650390625\n",
      "Step 842 -- Train loss: 1.411700963973999, Train Acc: 0.666015625 Test Acc: 0.6806640625\n",
      "Step 843 -- Train loss: 1.4174830913543701, Train Acc: 0.66259765625 Test Acc: 0.6455078125\n",
      "Step 844 -- Train loss: 1.424261212348938, Train Acc: 0.6533203125 Test Acc: 0.6572265625\n",
      "Step 845 -- Train loss: 1.4131325483322144, Train Acc: 0.64794921875 Test Acc: 0.6484375\n",
      "Step 846 -- Train loss: 1.4113569259643555, Train Acc: 0.64013671875 Test Acc: 0.630859375\n",
      "Step 847 -- Train loss: 1.413332223892212, Train Acc: 0.64306640625 Test Acc: 0.640625\n",
      "Step 848 -- Train loss: 1.4180012941360474, Train Acc: 0.62060546875 Test Acc: 0.6240234375\n",
      "Step 849 -- Train loss: 1.4049017429351807, Train Acc: 0.64892578125 Test Acc: 0.6328125\n",
      "Step 850 -- Train loss: 1.4115113019943237, Train Acc: 0.639892578125 Test Acc: 0.640625\n",
      "Step 851 -- Train loss: 1.4150162935256958, Train Acc: 0.615966796875 Test Acc: 0.6455078125\n",
      "Step 852 -- Train loss: 1.401808738708496, Train Acc: 0.622802734375 Test Acc: 0.607421875\n",
      "Step 853 -- Train loss: 1.4030853509902954, Train Acc: 0.624755859375 Test Acc: 0.6044921875\n",
      "Step 854 -- Train loss: 1.4075815677642822, Train Acc: 0.633544921875 Test Acc: 0.65234375\n",
      "Step 855 -- Train loss: 1.4029656648635864, Train Acc: 0.64306640625 Test Acc: 0.6064453125\n",
      "Step 856 -- Train loss: 1.401963710784912, Train Acc: 0.641357421875 Test Acc: 0.6337890625\n",
      "Step 857 -- Train loss: 1.4090317487716675, Train Acc: 0.641357421875 Test Acc: 0.65625\n",
      "Step 858 -- Train loss: 1.4047753810882568, Train Acc: 0.648193359375 Test Acc: 0.6416015625\n",
      "Step 859 -- Train loss: 1.3965790271759033, Train Acc: 0.655029296875 Test Acc: 0.6669921875\n",
      "Step 860 -- Train loss: 1.4023109674453735, Train Acc: 0.652587890625 Test Acc: 0.6337890625\n",
      "Step 861 -- Train loss: 1.412172794342041, Train Acc: 0.6396484375 Test Acc: 0.626953125\n",
      "Step 862 -- Train loss: 1.4055137634277344, Train Acc: 0.64013671875 Test Acc: 0.642578125\n",
      "Step 863 -- Train loss: 1.408575415611267, Train Acc: 0.648193359375 Test Acc: 0.642578125\n",
      "Step 864 -- Train loss: 1.395164132118225, Train Acc: 0.6318359375 Test Acc: 0.6181640625\n",
      "Step 865 -- Train loss: 1.396582841873169, Train Acc: 0.64892578125 Test Acc: 0.630859375\n",
      "Step 866 -- Train loss: 1.4008641242980957, Train Acc: 0.65869140625 Test Acc: 0.68359375\n",
      "Step 867 -- Train loss: 1.404133677482605, Train Acc: 0.6474609375 Test Acc: 0.6220703125\n",
      "Step 868 -- Train loss: 1.4022696018218994, Train Acc: 0.643310546875 Test Acc: 0.6328125\n",
      "Step 869 -- Train loss: 1.3895351886749268, Train Acc: 0.658935546875 Test Acc: 0.65234375\n",
      "Step 870 -- Train loss: 1.3982857465744019, Train Acc: 0.645751953125 Test Acc: 0.6611328125\n",
      "Step 871 -- Train loss: 1.395978331565857, Train Acc: 0.649169921875 Test Acc: 0.65234375\n",
      "Step 872 -- Train loss: 1.4152789115905762, Train Acc: 0.64599609375 Test Acc: 0.6435546875\n",
      "Step 873 -- Train loss: 1.4354313611984253, Train Acc: 0.63427734375 Test Acc: 0.62109375\n",
      "Step 874 -- Train loss: 1.4298211336135864, Train Acc: 0.62841796875 Test Acc: 0.6416015625\n",
      "Step 875 -- Train loss: 1.4424777030944824, Train Acc: 0.626220703125 Test Acc: 0.630859375\n",
      "Step 876 -- Train loss: 1.438491940498352, Train Acc: 0.62548828125 Test Acc: 0.6357421875\n",
      "Step 877 -- Train loss: 1.4505990743637085, Train Acc: 0.6142578125 Test Acc: 0.6201171875\n",
      "Step 878 -- Train loss: 1.4407634735107422, Train Acc: 0.6337890625 Test Acc: 0.6416015625\n",
      "Step 879 -- Train loss: 1.4185669422149658, Train Acc: 0.662109375 Test Acc: 0.6611328125\n",
      "Step 880 -- Train loss: 1.4042987823486328, Train Acc: 0.6845703125 Test Acc: 0.6787109375\n",
      "Step 881 -- Train loss: 1.4065792560577393, Train Acc: 0.669677734375 Test Acc: 0.685546875\n",
      "Step 882 -- Train loss: 1.395917296409607, Train Acc: 0.6689453125 Test Acc: 0.650390625\n",
      "Step 883 -- Train loss: 1.407785415649414, Train Acc: 0.671630859375 Test Acc: 0.6640625\n",
      "Step 884 -- Train loss: 1.3941657543182373, Train Acc: 0.677734375 Test Acc: 0.681640625\n",
      "Step 885 -- Train loss: 1.416777491569519, Train Acc: 0.660400390625 Test Acc: 0.6494140625\n",
      "Step 886 -- Train loss: 1.4155749082565308, Train Acc: 0.63525390625 Test Acc: 0.638671875\n",
      "Step 887 -- Train loss: 1.4208011627197266, Train Acc: 0.625 Test Acc: 0.6318359375\n",
      "Step 888 -- Train loss: 1.4108580350875854, Train Acc: 0.638671875 Test Acc: 0.607421875\n",
      "Step 889 -- Train loss: 1.409259557723999, Train Acc: 0.620361328125 Test Acc: 0.6328125\n",
      "Step 890 -- Train loss: 1.40211021900177, Train Acc: 0.638671875 Test Acc: 0.6376953125\n",
      "Step 891 -- Train loss: 1.3969424962997437, Train Acc: 0.625244140625 Test Acc: 0.634765625\n",
      "Step 892 -- Train loss: 1.3990559577941895, Train Acc: 0.632568359375 Test Acc: 0.64453125\n",
      "Step 893 -- Train loss: 1.3958653211593628, Train Acc: 0.643798828125 Test Acc: 0.646484375\n",
      "Step 894 -- Train loss: 1.4060128927230835, Train Acc: 0.645751953125 Test Acc: 0.6630859375\n",
      "Step 895 -- Train loss: 1.3914976119995117, Train Acc: 0.665771484375 Test Acc: 0.63671875\n",
      "Step 896 -- Train loss: 1.4027047157287598, Train Acc: 0.650634765625 Test Acc: 0.65234375\n",
      "Step 897 -- Train loss: 1.3920036554336548, Train Acc: 0.663330078125 Test Acc: 0.65234375\n",
      "Step 898 -- Train loss: 1.3899860382080078, Train Acc: 0.66748046875 Test Acc: 0.66015625\n",
      "Step 899 -- Train loss: 1.3993079662322998, Train Acc: 0.66064453125 Test Acc: 0.69140625\n",
      "Step 900 -- Train loss: 1.3874882459640503, Train Acc: 0.666259765625 Test Acc: 0.6748046875\n",
      "Step 901 -- Train loss: 1.3942216634750366, Train Acc: 0.67333984375 Test Acc: 0.697265625\n",
      "Step 902 -- Train loss: 1.3769967555999756, Train Acc: 0.669921875 Test Acc: 0.6455078125\n",
      "Step 903 -- Train loss: 1.3948383331298828, Train Acc: 0.63427734375 Test Acc: 0.62890625\n",
      "Step 904 -- Train loss: 1.4038454294204712, Train Acc: 0.62646484375 Test Acc: 0.646484375\n",
      "Step 905 -- Train loss: 1.3917057514190674, Train Acc: 0.666015625 Test Acc: 0.6328125\n",
      "Step 906 -- Train loss: 1.399190902709961, Train Acc: 0.6494140625 Test Acc: 0.6591796875\n",
      "Step 907 -- Train loss: 1.3897587060928345, Train Acc: 0.6435546875 Test Acc: 0.6376953125\n",
      "Step 908 -- Train loss: 1.3983064889907837, Train Acc: 0.642333984375 Test Acc: 0.6591796875\n",
      "Step 909 -- Train loss: 1.3795135021209717, Train Acc: 0.670166015625 Test Acc: 0.6435546875\n",
      "Step 910 -- Train loss: 1.396003007888794, Train Acc: 0.646728515625 Test Acc: 0.6484375\n",
      "Step 911 -- Train loss: 1.391532301902771, Train Acc: 0.660888671875 Test Acc: 0.6767578125\n",
      "Step 912 -- Train loss: 1.3914161920547485, Train Acc: 0.659912109375 Test Acc: 0.669921875\n",
      "Step 913 -- Train loss: 1.3892542123794556, Train Acc: 0.655517578125 Test Acc: 0.6494140625\n",
      "Step 914 -- Train loss: 1.386345386505127, Train Acc: 0.65478515625 Test Acc: 0.623046875\n",
      "Step 915 -- Train loss: 1.3830184936523438, Train Acc: 0.654296875 Test Acc: 0.66015625\n",
      "Step 916 -- Train loss: 1.3829292058944702, Train Acc: 0.6416015625 Test Acc: 0.6455078125\n",
      "Step 917 -- Train loss: 1.3819923400878906, Train Acc: 0.650634765625 Test Acc: 0.6416015625\n",
      "Step 918 -- Train loss: 1.387563705444336, Train Acc: 0.662841796875 Test Acc: 0.681640625\n",
      "Step 919 -- Train loss: 1.3880196809768677, Train Acc: 0.6572265625 Test Acc: 0.6484375\n",
      "Step 920 -- Train loss: 1.3949450254440308, Train Acc: 0.64404296875 Test Acc: 0.6455078125\n",
      "Step 921 -- Train loss: 1.3787612915039062, Train Acc: 0.664794921875 Test Acc: 0.689453125\n",
      "Step 922 -- Train loss: 1.3876416683197021, Train Acc: 0.654052734375 Test Acc: 0.6435546875\n",
      "Step 923 -- Train loss: 1.3760427236557007, Train Acc: 0.6474609375 Test Acc: 0.646484375\n",
      "Step 924 -- Train loss: 1.3690712451934814, Train Acc: 0.666015625 Test Acc: 0.6630859375\n",
      "Step 925 -- Train loss: 1.3763998746871948, Train Acc: 0.67724609375 Test Acc: 0.6884765625\n",
      "Step 926 -- Train loss: 1.3815590143203735, Train Acc: 0.655517578125 Test Acc: 0.6728515625\n",
      "Step 927 -- Train loss: 1.3788701295852661, Train Acc: 0.670166015625 Test Acc: 0.6494140625\n",
      "Step 928 -- Train loss: 1.3799015283584595, Train Acc: 0.668701171875 Test Acc: 0.6591796875\n",
      "Step 929 -- Train loss: 1.3827204704284668, Train Acc: 0.665771484375 Test Acc: 0.64453125\n",
      "Step 930 -- Train loss: 1.368656873703003, Train Acc: 0.640625 Test Acc: 0.6484375\n",
      "Step 931 -- Train loss: 1.3759748935699463, Train Acc: 0.637939453125 Test Acc: 0.64453125\n",
      "Step 932 -- Train loss: 1.3806771039962769, Train Acc: 0.635009765625 Test Acc: 0.6630859375\n",
      "Step 933 -- Train loss: 1.3816883563995361, Train Acc: 0.646484375 Test Acc: 0.6494140625\n",
      "Step 934 -- Train loss: 1.376598596572876, Train Acc: 0.653564453125 Test Acc: 0.66015625\n",
      "Step 935 -- Train loss: 1.3762775659561157, Train Acc: 0.647216796875 Test Acc: 0.6630859375\n",
      "Step 936 -- Train loss: 1.3770298957824707, Train Acc: 0.649658203125 Test Acc: 0.611328125\n",
      "Step 937 -- Train loss: 1.3812311887741089, Train Acc: 0.633544921875 Test Acc: 0.646484375\n",
      "Step 938 -- Train loss: 1.3697360754013062, Train Acc: 0.635498046875 Test Acc: 0.6259765625\n",
      "Step 939 -- Train loss: 1.379469394683838, Train Acc: 0.628173828125 Test Acc: 0.6064453125\n",
      "Step 940 -- Train loss: 1.3754465579986572, Train Acc: 0.6357421875 Test Acc: 0.65234375\n",
      "Step 941 -- Train loss: 1.3655072450637817, Train Acc: 0.662109375 Test Acc: 0.6435546875\n",
      "Step 942 -- Train loss: 1.3884941339492798, Train Acc: 0.65673828125 Test Acc: 0.6669921875\n",
      "Step 943 -- Train loss: 1.3630975484848022, Train Acc: 0.66650390625 Test Acc: 0.666015625\n",
      "Step 944 -- Train loss: 1.3852083683013916, Train Acc: 0.6611328125 Test Acc: 0.65625\n",
      "Step 945 -- Train loss: 1.3709933757781982, Train Acc: 0.665771484375 Test Acc: 0.67578125\n",
      "Step 946 -- Train loss: 1.3670659065246582, Train Acc: 0.6796875 Test Acc: 0.6611328125\n",
      "Step 947 -- Train loss: 1.3623288869857788, Train Acc: 0.659423828125 Test Acc: 0.6494140625\n",
      "Step 948 -- Train loss: 1.3592816591262817, Train Acc: 0.6572265625 Test Acc: 0.6611328125\n",
      "Step 949 -- Train loss: 1.3638124465942383, Train Acc: 0.679931640625 Test Acc: 0.689453125\n",
      "Step 950 -- Train loss: 1.3736391067504883, Train Acc: 0.693115234375 Test Acc: 0.7001953125\n",
      "Step 951 -- Train loss: 1.3664225339889526, Train Acc: 0.682861328125 Test Acc: 0.68359375\n",
      "Step 952 -- Train loss: 1.3764457702636719, Train Acc: 0.680908203125 Test Acc: 0.685546875\n",
      "Step 953 -- Train loss: 1.368252158164978, Train Acc: 0.676025390625 Test Acc: 0.6708984375\n",
      "Step 954 -- Train loss: 1.363391399383545, Train Acc: 0.6982421875 Test Acc: 0.6640625\n",
      "Step 955 -- Train loss: 1.355758786201477, Train Acc: 0.691162109375 Test Acc: 0.677734375\n",
      "Step 956 -- Train loss: 1.3555660247802734, Train Acc: 0.6767578125 Test Acc: 0.669921875\n",
      "Step 957 -- Train loss: 1.3663928508758545, Train Acc: 0.678955078125 Test Acc: 0.7080078125\n",
      "Step 958 -- Train loss: 1.3711369037628174, Train Acc: 0.671875 Test Acc: 0.6591796875\n",
      "Step 959 -- Train loss: 1.3781403303146362, Train Acc: 0.654052734375 Test Acc: 0.6611328125\n",
      "Step 960 -- Train loss: 1.3672128915786743, Train Acc: 0.658447265625 Test Acc: 0.66796875\n",
      "Step 961 -- Train loss: 1.3667242527008057, Train Acc: 0.67919921875 Test Acc: 0.671875\n",
      "Step 962 -- Train loss: 1.3619999885559082, Train Acc: 0.67431640625 Test Acc: 0.669921875\n",
      "Step 963 -- Train loss: 1.363888144493103, Train Acc: 0.66748046875 Test Acc: 0.6650390625\n",
      "Step 964 -- Train loss: 1.3670634031295776, Train Acc: 0.6728515625 Test Acc: 0.65234375\n",
      "Step 965 -- Train loss: 1.371222734451294, Train Acc: 0.673095703125 Test Acc: 0.6787109375\n",
      "Step 966 -- Train loss: 1.3715928792953491, Train Acc: 0.681884765625 Test Acc: 0.6884765625\n",
      "Step 967 -- Train loss: 1.3633079528808594, Train Acc: 0.68798828125 Test Acc: 0.6689453125\n",
      "Step 968 -- Train loss: 1.3720475435256958, Train Acc: 0.68310546875 Test Acc: 0.69921875\n",
      "Step 969 -- Train loss: 1.3715876340866089, Train Acc: 0.68505859375 Test Acc: 0.658203125\n",
      "Step 970 -- Train loss: 1.366459608078003, Train Acc: 0.684326171875 Test Acc: 0.712890625\n",
      "Step 971 -- Train loss: 1.3750019073486328, Train Acc: 0.674560546875 Test Acc: 0.69140625\n",
      "Step 972 -- Train loss: 1.3662775754928589, Train Acc: 0.68798828125 Test Acc: 0.6865234375\n",
      "Step 973 -- Train loss: 1.3623502254486084, Train Acc: 0.658935546875 Test Acc: 0.6796875\n",
      "Step 974 -- Train loss: 1.369084358215332, Train Acc: 0.638916015625 Test Acc: 0.6640625\n",
      "Step 975 -- Train loss: 1.35927414894104, Train Acc: 0.635498046875 Test Acc: 0.658203125\n",
      "Step 976 -- Train loss: 1.3704408407211304, Train Acc: 0.66650390625 Test Acc: 0.6826171875\n",
      "Step 977 -- Train loss: 1.357041835784912, Train Acc: 0.669189453125 Test Acc: 0.7099609375\n",
      "Step 978 -- Train loss: 1.3556289672851562, Train Acc: 0.685302734375 Test Acc: 0.6455078125\n",
      "Step 979 -- Train loss: 1.3668354749679565, Train Acc: 0.685302734375 Test Acc: 0.708984375\n",
      "Step 980 -- Train loss: 1.363370418548584, Train Acc: 0.706787109375 Test Acc: 0.70703125\n",
      "Step 981 -- Train loss: 1.3581284284591675, Train Acc: 0.70263671875 Test Acc: 0.685546875\n",
      "Step 982 -- Train loss: 1.3603609800338745, Train Acc: 0.684326171875 Test Acc: 0.6962890625\n",
      "Step 983 -- Train loss: 1.3569964170455933, Train Acc: 0.67626953125 Test Acc: 0.6962890625\n",
      "Step 984 -- Train loss: 1.3586739301681519, Train Acc: 0.672607421875 Test Acc: 0.6533203125\n",
      "Step 985 -- Train loss: 1.3565562963485718, Train Acc: 0.665283203125 Test Acc: 0.6640625\n",
      "Step 986 -- Train loss: 1.3546624183654785, Train Acc: 0.646484375 Test Acc: 0.61328125\n",
      "Step 987 -- Train loss: 1.3606258630752563, Train Acc: 0.63623046875 Test Acc: 0.640625\n",
      "Step 988 -- Train loss: 1.360267996788025, Train Acc: 0.62890625 Test Acc: 0.6328125\n",
      "Step 989 -- Train loss: 1.3581618070602417, Train Acc: 0.6396484375 Test Acc: 0.66015625\n",
      "Step 990 -- Train loss: 1.3649561405181885, Train Acc: 0.626953125 Test Acc: 0.63671875\n",
      "Step 991 -- Train loss: 1.366150975227356, Train Acc: 0.641845703125 Test Acc: 0.625\n",
      "Step 992 -- Train loss: 1.3612252473831177, Train Acc: 0.641845703125 Test Acc: 0.6455078125\n",
      "Step 993 -- Train loss: 1.3608163595199585, Train Acc: 0.6513671875 Test Acc: 0.6494140625\n",
      "Step 994 -- Train loss: 1.3556424379348755, Train Acc: 0.678466796875 Test Acc: 0.6728515625\n",
      "Step 995 -- Train loss: 1.356687307357788, Train Acc: 0.67138671875 Test Acc: 0.6552734375\n",
      "Step 996 -- Train loss: 1.3553811311721802, Train Acc: 0.690673828125 Test Acc: 0.65625\n",
      "Step 997 -- Train loss: 1.3463741540908813, Train Acc: 0.70361328125 Test Acc: 0.689453125\n",
      "Step 998 -- Train loss: 1.3471763134002686, Train Acc: 0.67626953125 Test Acc: 0.685546875\n",
      "Step 999 -- Train loss: 1.3403900861740112, Train Acc: 0.695556640625 Test Acc: 0.66015625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>att_prog_measure</td><td></td></tr><tr><td>data_repeat_frac</td><td></td></tr><tr><td>idx0_check</td><td></td></tr><tr><td>idx10_check</td><td></td></tr><tr><td>idx11_check</td><td></td></tr><tr><td>idx12_check</td><td></td></tr><tr><td>idx13_check</td><td></td></tr><tr><td>idx14_check</td><td></td></tr><tr><td>idx15_check</td><td></td></tr><tr><td>idx1_check</td><td></td></tr><tr><td>idx2_check</td><td></td></tr><tr><td>idx3_check</td><td></td></tr><tr><td>idx4_check</td><td></td></tr><tr><td>idx5_check</td><td></td></tr><tr><td>idx6_check</td><td></td></tr><tr><td>idx7_check</td><td></td></tr><tr><td>idx8_check</td><td></td></tr><tr><td>idx9_check</td><td></td></tr><tr><td>mean_cosine_sim</td><td></td></tr><tr><td>mean_cosine_sim_0</td><td></td></tr><tr><td>mean_cosine_sim_1</td><td></td></tr><tr><td>mean_cosine_sim_10</td><td></td></tr><tr><td>mean_cosine_sim_11</td><td></td></tr><tr><td>mean_cosine_sim_12</td><td></td></tr><tr><td>mean_cosine_sim_13</td><td></td></tr><tr><td>mean_cosine_sim_14</td><td></td></tr><tr><td>mean_cosine_sim_2</td><td></td></tr><tr><td>mean_cosine_sim_3</td><td></td></tr><tr><td>mean_cosine_sim_4</td><td></td></tr><tr><td>mean_cosine_sim_5</td><td></td></tr><tr><td>mean_cosine_sim_6</td><td></td></tr><tr><td>mean_cosine_sim_7</td><td></td></tr><tr><td>mean_cosine_sim_8</td><td></td></tr><tr><td>mean_cosine_sim_9</td><td></td></tr><tr><td>model_repeat_frac</td><td></td></tr><tr><td>test_acc</td><td></td></tr><tr><td>train_acc</td><td></td></tr><tr><td>train_loss</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>att_prog_measure</td><td>0.80747</td></tr><tr><td>data_repeat_frac</td><td>0.05625</td></tr><tr><td>idx0_check</td><td>1.0</td></tr><tr><td>idx10_check</td><td>0.74219</td></tr><tr><td>idx11_check</td><td>0.67578</td></tr><tr><td>idx12_check</td><td>0.67969</td></tr><tr><td>idx13_check</td><td>0.58984</td></tr><tr><td>idx14_check</td><td>0.72266</td></tr><tr><td>idx15_check</td><td>0.73828</td></tr><tr><td>idx1_check</td><td>0.62109</td></tr><tr><td>idx2_check</td><td>0.66406</td></tr><tr><td>idx3_check</td><td>0.74609</td></tr><tr><td>idx4_check</td><td>0.67578</td></tr><tr><td>idx5_check</td><td>0.67578</td></tr><tr><td>idx6_check</td><td>0.53906</td></tr><tr><td>idx7_check</td><td>0.63672</td></tr><tr><td>idx8_check</td><td>0.73047</td></tr><tr><td>idx9_check</td><td>0.69141</td></tr><tr><td>mean_cosine_sim</td><td>0.25149</td></tr><tr><td>mean_cosine_sim_0</td><td>-0.01365</td></tr><tr><td>mean_cosine_sim_1</td><td>0.22062</td></tr><tr><td>mean_cosine_sim_10</td><td>0.28713</td></tr><tr><td>mean_cosine_sim_11</td><td>0.30531</td></tr><tr><td>mean_cosine_sim_12</td><td>0.31241</td></tr><tr><td>mean_cosine_sim_13</td><td>0.35851</td></tr><tr><td>mean_cosine_sim_14</td><td>0.47508</td></tr><tr><td>mean_cosine_sim_2</td><td>0.21585</td></tr><tr><td>mean_cosine_sim_3</td><td>0.23565</td></tr><tr><td>mean_cosine_sim_4</td><td>0.22939</td></tr><tr><td>mean_cosine_sim_5</td><td>0.23957</td></tr><tr><td>mean_cosine_sim_6</td><td>0.24669</td></tr><tr><td>mean_cosine_sim_7</td><td>0.24949</td></tr><tr><td>mean_cosine_sim_8</td><td>0.29463</td></tr><tr><td>mean_cosine_sim_9</td><td>0.2833</td></tr><tr><td>model_repeat_frac</td><td>0.08229</td></tr><tr><td>test_acc</td><td>0.66016</td></tr><tr><td>train_acc</td><td>0.69556</td></tr><tr><td>train_loss</td><td>1.34039</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">mws_linear_noMLP_noMod_1000steps</strong> at: <a href='https://wandb.ai/wth_ucsd/loss_plateau_tf/runs/qsc08oxb' target=\"_blank\">https://wandb.ai/wth_ucsd/loss_plateau_tf/runs/qsc08oxb</a><br/>Synced 6 W&B file(s), 1000 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251104_002438-qsc08oxb/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "config = DotMap(config)\n",
    "\n",
    "config.model.vocab_size = 2 * (max(config.data.p, config.data.max_num) + 1)\n",
    "config.model.block_size = 2 * config.data.num_tokens + 1\n",
    "config.train.num_steps = 1000\n",
    "\n",
    "data_sampler = MovingWindowSumNoMod(\n",
    "    min_num=config.data.min_num,\n",
    "    max_num=config.data.max_num,\n",
    "    k=config.data.k,\n",
    "    p=config.data.p,\n",
    "    device=device\n",
    ")\n",
    "model = GPTLinear(config.model, return_att=True).to(device)\n",
    "\n",
    "# ## Freeze embedding layer weights\n",
    "# for param in model.transformer.wte.parameters():\n",
    "#     param.requires_grad = False\n",
    "# for param in model.transformer.wpe.parameters():\n",
    "#     param.requires_grad = False\n",
    "\n",
    "# Make sure optimizer only updates trainable parameters\n",
    "optim = Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=config.train.lr)\n",
    "\n",
    "if config.train.wandb:\n",
    "    wandb_run_name = f'mws_linear_noMLP_noMod_{config.train.num_steps}steps'\n",
    "    wandb.login(key=\"\")\n",
    "    wandb.init(project=\"loss_plateau_tf\", name=wandb_run_name, config=config)\n",
    "    wandb.watch(model)\n",
    "\n",
    "for step in range(config.train.num_steps):\n",
    "    train_step(\n",
    "        model=model,\n",
    "        optim=optim,\n",
    "        data_sampler=data_sampler,\n",
    "        step=step,\n",
    "        config=config,\n",
    "    )\n",
    "\n",
    "if config.train.wandb:\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
