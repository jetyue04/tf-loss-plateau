{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "# import yaml\n",
    "import argparse\n",
    "from dotmap import DotMap\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.optim import Adam\n",
    "from torch.nn.functional import cosine_similarity\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import wandb\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"./src\")  # make sure Python can find src/\n",
    "# from model_softmax import GPTSoftmax\n",
    "from data import MovingWindowSum\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class NewGELU(nn.Module):\n",
    "    \"\"\"\n",
    "    Implementation of the GELU activation function currently in Google BERT repo (identical to OpenAI GPT).\n",
    "    Reference: Gaussian Error Linear Units (GELU) paper: https://arxiv.org/abs/1606.08415\n",
    "    \"\"\"\n",
    "\n",
    "    def forward(self, x):\n",
    "        return (\n",
    "            0.5\n",
    "            * x\n",
    "            * (\n",
    "                1.0\n",
    "                + torch.tanh(\n",
    "                    math.sqrt(2.0 / math.pi) * (x + 0.044715 * torch.pow(x, 3.0))\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    A vanilla multi-head masked self-attention layer with a projection at the end.\n",
    "    It is possible to use torch.nn.MultiheadAttention here but I am including an\n",
    "    explicit implementation here to show that there is nothing too scary here.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config, return_att=False):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "\n",
    "        # key, query, value projections for all heads\n",
    "        self.k = nn.Linear(config.n_embd, config.n_embd)\n",
    "        self.q = nn.Linear(config.n_embd, config.n_embd)\n",
    "        self.v = nn.Linear(config.n_embd, config.n_embd)\n",
    "        \n",
    "        # output projection\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
    "\n",
    "        # causal mask to ensure that attention is only applied to the left in the input sequence\n",
    "        self.register_buffer(\n",
    "            \"bias\",\n",
    "            torch.tril(torch.ones(config.block_size, config.block_size)).view(\n",
    "                1, 1, config.block_size, config.block_size\n",
    "            ),\n",
    "        )\n",
    "        \n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        self.return_att = return_att\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = (\n",
    "            x.size()\n",
    "        )  # batch size, sequence length, embedding dimensionality (n_embd)\n",
    "\n",
    "        # calculate query, key, values for all heads\n",
    "        q = self.q(x)\n",
    "        k = self.k(x)\n",
    "        v = self.v(x)\n",
    "\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(\n",
    "            1, 2\n",
    "        )  # (B, nh, T, hs)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(\n",
    "            1, 2\n",
    "        )  # (B, nh, T, hs)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(\n",
    "            1, 2\n",
    "        )  # (B, nh, T, hs)\n",
    "\n",
    "        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
    "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "        att = att.masked_fill(self.bias[:, :, :T, :T] == 0, float('-inf'))\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        \n",
    "        att_copy = att.clone().detach()\n",
    "\n",
    "        y = att @ v  # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "        y = (\n",
    "            y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        )  # re-assemble all head outputs side by side\n",
    "\n",
    "        # output projection\n",
    "        y = self.c_proj(y)\n",
    "\n",
    "        if self.return_att:\n",
    "            return y, att_copy\n",
    "\n",
    "        return y\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\"an unassuming Transformer block\"\"\"\n",
    "\n",
    "    def __init__(self, config, return_att=False):\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
    "        self.attn = CausalSelfAttention(config, return_att=return_att)\n",
    "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
    "        self.mlp = nn.ModuleDict(\n",
    "            dict(\n",
    "                c_fc=nn.Linear(config.n_embd, 4 * config.n_embd),\n",
    "                c_proj=nn.Linear(4 * config.n_embd, config.n_embd),\n",
    "                act=NewGELU(),\n",
    "                # dropout=nn.Dropout(config.resid_pdrop),\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        m = self.mlp\n",
    "        self.mlpf = lambda x: m.c_proj(m.act(m.c_fc(x)))  # MLP forward\n",
    "        self.return_att = return_att\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.return_att:\n",
    "            x_prev, att = self.attn(self.ln_1(x))\n",
    "            x = x + x_prev\n",
    "            \n",
    "            x = x + self.mlpf(self.ln_2(x))\n",
    "            \n",
    "            return x, att\n",
    "\n",
    "        else:\n",
    "            x = x + self.attn(self.ln_1(x))\n",
    "            x = x + self.mlpf(self.ln_2(x))\n",
    "            return x\n",
    "\n",
    "\n",
    "class GPTSoftmax(nn.Module):\n",
    "    \"\"\"GPT Language Model\"\"\"\n",
    "\n",
    "    def __init__(self, config, return_att=False):\n",
    "        super().__init__()\n",
    "        assert config.vocab_size is not None\n",
    "        assert config.block_size is not None\n",
    "        self.block_size = config.block_size\n",
    "        self.return_att = return_att\n",
    "\n",
    "        self.transformer = nn.ModuleDict(\n",
    "            dict(\n",
    "                wte=nn.Embedding(config.vocab_size, config.n_embd),\n",
    "                wpe=nn.Embedding(config.block_size, config.n_embd),\n",
    "                h=nn.ModuleList(\n",
    "                    [\n",
    "                        Block(config, return_att=self.return_att)\n",
    "                        for _ in range(config.n_layer)\n",
    "                    ]\n",
    "                ),\n",
    "                ln_f=nn.LayerNorm(config.n_embd),\n",
    "            )\n",
    "        )\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "\n",
    "        # init all weights, and apply a special scaled init to the residual projections, per GPT-2 paper\n",
    "        self.apply(self._init_weights)\n",
    "        for pn, p in self.named_parameters():\n",
    "            if pn.endswith(\"c_proj.weight\"):\n",
    "                torch.nn.init.normal_(\n",
    "                    p, mean=0.0, std=0.02 / math.sqrt(2 * config.n_layer)\n",
    "                )\n",
    "\n",
    "        # report number of parameters (note we don't count the decoder parameters in lm_head)\n",
    "        n_params = sum(p.numel() for p in self.transformer.parameters())\n",
    "        # print(\"number of parameters: %.2fM\" % (n_params / 1e6,))\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            torch.nn.init.zeros_(module.bias)\n",
    "            torch.nn.init.ones_(module.weight)\n",
    "\n",
    "\n",
    "    # Only used for weight decay experiments -------------------------------------------\n",
    "    def configure_optimizers(self, train_config):\n",
    "        \"\"\"\n",
    "        This long function is unfortunately doing something very simple and is being very defensive:\n",
    "        We are separating out all parameters of the model into two buckets: those that will experience\n",
    "        weight decay for regularization and those that won't (biases, and layernorm/embedding weights).\n",
    "        We are then returning the PyTorch optimizer object.\n",
    "        \"\"\"\n",
    "\n",
    "        # separate out all parameters to those that will and won't experience regularizing weight decay\n",
    "        decay = set()\n",
    "        no_decay = set()\n",
    "        whitelist_weight_modules = (torch.nn.Linear, )\n",
    "        blacklist_weight_modules = (torch.nn.LayerNorm, torch.nn.Embedding)\n",
    "        for mn, m in self.named_modules():\n",
    "            for pn, p in m.named_parameters():\n",
    "                fpn = '%s.%s' % (mn, pn) if mn else pn # full param name\n",
    "                # random note: because named_modules and named_parameters are recursive\n",
    "                # we will see the same tensors p many many times. but doing it this way\n",
    "                # allows us to know which parent module any tensor p belongs to...\n",
    "                if pn.endswith('bias'):\n",
    "                    # all biases will not be decayed\n",
    "                    no_decay.add(fpn)\n",
    "                elif pn.endswith('weight') and isinstance(m, whitelist_weight_modules):\n",
    "                    # weights of whitelist modules will be weight decayed\n",
    "                    decay.add(fpn)\n",
    "                elif pn.endswith('weight') and isinstance(m, blacklist_weight_modules):\n",
    "                    # weights of blacklist modules will NOT be weight decayed\n",
    "                    no_decay.add(fpn)\n",
    "\n",
    "        # validate that we considered every parameter\n",
    "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
    "        inter_params = decay & no_decay\n",
    "        union_params = decay | no_decay\n",
    "        assert len(inter_params) == 0, \"parameters %s made it into both decay/no_decay sets!\" % (str(inter_params), )\n",
    "        assert len(param_dict.keys() - union_params) == 0, \"parameters %s were not separated into either decay/no_decay set!\" \\\n",
    "                                                    % (str(param_dict.keys() - union_params), )\n",
    "\n",
    "        # create the pytorch optimizer object\n",
    "        optim_groups = [\n",
    "            {\"params\": [param_dict[pn] for pn in sorted(list(decay))], \"weight_decay\": train_config.wd},\n",
    "            {\"params\": [param_dict[pn] for pn in sorted(list(no_decay))], \"weight_decay\": 0.0},\n",
    "        ]\n",
    "        optimizer = torch.optim.AdamW(optim_groups, lr=train_config.lr)\n",
    "        return optimizer\n",
    "    # -------------------------------------------\n",
    "\n",
    "\n",
    "    def forward(self, idx, prompt_len, targets=None, mask_idx =None):\n",
    "        device = idx.device\n",
    "        b, t = idx.size()\n",
    "        assert (\n",
    "            t <= self.block_size\n",
    "        ), f\"Cannot forward sequence of length {t}, block size is only {self.block_size}\"\n",
    "        pos = torch.arange(0, t, dtype=torch.long, device=device).unsqueeze(\n",
    "            0\n",
    "        )  # shape (1, t)\n",
    "\n",
    "        # forward the GPT model itself\n",
    "        tok_emb = self.transformer.wte(idx)  # token embeddings of shape (b, t, n_embd)\n",
    "        pos_emb = self.transformer.wpe(pos)  # position embeddings of shape (1, t, n_embd)\n",
    "        x = tok_emb + pos_emb\n",
    "        \n",
    "        for block in self.transformer.h:\n",
    "            if self.return_att:\n",
    "                x, attn_map = block(x)\n",
    "            else:\n",
    "                x = block(x)\n",
    "\n",
    "        \n",
    "        x = self.transformer.ln_f(x)\n",
    "        \n",
    "        # Track residual state before LM head for representation collapse\n",
    "        pre_lm_h = x.clone().detach()\n",
    "        \n",
    "        # Final logits\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        # if we are given some desired targets also calculate the loss\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            # Masking input tokens\n",
    "            targets_masked = targets.clone()\n",
    "            targets_masked[:, :prompt_len-1] = -1\n",
    "            if mask_idx:\n",
    "                targets_masked[:, prompt_len+mask_idx:] = -1\n",
    "            loss = F.cross_entropy(\n",
    "                logits.reshape(-1, logits.size(-1)),\n",
    "                targets_masked.reshape(-1),\n",
    "                # targets.reshape(-1),\n",
    "                ignore_index=-1,\n",
    "            )\n",
    "\n",
    "        if self.return_att:\n",
    "            return attn_map, pre_lm_h, logits, loss\n",
    "        \n",
    "        return pre_lm_h, logits, loss\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def generate(\n",
    "        self, idx, max_new_tokens, prompt_len, temperature=1.0, do_sample=False, top_k=None, mask_idx= None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Take a conditioning sequence of indices idx (LongTensor of shape (b,t)) and complete\n",
    "        the sequence max_new_tokens times, feeding the predictions back into the model each time.\n",
    "        Most likely you'll want to make sure to be in model.eval() mode of operation for this.\n",
    "        \"\"\"\n",
    "        \n",
    "        for _ in range(max_new_tokens):\n",
    "            # if the sequence context is growing too long we must crop it at block_size\n",
    "            idx_cond = (\n",
    "                idx if idx.size(1) <= self.block_size else idx[:, -self.block_size :]\n",
    "            )\n",
    "            # forward the model to get the logits for the index in the sequence\n",
    "            _, _, logits, _ = self(idx_cond, prompt_len=prompt_len, mask_idx= mask_idx)\n",
    "            # pluck the logits at the final step and scale by desired temperature\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            # optionally crop the logits to only the top k options\n",
    "            if top_k is not None:\n",
    "                v, _ = torch.topk(logits, top_k)\n",
    "                logits[logits < v[:, [-1]]] = -float(\"Inf\")\n",
    "            # apply softmax to convert logits to (normalized) probabilities\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            # either sample from the distribution or take the most likely element\n",
    "            if do_sample:\n",
    "                idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            else:\n",
    "                _, idx_next = torch.topk(probs, k=1, dim=-1)\n",
    "            # append sampled index to the running sequence and continue\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "\n",
    "        return idx\n",
    "\n",
    "\n",
    "\n",
    "# Data\n",
    "\n",
    "class MovingWindowSum:\n",
    "    def __init__(self, min_num=1, max_num=16, k=2, p=17, sep=17, device=\"cuda\"):\n",
    "        self.min_num = min_num\n",
    "        self.max_num = max_num\n",
    "        self.k = k\n",
    "        self.p = p\n",
    "        self.sep = sep\n",
    "        self.device = device\n",
    "        assert self.p > self.max_num\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def sample(\n",
    "        self,\n",
    "        num_samples,\n",
    "        num_tokens,\n",
    "    ):\n",
    "        random_ints = torch.randint(\n",
    "            low=self.min_num, high=self.max_num + 1, size=(num_samples, num_tokens)\n",
    "        ).to(self.device)\n",
    "\n",
    "        random_ints_np = random_ints.detach().cpu().numpy()\n",
    "        convolution = torch.stack(\n",
    "            [\n",
    "                torch.from_numpy(\n",
    "                    np.convolve(\n",
    "                        random_ints_np[i],\n",
    "                        np.ones(self.k),\n",
    "                        mode=\"valid\",\n",
    "                    )\n",
    "                )\n",
    "                for i in range(random_ints.shape[0])\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        moving_sum = random_ints.clone().detach()\n",
    "        moving_sum[:, self.k - 1 :] = convolution\n",
    "\n",
    "        # for i in range(num_samples):\n",
    "        #     for j in range(0, self.k - 1):\n",
    "        #         if moving_sum[i, j] != random_ints[i, j]:\n",
    "        #             print(f\"ERROR! {i} {j}\")\n",
    "        #     for j in range(self.k - 1, num_tokens):\n",
    "        #         if moving_sum[i, j] != torch.sum(random_ints[i, j-self.k+1:j+1]):\n",
    "        #             print(f\"ERROR! {i} {j}\")\n",
    "\n",
    "        # exit()\n",
    "        samples = (\n",
    "            torch.cat(\n",
    "                [\n",
    "                    random_ints,\n",
    "                    self.sep * torch.ones(size=(num_samples, 1)).to(self.device),\n",
    "                    torch.remainder(input=moving_sum, other=self.p),\n",
    "                ],\n",
    "                axis=-1,\n",
    "            )\n",
    "            .to(int)\n",
    "            .detach()\n",
    "        )\n",
    "\n",
    "        return samples\n",
    "\n",
    "# Train\n",
    "def train_step(\n",
    "    model,\n",
    "    optim,\n",
    "    data_sampler,\n",
    "    step,\n",
    "    config,\n",
    "    mask_idx = None\n",
    "):\n",
    "    n_train, n_test, num_tokens = (\n",
    "        config.data.n_train,\n",
    "        config.data.n_test,\n",
    "        config.data.num_tokens,\n",
    "    )\n",
    "\n",
    "    data = data_sampler.sample(\n",
    "        num_samples=n_train + n_test,\n",
    "        num_tokens=num_tokens,\n",
    "    )\n",
    "\n",
    "    train_data = data[:n_train, :]\n",
    "    test_data = data[n_train:, :]\n",
    "\n",
    "    prompt_len = num_tokens + 1\n",
    "    gen_len = num_tokens\n",
    "    acc_start = num_tokens + 1\n",
    "\n",
    "    model.train()\n",
    "    optim.zero_grad(set_to_none=True)\n",
    "\n",
    "    targets = train_data[:, 1:].clone().long()\n",
    "    # targets[:, :prompt_len-1] = -1\n",
    "    # if mask_idx: ## Mask idx will correspond to the y_maskidx and onwards being masked (0 indexed)\n",
    "    #     targets[:, (prompt_len + mask_idx):] = -1\n",
    "        \n",
    "    _, _, _, loss = model(\n",
    "        train_data[:, :-1], targets=targets, prompt_len =prompt_len, mask_idx=mask_idx,\n",
    "    )\n",
    "    loss.backward()\n",
    "\n",
    "    if config.train.grad_clip > 0:\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), config.train.grad_clip)\n",
    "\n",
    "    optim.step()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Log train loss, train / test acc, repetition frequency\n",
    "        attn_map, pre_lm_h, _, train_loss = model(train_data[:, :-1], targets=train_data[:, 1:], prompt_len =prompt_len, mask_idx= mask_idx)\n",
    "\n",
    "        train_pred = model.generate(\n",
    "            idx=train_data[:, :prompt_len],\n",
    "            max_new_tokens=gen_len,\n",
    "            prompt_len =prompt_len,\n",
    "            mask_idx= mask_idx,\n",
    "        )\n",
    "        test_pred = model.generate(\n",
    "            idx=test_data[:, :prompt_len],\n",
    "            max_new_tokens=gen_len,\n",
    "            prompt_len =prompt_len,\n",
    "            mask_idx= mask_idx,\n",
    "        )\n",
    "\n",
    "        train_acc = torch.mean(\n",
    "            (train_pred[:, acc_start:] == train_data[:, acc_start:]).to(float)\n",
    "        ).item()\n",
    "        test_acc = torch.mean(\n",
    "            (test_pred[:, acc_start:] == test_data[:, acc_start:]).to(float)\n",
    "        ).item()\n",
    "\n",
    "        data_repeat_frac = torch.mean((test_data[:, acc_start:-1] == test_data[:, acc_start+1:]).to(float))\n",
    "        model_repeat_frac = torch.mean((test_pred[:, acc_start:-1] == test_pred[:, acc_start+1:]).to(float))\n",
    "\n",
    "        # Log attention progress measure\n",
    "        attn_map_output_seq = attn_map[:, :, acc_start-1:]\n",
    "        att_mask = torch.zeros_like(attn_map_output_seq).to(device)\n",
    "\n",
    "        att_mask[:, :, 0, 0] = 1\n",
    "        for i in range(num_tokens - 1):\n",
    "            att_mask[:, :, i + 1, i : i + 2] = 1\n",
    "\n",
    "        att_prog_measure = torch.mean(\n",
    "            torch.sum(torch.abs(attn_map_output_seq) * att_mask, dim=(-3, -2, -1)) /\n",
    "            torch.sum(torch.abs(attn_map_output_seq), dim=(-3, -2, -1)),\n",
    "            dim=0\n",
    "        )\n",
    "\n",
    "        # Log pair-wise cosine similarity between hidden states\n",
    "        embed_start = acc_start - 1\n",
    "        embed_len = gen_len\n",
    "\n",
    "        logit_cs = torch.zeros((embed_len, embed_len))\n",
    "\n",
    "        for i_1 in range(embed_start, embed_start + embed_len):\n",
    "            for i_2 in range(embed_start, i_1):\n",
    "                logit_cs[i_1 - embed_start, i_2 - embed_start] = torch.mean(\n",
    "                    (\n",
    "                        cosine_similarity(\n",
    "                            pre_lm_h[:, i_1, :], pre_lm_h[:, i_2, :], dim=-1\n",
    "                        )\n",
    "                    ), dim=0\n",
    "                )\n",
    "\n",
    "        # Log plots for cosine similarity, attention map\n",
    "        logit_fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(30, 15))\n",
    "\n",
    "        im1 = ax[0].imshow(logit_cs)\n",
    "        ax[0].set_title(\"avg pre_lm_h cosine sim\")\n",
    "        cb1 = logit_fig.colorbar(im1, location=\"right\", shrink=0.99, pad=0.02, ax=ax[0])\n",
    "\n",
    "        avg_attn_map = torch.mean(attn_map, dim=0).squeeze().detach().cpu().numpy()\n",
    "\n",
    "        im2 = ax[1].imshow(avg_attn_map)\n",
    "        ax[1].set_title(\"att map\")\n",
    "        cb4 = logit_fig.colorbar(im2, location=\"right\", shrink=0.99, pad=0.02, ax=ax[1])\n",
    "        ax[1].set_xticks(range(avg_attn_map.shape[-1]))\n",
    "        ax[1].set_yticks(range(avg_attn_map.shape[-2]))\n",
    "\n",
    "        for i1 in range(embed_len):\n",
    "            for i2 in range(embed_len):\n",
    "                text1 = ax[0].text(\n",
    "                    i2,\n",
    "                    i1,\n",
    "                    round(logit_cs[i1, i2].item(), 2),\n",
    "                    ha=\"center\",\n",
    "                    va=\"center\",\n",
    "                    color=\"w\",\n",
    "                )\n",
    "\n",
    "\n",
    "        print(\n",
    "            f\"Step {step} -- Train loss: {train_loss}, Train Acc: {train_acc} Test Acc: {test_acc}\"\n",
    "        )\n",
    "        # print(f\"input: {test_data[0]} \\n predicted:{test_pred[0]}\")\n",
    "\n",
    "        if config.train.wandb:\n",
    "\n",
    "            log_data = {\n",
    "                \"train_loss\": train_loss,\n",
    "                \"train_acc\": train_acc,\n",
    "                \"test_acc\": test_acc,\n",
    "                \"data_repeat_frac\": data_repeat_frac,\n",
    "                \"model_repeat_frac\": model_repeat_frac,\n",
    "                \"att_prog_measure\": att_prog_measure,\n",
    "                \"pre_lm_h_cosine_sim\": logit_fig,\n",
    "                \"mean_cosine_sim\": torch.sum(logit_cs[:, 1:]) / (0.5 * (gen_len-1) * (gen_len-2))\n",
    "            }\n",
    "\n",
    "            for output_pos in range(gen_len):\n",
    "                log_data.update(\n",
    "                    {\n",
    "                        f\"idx{output_pos}_check\": torch.mean(\n",
    "                            (train_pred[:, acc_start + output_pos] == train_data[:, acc_start + output_pos]).to(float)\n",
    "                        ).item()\n",
    "                    }\n",
    "                )\n",
    "\n",
    "                if output_pos < gen_len-1:\n",
    "                    log_data.update(\n",
    "                        {\n",
    "                            f\"mean_cosine_sim_{output_pos}\": torch.sum(logit_cs[:, output_pos]) / (gen_len-1-output_pos)\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "            wandb.log(log_data)\n",
    "\n",
    "        plt.close()\n",
    "        del (\n",
    "            logit_fig,\n",
    "            ax,\n",
    "            logit_cs,\n",
    "        )\n",
    "\n",
    "        if config.train.save_ckpt:\n",
    "            if (step == 0) or ((step + 1) % config.train.ckpt_freq == 0):\n",
    "                model.train()\n",
    "                torch.save(\n",
    "                    {\n",
    "                        \"epoch\": step,\n",
    "                        \"model\": model.state_dict(),\n",
    "                        \"optim\": optim.state_dict(),\n",
    "                        \"train_loss\": train_loss,\n",
    "                        \"test_acc\": test_acc,\n",
    "                    },\n",
    "                    \"./mws_k2_l1_h1_a16_n16.tar\",\n",
    "                )\n",
    "                print(f\"saved state at epoch {step} to {f'./mws_k2_l1_h1_a16_n16.tar'}\")\n",
    "\n",
    "                if config.train.wandb:\n",
    "                    model_wandb = wandb.Artifact(\n",
    "                        f\"model_step{step}\", type=\"model\"\n",
    "                    )\n",
    "                    model_wandb.add_file(f\"./mws_k2_l1_h1_a16_n16.tar\")\n",
    "                    wandb.log_artifact(model_wandb)\n",
    "                    print(\"model uploaded to wandb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.22.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jyue/private/tf-loss-plateau/wandb/run-20251104_022613-gei2zj11</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/wth_ucsd/loss_plateau_tf/runs/gei2zj11' target=\"_blank\">mws_softmax_attention_loss_noinput_indexTrainTest</a></strong> to <a href='https://wandb.ai/wth_ucsd/loss_plateau_tf' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/wth_ucsd/loss_plateau_tf' target=\"_blank\">https://wandb.ai/wth_ucsd/loss_plateau_tf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/wth_ucsd/loss_plateau_tf/runs/gei2zj11' target=\"_blank\">https://wandb.ai/wth_ucsd/loss_plateau_tf/runs/gei2zj11</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "config = {\n",
    "'model':\n",
    "  {\n",
    "    'n_layer': 1,\n",
    "    'n_head': 1,\n",
    "    'n_embd': 256,\n",
    "    'linear': True,\n",
    "  },\n",
    "\n",
    "'data':\n",
    "  {\n",
    "    'name': 'window',\n",
    "    'min_num': 1,\n",
    "    'max_num': 16,\n",
    "    'k': 2,\n",
    "    'p': 17,\n",
    "    'sep': 17,\n",
    "    'cot': False,\n",
    "    'num_tokens': 16,\n",
    "    'n_train': 256,\n",
    "    'n_test': 64,\n",
    "    'fixed_len': True,\n",
    "  },\n",
    "\n",
    "'train':\n",
    "  {\n",
    "    'lr': 0.0005,\n",
    "    'grad_clip': -1,\n",
    "    'num_steps': 2000,\n",
    "    'norm_type': \"none_rank\",\n",
    "    'wandb': True,\n",
    "    'save_ckpt': False,\n",
    "    'ckpt_freq': 20,\n",
    "  }\n",
    "}\n",
    "\n",
    "config = DotMap(config)\n",
    "config.model.vocab_size = max(config.data.p, config.data.max_num) + 1\n",
    "config.model.block_size = 2 * config.data.num_tokens + 1\n",
    "\n",
    "data_sampler = MovingWindowSum(\n",
    "    min_num=config.data.min_num,\n",
    "    max_num=config.data.max_num,\n",
    "    k=config.data.k,\n",
    "    p=config.data.p,\n",
    ")\n",
    "\n",
    "wandb.login(key=\"\")  # login once\n",
    "model = GPTSoftmax(config.model, return_att=True).to(device)\n",
    "optim = Adam(model.parameters(), lr=config.train.lr)\n",
    "if config.train.wandb:\n",
    "    wandb_run_name = f'mws_softmax_attention_loss_noinput_indexTrainTest'\n",
    "    wandb.init(project=\"loss_plateau_tf\", name=wandb_run_name, config=config)\n",
    "    wandb.watch(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0 -- Train loss: 2.7549939155578613, Train Acc: 0.064208984375 Test Acc: 0.0625\n",
      "Step 1 -- Train loss: 2.7794904708862305, Train Acc: 0.0595703125 Test Acc: 0.0576171875\n",
      "Step 2 -- Train loss: 2.749708652496338, Train Acc: 0.06103515625 Test Acc: 0.0546875\n",
      "Step 3 -- Train loss: 2.7396187782287598, Train Acc: 0.0615234375 Test Acc: 0.064453125\n",
      "Step 4 -- Train loss: 2.677426815032959, Train Acc: 0.071533203125 Test Acc: 0.078125\n",
      "Step 5 -- Train loss: 2.4569082260131836, Train Acc: 0.08740234375 Test Acc: 0.0927734375\n",
      "Step 6 -- Train loss: 2.1594789028167725, Train Acc: 0.102294921875 Test Acc: 0.1142578125\n",
      "Step 7 -- Train loss: 1.9396929740905762, Train Acc: 0.1025390625 Test Acc: 0.1025390625\n",
      "Step 8 -- Train loss: 1.7339181900024414, Train Acc: 0.114013671875 Test Acc: 0.1103515625\n",
      "Step 9 -- Train loss: 1.6482175588607788, Train Acc: 0.10986328125 Test Acc: 0.1162109375\n",
      "Step 10 -- Train loss: 1.6036806106567383, Train Acc: 0.105712890625 Test Acc: 0.1064453125\n",
      "Step 11 -- Train loss: 1.580741047859192, Train Acc: 0.1025390625 Test Acc: 0.1083984375\n",
      "Step 12 -- Train loss: 1.5577540397644043, Train Acc: 0.110595703125 Test Acc: 0.111328125\n",
      "Step 13 -- Train loss: 1.5224430561065674, Train Acc: 0.1083984375 Test Acc: 0.111328125\n",
      "Step 14 -- Train loss: 1.5349336862564087, Train Acc: 0.10009765625 Test Acc: 0.1123046875\n",
      "Step 15 -- Train loss: 1.5159051418304443, Train Acc: 0.1103515625 Test Acc: 0.107421875\n",
      "Step 16 -- Train loss: 1.4930148124694824, Train Acc: 0.109619140625 Test Acc: 0.1201171875\n",
      "Step 17 -- Train loss: 1.494773268699646, Train Acc: 0.1103515625 Test Acc: 0.11328125\n",
      "Step 18 -- Train loss: 1.4914780855178833, Train Acc: 0.110595703125 Test Acc: 0.0927734375\n",
      "Step 19 -- Train loss: 1.4948935508728027, Train Acc: 0.109130859375 Test Acc: 0.09765625\n",
      "Step 20 -- Train loss: 1.4739471673965454, Train Acc: 0.11083984375 Test Acc: 0.1015625\n",
      "Step 21 -- Train loss: 1.4514564275741577, Train Acc: 0.1044921875 Test Acc: 0.1064453125\n",
      "Step 22 -- Train loss: 1.4649022817611694, Train Acc: 0.1142578125 Test Acc: 0.1123046875\n",
      "Step 23 -- Train loss: 1.4549561738967896, Train Acc: 0.11669921875 Test Acc: 0.119140625\n",
      "Step 24 -- Train loss: 1.4513431787490845, Train Acc: 0.11279296875 Test Acc: 0.107421875\n",
      "Step 25 -- Train loss: 1.4553394317626953, Train Acc: 0.112060546875 Test Acc: 0.1171875\n",
      "Step 26 -- Train loss: 1.4369325637817383, Train Acc: 0.114013671875 Test Acc: 0.111328125\n",
      "Step 27 -- Train loss: 1.4428026676177979, Train Acc: 0.10400390625 Test Acc: 0.107421875\n",
      "Step 28 -- Train loss: 1.4599882364273071, Train Acc: 0.105712890625 Test Acc: 0.12109375\n",
      "Step 29 -- Train loss: 1.4642354249954224, Train Acc: 0.11181640625 Test Acc: 0.1103515625\n",
      "Step 30 -- Train loss: 1.450516939163208, Train Acc: 0.107666015625 Test Acc: 0.126953125\n",
      "Step 31 -- Train loss: 1.452438473701477, Train Acc: 0.109375 Test Acc: 0.115234375\n",
      "Step 32 -- Train loss: 1.452069640159607, Train Acc: 0.11279296875 Test Acc: 0.11328125\n",
      "Step 33 -- Train loss: 1.4398868083953857, Train Acc: 0.113525390625 Test Acc: 0.119140625\n",
      "Step 34 -- Train loss: 1.4408514499664307, Train Acc: 0.112060546875 Test Acc: 0.1083984375\n",
      "Step 35 -- Train loss: 1.4364476203918457, Train Acc: 0.10791015625 Test Acc: 0.1025390625\n",
      "Step 36 -- Train loss: 1.4404504299163818, Train Acc: 0.111328125 Test Acc: 0.107421875\n",
      "Step 37 -- Train loss: 1.4384468793869019, Train Acc: 0.11083984375 Test Acc: 0.1123046875\n",
      "Step 38 -- Train loss: 1.4403830766677856, Train Acc: 0.1142578125 Test Acc: 0.1123046875\n",
      "Step 39 -- Train loss: 1.4370641708374023, Train Acc: 0.117431640625 Test Acc: 0.111328125\n",
      "Step 40 -- Train loss: 1.440661072731018, Train Acc: 0.113525390625 Test Acc: 0.115234375\n",
      "Step 41 -- Train loss: 1.4388431310653687, Train Acc: 0.107177734375 Test Acc: 0.11328125\n",
      "Step 42 -- Train loss: 1.4430246353149414, Train Acc: 0.115234375 Test Acc: 0.1240234375\n",
      "Step 43 -- Train loss: 1.4260716438293457, Train Acc: 0.111083984375 Test Acc: 0.1181640625\n",
      "Step 44 -- Train loss: 1.436105489730835, Train Acc: 0.112548828125 Test Acc: 0.1181640625\n",
      "Step 45 -- Train loss: 1.4300707578659058, Train Acc: 0.111083984375 Test Acc: 0.1181640625\n",
      "Step 46 -- Train loss: 1.431803822517395, Train Acc: 0.115478515625 Test Acc: 0.115234375\n",
      "Step 47 -- Train loss: 1.4379160404205322, Train Acc: 0.114501953125 Test Acc: 0.1162109375\n",
      "Step 48 -- Train loss: 1.4452234506607056, Train Acc: 0.110595703125 Test Acc: 0.1142578125\n",
      "Step 49 -- Train loss: 1.433126449584961, Train Acc: 0.110595703125 Test Acc: 0.1171875\n",
      "Step 50 -- Train loss: 1.428527593612671, Train Acc: 0.111572265625 Test Acc: 0.111328125\n",
      "Step 51 -- Train loss: 1.4293544292449951, Train Acc: 0.111083984375 Test Acc: 0.1162109375\n",
      "Step 52 -- Train loss: 1.419076919555664, Train Acc: 0.12060546875 Test Acc: 0.107421875\n",
      "Step 53 -- Train loss: 1.435179591178894, Train Acc: 0.115966796875 Test Acc: 0.11328125\n",
      "Step 54 -- Train loss: 1.4397361278533936, Train Acc: 0.113525390625 Test Acc: 0.10546875\n",
      "Step 55 -- Train loss: 1.4396620988845825, Train Acc: 0.1181640625 Test Acc: 0.107421875\n",
      "Step 56 -- Train loss: 1.4380278587341309, Train Acc: 0.11328125 Test Acc: 0.119140625\n",
      "Step 57 -- Train loss: 1.4347769021987915, Train Acc: 0.109130859375 Test Acc: 0.109375\n",
      "Step 58 -- Train loss: 1.4422259330749512, Train Acc: 0.106689453125 Test Acc: 0.1181640625\n",
      "Step 59 -- Train loss: 1.4250932931900024, Train Acc: 0.11669921875 Test Acc: 0.109375\n",
      "Step 60 -- Train loss: 1.4283392429351807, Train Acc: 0.113037109375 Test Acc: 0.1015625\n",
      "Step 61 -- Train loss: 1.4239592552185059, Train Acc: 0.114501953125 Test Acc: 0.1162109375\n",
      "Step 62 -- Train loss: 1.4350064992904663, Train Acc: 0.11279296875 Test Acc: 0.1181640625\n",
      "Step 63 -- Train loss: 1.4205598831176758, Train Acc: 0.10888671875 Test Acc: 0.1015625\n",
      "Step 64 -- Train loss: 1.429779052734375, Train Acc: 0.112548828125 Test Acc: 0.12109375\n",
      "Step 65 -- Train loss: 1.4377325773239136, Train Acc: 0.104736328125 Test Acc: 0.10546875\n",
      "Step 66 -- Train loss: 1.420479655265808, Train Acc: 0.10693359375 Test Acc: 0.10546875\n",
      "Step 67 -- Train loss: 1.4264463186264038, Train Acc: 0.115234375 Test Acc: 0.1201171875\n",
      "Step 68 -- Train loss: 1.4288910627365112, Train Acc: 0.105224609375 Test Acc: 0.111328125\n",
      "Step 69 -- Train loss: 1.430497646331787, Train Acc: 0.107177734375 Test Acc: 0.107421875\n",
      "Step 70 -- Train loss: 1.41852605342865, Train Acc: 0.115478515625 Test Acc: 0.109375\n",
      "Step 71 -- Train loss: 1.4238685369491577, Train Acc: 0.11767578125 Test Acc: 0.11328125\n",
      "Step 72 -- Train loss: 1.428964376449585, Train Acc: 0.114990234375 Test Acc: 0.119140625\n",
      "Step 73 -- Train loss: 1.4347397089004517, Train Acc: 0.11572265625 Test Acc: 0.1044921875\n",
      "Step 74 -- Train loss: 1.4278302192687988, Train Acc: 0.112548828125 Test Acc: 0.115234375\n",
      "Step 75 -- Train loss: 1.4384214878082275, Train Acc: 0.105224609375 Test Acc: 0.111328125\n",
      "Step 76 -- Train loss: 1.4253848791122437, Train Acc: 0.109375 Test Acc: 0.10546875\n",
      "Step 77 -- Train loss: 1.428115725517273, Train Acc: 0.111083984375 Test Acc: 0.1044921875\n",
      "Step 78 -- Train loss: 1.4263967275619507, Train Acc: 0.11181640625 Test Acc: 0.1171875\n",
      "Step 79 -- Train loss: 1.4302843809127808, Train Acc: 0.116455078125 Test Acc: 0.1337890625\n",
      "Step 80 -- Train loss: 1.4232674837112427, Train Acc: 0.112548828125 Test Acc: 0.109375\n",
      "Step 81 -- Train loss: 1.4294501543045044, Train Acc: 0.114990234375 Test Acc: 0.1142578125\n",
      "Step 82 -- Train loss: 1.4301352500915527, Train Acc: 0.1123046875 Test Acc: 0.1171875\n",
      "Step 83 -- Train loss: 1.43095064163208, Train Acc: 0.112060546875 Test Acc: 0.111328125\n",
      "Step 84 -- Train loss: 1.4346181154251099, Train Acc: 0.1142578125 Test Acc: 0.1162109375\n",
      "Step 85 -- Train loss: 1.4351935386657715, Train Acc: 0.1162109375 Test Acc: 0.1015625\n",
      "Step 86 -- Train loss: 1.4241009950637817, Train Acc: 0.12255859375 Test Acc: 0.125\n",
      "Step 87 -- Train loss: 1.4182828664779663, Train Acc: 0.113037109375 Test Acc: 0.0986328125\n",
      "Step 88 -- Train loss: 1.4237803220748901, Train Acc: 0.11669921875 Test Acc: 0.0966796875\n",
      "Step 89 -- Train loss: 1.4165656566619873, Train Acc: 0.122802734375 Test Acc: 0.1162109375\n",
      "Step 90 -- Train loss: 1.4270434379577637, Train Acc: 0.1123046875 Test Acc: 0.107421875\n",
      "Step 91 -- Train loss: 1.4213398694992065, Train Acc: 0.11328125 Test Acc: 0.111328125\n",
      "Step 92 -- Train loss: 1.407841444015503, Train Acc: 0.1181640625 Test Acc: 0.1083984375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x7f469c11e8b0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jyue/.conda/envs/emerge/lib/python3.8/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 93 -- Train loss: 1.425818681716919, Train Acc: 0.1181640625 Test Acc: 0.1181640625\n",
      "Step 94 -- Train loss: 1.4303148984909058, Train Acc: 0.1103515625 Test Acc: 0.1162109375\n",
      "Step 95 -- Train loss: 1.4312187433242798, Train Acc: 0.11376953125 Test Acc: 0.111328125\n",
      "Step 96 -- Train loss: 1.4325673580169678, Train Acc: 0.117431640625 Test Acc: 0.1162109375\n",
      "Step 97 -- Train loss: 1.427775263786316, Train Acc: 0.113037109375 Test Acc: 0.1123046875\n",
      "Step 98 -- Train loss: 1.425663948059082, Train Acc: 0.113525390625 Test Acc: 0.107421875\n",
      "Step 99 -- Train loss: 1.4318015575408936, Train Acc: 0.10693359375 Test Acc: 0.1044921875\n",
      "Step 100 -- Train loss: 1.4181442260742188, Train Acc: 0.11083984375 Test Acc: 0.119140625\n",
      "Step 101 -- Train loss: 1.422372817993164, Train Acc: 0.110595703125 Test Acc: 0.1103515625\n",
      "Step 102 -- Train loss: 1.4247970581054688, Train Acc: 0.115966796875 Test Acc: 0.103515625\n",
      "Step 103 -- Train loss: 1.4135059118270874, Train Acc: 0.12060546875 Test Acc: 0.1162109375\n",
      "Step 104 -- Train loss: 1.4352424144744873, Train Acc: 0.113525390625 Test Acc: 0.1064453125\n",
      "Step 105 -- Train loss: 1.4238158464431763, Train Acc: 0.120361328125 Test Acc: 0.1259765625\n",
      "Step 106 -- Train loss: 1.4484105110168457, Train Acc: 0.117431640625 Test Acc: 0.1201171875\n",
      "Step 107 -- Train loss: 1.4272035360336304, Train Acc: 0.109375 Test Acc: 0.109375\n",
      "Step 108 -- Train loss: 1.4344390630722046, Train Acc: 0.1103515625 Test Acc: 0.125\n",
      "Step 109 -- Train loss: 1.4144551753997803, Train Acc: 0.1171875 Test Acc: 0.1181640625\n",
      "Step 110 -- Train loss: 1.4255434274673462, Train Acc: 0.119140625 Test Acc: 0.1201171875\n",
      "Step 111 -- Train loss: 1.4150933027267456, Train Acc: 0.111328125 Test Acc: 0.123046875\n",
      "Step 112 -- Train loss: 1.4302396774291992, Train Acc: 0.115234375 Test Acc: 0.115234375\n",
      "Step 113 -- Train loss: 1.4311753511428833, Train Acc: 0.11328125 Test Acc: 0.1123046875\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m### Training on y0 and y1\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m500\u001b[39m):\n\u001b[0;32m----> 3\u001b[0m     \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata_sampler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_sampler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmask_idx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 423\u001b[0m, in \u001b[0;36mtrain_step\u001b[0;34m(model, optim, data_sampler, step, config, mask_idx)\u001b[0m\n\u001b[1;32m    420\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m    421\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m    422\u001b[0m     \u001b[38;5;66;03m# Log train loss, train / test acc, repetition frequency\u001b[39;00m\n\u001b[0;32m--> 423\u001b[0m     attn_map, pre_lm_h, _, train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_data\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_len\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask_idx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmask_idx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    425\u001b[0m     train_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[1;32m    426\u001b[0m         idx\u001b[38;5;241m=\u001b[39mtrain_data[:, :prompt_len],\n\u001b[1;32m    427\u001b[0m         max_new_tokens\u001b[38;5;241m=\u001b[39mgen_len,\n\u001b[1;32m    428\u001b[0m         prompt_len \u001b[38;5;241m=\u001b[39mprompt_len,\n\u001b[1;32m    429\u001b[0m         mask_idx\u001b[38;5;241m=\u001b[39m mask_idx,\n\u001b[1;32m    430\u001b[0m     )\n\u001b[1;32m    431\u001b[0m     test_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[1;32m    432\u001b[0m         idx\u001b[38;5;241m=\u001b[39mtest_data[:, :prompt_len],\n\u001b[1;32m    433\u001b[0m         max_new_tokens\u001b[38;5;241m=\u001b[39mgen_len,\n\u001b[1;32m    434\u001b[0m         prompt_len \u001b[38;5;241m=\u001b[39mprompt_len,\n\u001b[1;32m    435\u001b[0m         mask_idx\u001b[38;5;241m=\u001b[39m mask_idx,\n\u001b[1;32m    436\u001b[0m     )\n",
      "File \u001b[0;32m~/.conda/envs/emerge/lib/python3.8/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/emerge/lib/python3.8/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[5], line 237\u001b[0m, in \u001b[0;36mGPTSoftmax.forward\u001b[0;34m(self, idx, prompt_len, targets, mask_idx)\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;66;03m# forward the GPT model itself\u001b[39;00m\n\u001b[1;32m    236\u001b[0m tok_emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer\u001b[38;5;241m.\u001b[39mwte(idx)  \u001b[38;5;66;03m# token embeddings of shape (b, t, n_embd)\u001b[39;00m\n\u001b[0;32m--> 237\u001b[0m pos_emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwpe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpos\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# position embeddings of shape (1, t, n_embd)\u001b[39;00m\n\u001b[1;32m    238\u001b[0m x \u001b[38;5;241m=\u001b[39m tok_emb \u001b[38;5;241m+\u001b[39m pos_emb\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer\u001b[38;5;241m.\u001b[39mh:\n",
      "File \u001b[0;32m~/.conda/envs/emerge/lib/python3.8/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/emerge/lib/python3.8/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/emerge/lib/python3.8/site-packages/torch/nn/modules/sparse.py:162\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 162\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/emerge/lib/python3.8/site-packages/torch/nn/functional.py:2233\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2227\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2228\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2229\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2230\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2231\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2232\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2233\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "### Training on y0 and y1\n",
    "for step in range(500):\n",
    "    train_step(\n",
    "        model=model,\n",
    "        optim=optim,\n",
    "        data_sampler=data_sampler,\n",
    "        step=step,\n",
    "        config=config,\n",
    "        mask_idx=1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0 -- Train loss: 2.6748199462890625, Train Acc: 0.111572265625 Test Acc: 0.11328125\n",
      "Step 1 -- Train loss: 2.6719019412994385, Train Acc: 0.119140625 Test Acc: 0.119140625\n",
      "Step 2 -- Train loss: 2.6667020320892334, Train Acc: 0.11572265625 Test Acc: 0.099609375\n",
      "Step 3 -- Train loss: 2.6699678897857666, Train Acc: 0.10791015625 Test Acc: 0.1064453125\n",
      "Step 4 -- Train loss: 2.6721062660217285, Train Acc: 0.11181640625 Test Acc: 0.109375\n",
      "Step 5 -- Train loss: 2.6682567596435547, Train Acc: 0.115966796875 Test Acc: 0.1123046875\n",
      "Step 6 -- Train loss: 2.670205593109131, Train Acc: 0.11181640625 Test Acc: 0.1083984375\n",
      "Step 7 -- Train loss: 2.6663928031921387, Train Acc: 0.115966796875 Test Acc: 0.12109375\n",
      "Step 8 -- Train loss: 2.6649703979492188, Train Acc: 0.1220703125 Test Acc: 0.12109375\n",
      "Step 9 -- Train loss: 2.6670596599578857, Train Acc: 0.115478515625 Test Acc: 0.1142578125\n",
      "Step 10 -- Train loss: 2.663695812225342, Train Acc: 0.115234375 Test Acc: 0.1162109375\n",
      "Step 11 -- Train loss: 2.6641921997070312, Train Acc: 0.11669921875 Test Acc: 0.10546875\n",
      "Step 12 -- Train loss: 2.667025089263916, Train Acc: 0.114013671875 Test Acc: 0.10546875\n",
      "Step 13 -- Train loss: 2.661773443222046, Train Acc: 0.12451171875 Test Acc: 0.125\n",
      "Step 14 -- Train loss: 2.663207769393921, Train Acc: 0.11474609375 Test Acc: 0.119140625\n",
      "Step 15 -- Train loss: 2.6635422706604004, Train Acc: 0.1142578125 Test Acc: 0.109375\n",
      "Step 16 -- Train loss: 2.6629889011383057, Train Acc: 0.1064453125 Test Acc: 0.1044921875\n",
      "Step 17 -- Train loss: 2.661576747894287, Train Acc: 0.1162109375 Test Acc: 0.1064453125\n",
      "Step 18 -- Train loss: 2.661202907562256, Train Acc: 0.114501953125 Test Acc: 0.1162109375\n",
      "Step 19 -- Train loss: 2.6630866527557373, Train Acc: 0.10986328125 Test Acc: 0.115234375\n",
      "Step 20 -- Train loss: 2.663043260574341, Train Acc: 0.114990234375 Test Acc: 0.1025390625\n",
      "Step 21 -- Train loss: 2.662770986557007, Train Acc: 0.116455078125 Test Acc: 0.1220703125\n",
      "Step 22 -- Train loss: 2.661839008331299, Train Acc: 0.113037109375 Test Acc: 0.1162109375\n",
      "Step 23 -- Train loss: 2.6592512130737305, Train Acc: 0.12158203125 Test Acc: 0.123046875\n",
      "Step 24 -- Train loss: 2.6601767539978027, Train Acc: 0.11328125 Test Acc: 0.1103515625\n",
      "Step 25 -- Train loss: 2.6611227989196777, Train Acc: 0.118896484375 Test Acc: 0.1240234375\n",
      "Step 26 -- Train loss: 2.6598360538482666, Train Acc: 0.123291015625 Test Acc: 0.1298828125\n",
      "Step 27 -- Train loss: 2.6609933376312256, Train Acc: 0.114990234375 Test Acc: 0.142578125\n",
      "Step 28 -- Train loss: 2.6596288681030273, Train Acc: 0.121826171875 Test Acc: 0.123046875\n",
      "Step 29 -- Train loss: 2.660339117050171, Train Acc: 0.116455078125 Test Acc: 0.11328125\n",
      "Step 30 -- Train loss: 2.659763813018799, Train Acc: 0.12109375 Test Acc: 0.1083984375\n",
      "Step 31 -- Train loss: 2.659557580947876, Train Acc: 0.107421875 Test Acc: 0.11328125\n",
      "Step 32 -- Train loss: 2.6606061458587646, Train Acc: 0.113525390625 Test Acc: 0.11328125\n",
      "Step 33 -- Train loss: 2.659353017807007, Train Acc: 0.119140625 Test Acc: 0.11328125\n",
      "Step 34 -- Train loss: 2.6589465141296387, Train Acc: 0.1181640625 Test Acc: 0.1103515625\n",
      "Step 35 -- Train loss: 2.658320188522339, Train Acc: 0.125244140625 Test Acc: 0.109375\n",
      "Step 36 -- Train loss: 2.6602139472961426, Train Acc: 0.1162109375 Test Acc: 0.109375\n",
      "Step 37 -- Train loss: 2.659245491027832, Train Acc: 0.1181640625 Test Acc: 0.1162109375\n",
      "Step 38 -- Train loss: 2.659895658493042, Train Acc: 0.11279296875 Test Acc: 0.12109375\n",
      "Step 39 -- Train loss: 2.6588246822357178, Train Acc: 0.120849609375 Test Acc: 0.123046875\n",
      "Step 40 -- Train loss: 2.6576590538024902, Train Acc: 0.11962890625 Test Acc: 0.1142578125\n",
      "Step 41 -- Train loss: 2.6589624881744385, Train Acc: 0.121337890625 Test Acc: 0.111328125\n",
      "Step 42 -- Train loss: 2.6590681076049805, Train Acc: 0.116943359375 Test Acc: 0.1103515625\n",
      "Step 43 -- Train loss: 2.658919095993042, Train Acc: 0.122314453125 Test Acc: 0.115234375\n",
      "Step 44 -- Train loss: 2.6615710258483887, Train Acc: 0.117919921875 Test Acc: 0.125\n",
      "Step 45 -- Train loss: 2.6593525409698486, Train Acc: 0.11669921875 Test Acc: 0.1240234375\n",
      "Step 46 -- Train loss: 2.6594161987304688, Train Acc: 0.120361328125 Test Acc: 0.10546875\n",
      "Step 47 -- Train loss: 2.6592469215393066, Train Acc: 0.115234375 Test Acc: 0.1064453125\n",
      "Step 48 -- Train loss: 2.6611380577087402, Train Acc: 0.110595703125 Test Acc: 0.1181640625\n",
      "Step 49 -- Train loss: 2.6591529846191406, Train Acc: 0.120361328125 Test Acc: 0.1142578125\n",
      "Step 50 -- Train loss: 2.6574466228485107, Train Acc: 0.115478515625 Test Acc: 0.1240234375\n",
      "Step 51 -- Train loss: 2.659264087677002, Train Acc: 0.117431640625 Test Acc: 0.103515625\n",
      "Step 52 -- Train loss: 2.658632755279541, Train Acc: 0.11572265625 Test Acc: 0.1162109375\n",
      "Step 53 -- Train loss: 2.6581835746765137, Train Acc: 0.121337890625 Test Acc: 0.1240234375\n",
      "Step 54 -- Train loss: 2.659127950668335, Train Acc: 0.119384765625 Test Acc: 0.11328125\n",
      "Step 55 -- Train loss: 2.6560733318328857, Train Acc: 0.12109375 Test Acc: 0.1103515625\n",
      "Step 56 -- Train loss: 2.65946626663208, Train Acc: 0.11865234375 Test Acc: 0.1337890625\n",
      "Step 57 -- Train loss: 2.659348964691162, Train Acc: 0.120849609375 Test Acc: 0.1181640625\n",
      "Step 58 -- Train loss: 2.6597344875335693, Train Acc: 0.1181640625 Test Acc: 0.107421875\n",
      "Step 59 -- Train loss: 2.657924175262451, Train Acc: 0.123779296875 Test Acc: 0.115234375\n",
      "Step 60 -- Train loss: 2.6580827236175537, Train Acc: 0.117431640625 Test Acc: 0.1142578125\n",
      "Step 61 -- Train loss: 2.6569371223449707, Train Acc: 0.111083984375 Test Acc: 0.119140625\n",
      "Step 62 -- Train loss: 2.660510778427124, Train Acc: 0.10986328125 Test Acc: 0.1171875\n",
      "Step 63 -- Train loss: 2.6603410243988037, Train Acc: 0.113037109375 Test Acc: 0.1123046875\n",
      "Step 64 -- Train loss: 2.6596343517303467, Train Acc: 0.111083984375 Test Acc: 0.1044921875\n",
      "Step 65 -- Train loss: 2.657930850982666, Train Acc: 0.1171875 Test Acc: 0.1025390625\n",
      "Step 66 -- Train loss: 2.6592977046966553, Train Acc: 0.112060546875 Test Acc: 0.1181640625\n",
      "Step 67 -- Train loss: 2.6583683490753174, Train Acc: 0.113037109375 Test Acc: 0.10546875\n",
      "Step 68 -- Train loss: 2.6587629318237305, Train Acc: 0.12158203125 Test Acc: 0.1064453125\n",
      "Step 69 -- Train loss: 2.6582462787628174, Train Acc: 0.120849609375 Test Acc: 0.119140625\n",
      "Step 70 -- Train loss: 2.6581876277923584, Train Acc: 0.1162109375 Test Acc: 0.119140625\n",
      "Step 71 -- Train loss: 2.660658597946167, Train Acc: 0.1103515625 Test Acc: 0.1201171875\n",
      "Step 72 -- Train loss: 2.659536123275757, Train Acc: 0.1142578125 Test Acc: 0.111328125\n",
      "Step 73 -- Train loss: 2.6578598022460938, Train Acc: 0.11865234375 Test Acc: 0.107421875\n",
      "Step 74 -- Train loss: 2.659493923187256, Train Acc: 0.1142578125 Test Acc: 0.1181640625\n",
      "Step 75 -- Train loss: 2.66058349609375, Train Acc: 0.1279296875 Test Acc: 0.1201171875\n",
      "Step 76 -- Train loss: 2.660165309906006, Train Acc: 0.109375 Test Acc: 0.1064453125\n",
      "Step 77 -- Train loss: 2.658998727798462, Train Acc: 0.11572265625 Test Acc: 0.125\n",
      "Step 78 -- Train loss: 2.657693386077881, Train Acc: 0.126220703125 Test Acc: 0.1171875\n",
      "Step 79 -- Train loss: 2.6590874195098877, Train Acc: 0.11181640625 Test Acc: 0.115234375\n",
      "Step 80 -- Train loss: 2.6575937271118164, Train Acc: 0.1142578125 Test Acc: 0.11328125\n",
      "Step 81 -- Train loss: 2.6586623191833496, Train Acc: 0.115966796875 Test Acc: 0.119140625\n",
      "Step 82 -- Train loss: 2.6573808193206787, Train Acc: 0.119140625 Test Acc: 0.10546875\n",
      "Step 83 -- Train loss: 2.659238576889038, Train Acc: 0.12255859375 Test Acc: 0.11328125\n",
      "Step 84 -- Train loss: 2.659390449523926, Train Acc: 0.116455078125 Test Acc: 0.130859375\n",
      "Step 85 -- Train loss: 2.6572797298431396, Train Acc: 0.1240234375 Test Acc: 0.1142578125\n",
      "Step 86 -- Train loss: 2.658540964126587, Train Acc: 0.1181640625 Test Acc: 0.115234375\n",
      "Step 87 -- Train loss: 2.657529592514038, Train Acc: 0.119873046875 Test Acc: 0.1064453125\n",
      "Step 88 -- Train loss: 2.6583852767944336, Train Acc: 0.11767578125 Test Acc: 0.1259765625\n",
      "Step 89 -- Train loss: 2.660496473312378, Train Acc: 0.109130859375 Test Acc: 0.1142578125\n",
      "Step 90 -- Train loss: 2.657407283782959, Train Acc: 0.118408203125 Test Acc: 0.1103515625\n",
      "Step 91 -- Train loss: 2.6585347652435303, Train Acc: 0.12158203125 Test Acc: 0.111328125\n",
      "Step 92 -- Train loss: 2.660029411315918, Train Acc: 0.115234375 Test Acc: 0.1240234375\n",
      "Step 93 -- Train loss: 2.659146785736084, Train Acc: 0.1123046875 Test Acc: 0.111328125\n",
      "Step 94 -- Train loss: 2.6581528186798096, Train Acc: 0.119873046875 Test Acc: 0.111328125\n",
      "Step 95 -- Train loss: 2.6581339836120605, Train Acc: 0.114501953125 Test Acc: 0.12890625\n",
      "Step 96 -- Train loss: 2.660076856613159, Train Acc: 0.113037109375 Test Acc: 0.1103515625\n",
      "Step 97 -- Train loss: 2.6580514907836914, Train Acc: 0.120849609375 Test Acc: 0.1171875\n",
      "Step 98 -- Train loss: 2.6576616764068604, Train Acc: 0.123291015625 Test Acc: 0.12890625\n",
      "Step 99 -- Train loss: 2.657069683074951, Train Acc: 0.11962890625 Test Acc: 0.1142578125\n",
      "Step 100 -- Train loss: 2.658426523208618, Train Acc: 0.119384765625 Test Acc: 0.1181640625\n",
      "Step 101 -- Train loss: 2.6587154865264893, Train Acc: 0.119384765625 Test Acc: 0.1142578125\n",
      "Step 102 -- Train loss: 2.6583452224731445, Train Acc: 0.119384765625 Test Acc: 0.125\n",
      "Step 103 -- Train loss: 2.6589744091033936, Train Acc: 0.113037109375 Test Acc: 0.123046875\n",
      "Step 104 -- Train loss: 2.658007860183716, Train Acc: 0.118408203125 Test Acc: 0.12109375\n",
      "Step 105 -- Train loss: 2.65816330909729, Train Acc: 0.116943359375 Test Acc: 0.12109375\n",
      "Step 106 -- Train loss: 2.6590640544891357, Train Acc: 0.119384765625 Test Acc: 0.1162109375\n",
      "Step 107 -- Train loss: 2.6599183082580566, Train Acc: 0.11474609375 Test Acc: 0.123046875\n",
      "Step 108 -- Train loss: 2.659289598464966, Train Acc: 0.115966796875 Test Acc: 0.115234375\n",
      "Step 109 -- Train loss: 2.6580231189727783, Train Acc: 0.117431640625 Test Acc: 0.119140625\n",
      "Step 110 -- Train loss: 2.6574513912200928, Train Acc: 0.11083984375 Test Acc: 0.119140625\n",
      "Step 111 -- Train loss: 2.6574647426605225, Train Acc: 0.12255859375 Test Acc: 0.1142578125\n",
      "Step 112 -- Train loss: 2.657654285430908, Train Acc: 0.12060546875 Test Acc: 0.123046875\n",
      "Step 113 -- Train loss: 2.6578726768493652, Train Acc: 0.12060546875 Test Acc: 0.1181640625\n",
      "Step 114 -- Train loss: 2.657686471939087, Train Acc: 0.11572265625 Test Acc: 0.11328125\n",
      "Step 115 -- Train loss: 2.6564934253692627, Train Acc: 0.123779296875 Test Acc: 0.125\n",
      "Step 116 -- Train loss: 2.6574349403381348, Train Acc: 0.112548828125 Test Acc: 0.1171875\n",
      "Step 117 -- Train loss: 2.657571315765381, Train Acc: 0.115966796875 Test Acc: 0.1181640625\n",
      "Step 118 -- Train loss: 2.658414363861084, Train Acc: 0.118408203125 Test Acc: 0.1181640625\n",
      "Step 119 -- Train loss: 2.6597788333892822, Train Acc: 0.11767578125 Test Acc: 0.11328125\n",
      "Step 120 -- Train loss: 2.657698154449463, Train Acc: 0.111328125 Test Acc: 0.1142578125\n",
      "Step 121 -- Train loss: 2.657771348953247, Train Acc: 0.120361328125 Test Acc: 0.111328125\n",
      "Step 122 -- Train loss: 2.658297300338745, Train Acc: 0.118896484375 Test Acc: 0.107421875\n",
      "Step 123 -- Train loss: 2.6589577198028564, Train Acc: 0.122314453125 Test Acc: 0.1171875\n",
      "Step 124 -- Train loss: 2.6587088108062744, Train Acc: 0.115966796875 Test Acc: 0.126953125\n",
      "Step 125 -- Train loss: 2.6592767238616943, Train Acc: 0.113037109375 Test Acc: 0.119140625\n",
      "Step 126 -- Train loss: 2.6568374633789062, Train Acc: 0.1123046875 Test Acc: 0.1171875\n",
      "Step 127 -- Train loss: 2.657137155532837, Train Acc: 0.117431640625 Test Acc: 0.1181640625\n",
      "Step 128 -- Train loss: 2.65657114982605, Train Acc: 0.119140625 Test Acc: 0.12890625\n",
      "Step 129 -- Train loss: 2.6581029891967773, Train Acc: 0.115478515625 Test Acc: 0.119140625\n",
      "Step 130 -- Train loss: 2.6586732864379883, Train Acc: 0.12255859375 Test Acc: 0.125\n",
      "Step 131 -- Train loss: 2.6584155559539795, Train Acc: 0.1240234375 Test Acc: 0.111328125\n",
      "Step 132 -- Train loss: 2.6590521335601807, Train Acc: 0.112060546875 Test Acc: 0.1181640625\n",
      "Step 133 -- Train loss: 2.658484697341919, Train Acc: 0.109375 Test Acc: 0.1103515625\n",
      "Step 134 -- Train loss: 2.6585443019866943, Train Acc: 0.111328125 Test Acc: 0.1240234375\n",
      "Step 135 -- Train loss: 2.6588706970214844, Train Acc: 0.11767578125 Test Acc: 0.1201171875\n",
      "Step 136 -- Train loss: 2.6586754322052, Train Acc: 0.118896484375 Test Acc: 0.1103515625\n",
      "Step 137 -- Train loss: 2.657719373703003, Train Acc: 0.119140625 Test Acc: 0.1064453125\n",
      "Step 138 -- Train loss: 2.6576101779937744, Train Acc: 0.123291015625 Test Acc: 0.1142578125\n",
      "Step 139 -- Train loss: 2.658151865005493, Train Acc: 0.11669921875 Test Acc: 0.11328125\n",
      "Step 140 -- Train loss: 2.6595351696014404, Train Acc: 0.11474609375 Test Acc: 0.119140625\n",
      "Step 141 -- Train loss: 2.658268928527832, Train Acc: 0.117919921875 Test Acc: 0.130859375\n",
      "Step 142 -- Train loss: 2.65790057182312, Train Acc: 0.121337890625 Test Acc: 0.1142578125\n",
      "Step 143 -- Train loss: 2.6586697101593018, Train Acc: 0.11572265625 Test Acc: 0.134765625\n",
      "Step 144 -- Train loss: 2.658191680908203, Train Acc: 0.117431640625 Test Acc: 0.103515625\n",
      "Step 145 -- Train loss: 2.6583478450775146, Train Acc: 0.122802734375 Test Acc: 0.123046875\n",
      "Step 146 -- Train loss: 2.6573028564453125, Train Acc: 0.123291015625 Test Acc: 0.1240234375\n",
      "Step 147 -- Train loss: 2.6595983505249023, Train Acc: 0.12158203125 Test Acc: 0.126953125\n",
      "Step 148 -- Train loss: 2.657336711883545, Train Acc: 0.12451171875 Test Acc: 0.12890625\n",
      "Step 149 -- Train loss: 2.657930850982666, Train Acc: 0.119384765625 Test Acc: 0.123046875\n",
      "Step 150 -- Train loss: 2.6584010124206543, Train Acc: 0.12060546875 Test Acc: 0.1396484375\n",
      "Step 151 -- Train loss: 2.657197952270508, Train Acc: 0.11572265625 Test Acc: 0.126953125\n",
      "Step 152 -- Train loss: 2.656299352645874, Train Acc: 0.124267578125 Test Acc: 0.1279296875\n",
      "Step 153 -- Train loss: 2.657238006591797, Train Acc: 0.119384765625 Test Acc: 0.1142578125\n",
      "Step 154 -- Train loss: 2.6583924293518066, Train Acc: 0.12158203125 Test Acc: 0.111328125\n",
      "Step 155 -- Train loss: 2.6585774421691895, Train Acc: 0.114990234375 Test Acc: 0.1220703125\n",
      "Step 156 -- Train loss: 2.658627510070801, Train Acc: 0.11572265625 Test Acc: 0.12109375\n",
      "Step 157 -- Train loss: 2.659430980682373, Train Acc: 0.114990234375 Test Acc: 0.119140625\n",
      "Step 158 -- Train loss: 2.658506155014038, Train Acc: 0.116943359375 Test Acc: 0.1171875\n",
      "Step 159 -- Train loss: 2.6591453552246094, Train Acc: 0.1162109375 Test Acc: 0.125\n",
      "Step 160 -- Train loss: 2.657395839691162, Train Acc: 0.12158203125 Test Acc: 0.119140625\n",
      "Step 161 -- Train loss: 2.6582868099212646, Train Acc: 0.1181640625 Test Acc: 0.11328125\n",
      "Step 162 -- Train loss: 2.6580843925476074, Train Acc: 0.118408203125 Test Acc: 0.1044921875\n",
      "Step 163 -- Train loss: 2.6585540771484375, Train Acc: 0.11279296875 Test Acc: 0.1171875\n",
      "Step 164 -- Train loss: 2.6566483974456787, Train Acc: 0.114990234375 Test Acc: 0.1220703125\n",
      "Step 165 -- Train loss: 2.6577377319335938, Train Acc: 0.114501953125 Test Acc: 0.1220703125\n",
      "Step 166 -- Train loss: 2.658074378967285, Train Acc: 0.115234375 Test Acc: 0.1171875\n",
      "Step 167 -- Train loss: 2.6554479598999023, Train Acc: 0.121337890625 Test Acc: 0.1201171875\n",
      "Step 168 -- Train loss: 2.6583244800567627, Train Acc: 0.117431640625 Test Acc: 0.1181640625\n",
      "Step 169 -- Train loss: 2.658221483230591, Train Acc: 0.121826171875 Test Acc: 0.1220703125\n",
      "Step 170 -- Train loss: 2.658651828765869, Train Acc: 0.11669921875 Test Acc: 0.1201171875\n",
      "Step 171 -- Train loss: 2.6567344665527344, Train Acc: 0.120361328125 Test Acc: 0.126953125\n",
      "Step 172 -- Train loss: 2.6576459407806396, Train Acc: 0.118408203125 Test Acc: 0.126953125\n",
      "Step 173 -- Train loss: 2.659550666809082, Train Acc: 0.10986328125 Test Acc: 0.1083984375\n",
      "Step 174 -- Train loss: 2.6571455001831055, Train Acc: 0.11328125 Test Acc: 0.1181640625\n",
      "Step 175 -- Train loss: 2.656695604324341, Train Acc: 0.122314453125 Test Acc: 0.115234375\n",
      "Step 176 -- Train loss: 2.657714605331421, Train Acc: 0.119384765625 Test Acc: 0.109375\n",
      "Step 177 -- Train loss: 2.6587724685668945, Train Acc: 0.12451171875 Test Acc: 0.1083984375\n",
      "Step 178 -- Train loss: 2.6591997146606445, Train Acc: 0.11376953125 Test Acc: 0.1103515625\n",
      "Step 179 -- Train loss: 2.6585240364074707, Train Acc: 0.109375 Test Acc: 0.1142578125\n",
      "Step 180 -- Train loss: 2.658703565597534, Train Acc: 0.111083984375 Test Acc: 0.123046875\n",
      "Step 181 -- Train loss: 2.6570169925689697, Train Acc: 0.122802734375 Test Acc: 0.1181640625\n",
      "Step 182 -- Train loss: 2.6573448181152344, Train Acc: 0.12451171875 Test Acc: 0.1181640625\n",
      "Step 183 -- Train loss: 2.6586883068084717, Train Acc: 0.11767578125 Test Acc: 0.103515625\n",
      "Step 184 -- Train loss: 2.6593151092529297, Train Acc: 0.12158203125 Test Acc: 0.115234375\n",
      "Step 185 -- Train loss: 2.658205270767212, Train Acc: 0.12158203125 Test Acc: 0.1181640625\n",
      "Step 186 -- Train loss: 2.6582181453704834, Train Acc: 0.115234375 Test Acc: 0.1201171875\n",
      "Step 187 -- Train loss: 2.6575815677642822, Train Acc: 0.118896484375 Test Acc: 0.1220703125\n",
      "Step 188 -- Train loss: 2.6576790809631348, Train Acc: 0.124267578125 Test Acc: 0.11328125\n",
      "Step 189 -- Train loss: 2.660555601119995, Train Acc: 0.11181640625 Test Acc: 0.119140625\n",
      "Step 190 -- Train loss: 2.65895676612854, Train Acc: 0.11669921875 Test Acc: 0.1123046875\n",
      "Step 191 -- Train loss: 2.659059762954712, Train Acc: 0.11083984375 Test Acc: 0.107421875\n",
      "Step 192 -- Train loss: 2.6584677696228027, Train Acc: 0.119873046875 Test Acc: 0.1123046875\n",
      "Step 193 -- Train loss: 2.656581401824951, Train Acc: 0.124267578125 Test Acc: 0.1171875\n",
      "Step 194 -- Train loss: 2.6581757068634033, Train Acc: 0.11767578125 Test Acc: 0.123046875\n",
      "Step 195 -- Train loss: 2.6577887535095215, Train Acc: 0.123046875 Test Acc: 0.1259765625\n",
      "Step 196 -- Train loss: 2.6586191654205322, Train Acc: 0.120849609375 Test Acc: 0.1171875\n",
      "Step 197 -- Train loss: 2.658724546432495, Train Acc: 0.116943359375 Test Acc: 0.1181640625\n",
      "Step 198 -- Train loss: 2.658287763595581, Train Acc: 0.116455078125 Test Acc: 0.1171875\n",
      "Step 199 -- Train loss: 2.655684471130371, Train Acc: 0.126220703125 Test Acc: 0.12109375\n",
      "Step 200 -- Train loss: 2.656770706176758, Train Acc: 0.111083984375 Test Acc: 0.1142578125\n",
      "Step 201 -- Train loss: 2.6590118408203125, Train Acc: 0.12060546875 Test Acc: 0.1044921875\n",
      "Step 202 -- Train loss: 2.658398151397705, Train Acc: 0.11865234375 Test Acc: 0.1201171875\n",
      "Step 203 -- Train loss: 2.6564884185791016, Train Acc: 0.1181640625 Test Acc: 0.12109375\n",
      "Step 204 -- Train loss: 2.6573433876037598, Train Acc: 0.113525390625 Test Acc: 0.1103515625\n",
      "Step 205 -- Train loss: 2.6589279174804688, Train Acc: 0.1220703125 Test Acc: 0.1123046875\n",
      "Step 206 -- Train loss: 2.656869411468506, Train Acc: 0.119384765625 Test Acc: 0.1220703125\n",
      "Step 207 -- Train loss: 2.656773090362549, Train Acc: 0.12060546875 Test Acc: 0.1259765625\n",
      "Step 208 -- Train loss: 2.656968116760254, Train Acc: 0.12255859375 Test Acc: 0.1142578125\n",
      "Step 209 -- Train loss: 2.6585745811462402, Train Acc: 0.114013671875 Test Acc: 0.1123046875\n",
      "Step 210 -- Train loss: 2.659045934677124, Train Acc: 0.1123046875 Test Acc: 0.1103515625\n",
      "Step 211 -- Train loss: 2.6588735580444336, Train Acc: 0.116943359375 Test Acc: 0.115234375\n",
      "Step 212 -- Train loss: 2.6569080352783203, Train Acc: 0.11474609375 Test Acc: 0.1142578125\n",
      "Step 213 -- Train loss: 2.6586530208587646, Train Acc: 0.116943359375 Test Acc: 0.1103515625\n",
      "Step 214 -- Train loss: 2.6558735370635986, Train Acc: 0.122314453125 Test Acc: 0.11328125\n",
      "Step 215 -- Train loss: 2.657409906387329, Train Acc: 0.114013671875 Test Acc: 0.1123046875\n",
      "Step 216 -- Train loss: 2.657132625579834, Train Acc: 0.122802734375 Test Acc: 0.1171875\n",
      "Step 217 -- Train loss: 2.65787410736084, Train Acc: 0.120361328125 Test Acc: 0.1064453125\n",
      "Step 218 -- Train loss: 2.6581811904907227, Train Acc: 0.1181640625 Test Acc: 0.1142578125\n",
      "Step 219 -- Train loss: 2.658884286880493, Train Acc: 0.117431640625 Test Acc: 0.1328125\n",
      "Step 220 -- Train loss: 2.657806158065796, Train Acc: 0.115234375 Test Acc: 0.103515625\n",
      "Step 221 -- Train loss: 2.657625198364258, Train Acc: 0.110107421875 Test Acc: 0.126953125\n",
      "Step 222 -- Train loss: 2.656689405441284, Train Acc: 0.118896484375 Test Acc: 0.12890625\n",
      "Step 223 -- Train loss: 2.658090829849243, Train Acc: 0.11572265625 Test Acc: 0.1142578125\n",
      "Step 224 -- Train loss: 2.6589345932006836, Train Acc: 0.119873046875 Test Acc: 0.1181640625\n",
      "Step 225 -- Train loss: 2.6567869186401367, Train Acc: 0.115478515625 Test Acc: 0.11328125\n",
      "Step 226 -- Train loss: 2.6586453914642334, Train Acc: 0.123779296875 Test Acc: 0.119140625\n",
      "Step 227 -- Train loss: 2.6559672355651855, Train Acc: 0.1240234375 Test Acc: 0.12890625\n",
      "Step 228 -- Train loss: 2.657041311264038, Train Acc: 0.122314453125 Test Acc: 0.1123046875\n",
      "Step 229 -- Train loss: 2.6586642265319824, Train Acc: 0.117919921875 Test Acc: 0.12109375\n",
      "Step 230 -- Train loss: 2.6600728034973145, Train Acc: 0.118408203125 Test Acc: 0.1181640625\n",
      "Step 231 -- Train loss: 2.6571383476257324, Train Acc: 0.118896484375 Test Acc: 0.1123046875\n",
      "Step 232 -- Train loss: 2.658721923828125, Train Acc: 0.115478515625 Test Acc: 0.1123046875\n",
      "Step 233 -- Train loss: 2.6553900241851807, Train Acc: 0.129150390625 Test Acc: 0.11328125\n",
      "Step 234 -- Train loss: 2.658015727996826, Train Acc: 0.11669921875 Test Acc: 0.1279296875\n",
      "Step 235 -- Train loss: 2.657735824584961, Train Acc: 0.12255859375 Test Acc: 0.115234375\n",
      "Step 236 -- Train loss: 2.6587865352630615, Train Acc: 0.116455078125 Test Acc: 0.119140625\n",
      "Step 237 -- Train loss: 2.657639265060425, Train Acc: 0.118408203125 Test Acc: 0.1181640625\n",
      "Step 238 -- Train loss: 2.657827138900757, Train Acc: 0.12158203125 Test Acc: 0.1103515625\n",
      "Step 239 -- Train loss: 2.6575355529785156, Train Acc: 0.116455078125 Test Acc: 0.1259765625\n",
      "Step 240 -- Train loss: 2.6560990810394287, Train Acc: 0.1220703125 Test Acc: 0.1083984375\n",
      "Step 241 -- Train loss: 2.656920909881592, Train Acc: 0.113525390625 Test Acc: 0.1083984375\n",
      "Step 242 -- Train loss: 2.658008098602295, Train Acc: 0.1279296875 Test Acc: 0.1162109375\n",
      "Step 243 -- Train loss: 2.6575119495391846, Train Acc: 0.1220703125 Test Acc: 0.111328125\n",
      "Step 244 -- Train loss: 2.657026767730713, Train Acc: 0.118896484375 Test Acc: 0.123046875\n",
      "Step 245 -- Train loss: 2.656306028366089, Train Acc: 0.122314453125 Test Acc: 0.1142578125\n",
      "Step 246 -- Train loss: 2.6577062606811523, Train Acc: 0.121826171875 Test Acc: 0.1015625\n",
      "Step 247 -- Train loss: 2.656046152114868, Train Acc: 0.121337890625 Test Acc: 0.1259765625\n",
      "Step 248 -- Train loss: 2.657784938812256, Train Acc: 0.12353515625 Test Acc: 0.12109375\n",
      "Step 249 -- Train loss: 2.6580138206481934, Train Acc: 0.1162109375 Test Acc: 0.1162109375\n",
      "Step 250 -- Train loss: 2.6569125652313232, Train Acc: 0.117431640625 Test Acc: 0.1181640625\n",
      "Step 251 -- Train loss: 2.6581146717071533, Train Acc: 0.115234375 Test Acc: 0.1220703125\n",
      "Step 252 -- Train loss: 2.657243013381958, Train Acc: 0.124267578125 Test Acc: 0.1083984375\n",
      "Step 253 -- Train loss: 2.6595897674560547, Train Acc: 0.115234375 Test Acc: 0.119140625\n",
      "Step 254 -- Train loss: 2.658442497253418, Train Acc: 0.11328125 Test Acc: 0.1123046875\n",
      "Step 255 -- Train loss: 2.6573615074157715, Train Acc: 0.114501953125 Test Acc: 0.130859375\n",
      "Step 256 -- Train loss: 2.6571712493896484, Train Acc: 0.118408203125 Test Acc: 0.1220703125\n",
      "Step 257 -- Train loss: 2.6581993103027344, Train Acc: 0.115966796875 Test Acc: 0.111328125\n",
      "Step 258 -- Train loss: 2.6584370136260986, Train Acc: 0.11962890625 Test Acc: 0.12109375\n",
      "Step 259 -- Train loss: 2.6585869789123535, Train Acc: 0.119873046875 Test Acc: 0.123046875\n",
      "Step 260 -- Train loss: 2.6589059829711914, Train Acc: 0.110107421875 Test Acc: 0.115234375\n",
      "Step 261 -- Train loss: 2.6575632095336914, Train Acc: 0.116455078125 Test Acc: 0.107421875\n",
      "Step 262 -- Train loss: 2.658055305480957, Train Acc: 0.1162109375 Test Acc: 0.119140625\n",
      "Step 263 -- Train loss: 2.6585211753845215, Train Acc: 0.111083984375 Test Acc: 0.1201171875\n",
      "Step 264 -- Train loss: 2.6573901176452637, Train Acc: 0.11865234375 Test Acc: 0.0966796875\n",
      "Step 265 -- Train loss: 2.6562552452087402, Train Acc: 0.12060546875 Test Acc: 0.1240234375\n",
      "Step 266 -- Train loss: 2.6579015254974365, Train Acc: 0.121826171875 Test Acc: 0.1162109375\n",
      "Step 267 -- Train loss: 2.6580796241760254, Train Acc: 0.121826171875 Test Acc: 0.1083984375\n",
      "Step 268 -- Train loss: 2.6576805114746094, Train Acc: 0.122802734375 Test Acc: 0.1142578125\n",
      "Step 269 -- Train loss: 2.6588640213012695, Train Acc: 0.118408203125 Test Acc: 0.1220703125\n",
      "Step 270 -- Train loss: 2.6566109657287598, Train Acc: 0.12255859375 Test Acc: 0.111328125\n",
      "Step 271 -- Train loss: 2.657745122909546, Train Acc: 0.120849609375 Test Acc: 0.126953125\n",
      "Step 272 -- Train loss: 2.6574413776397705, Train Acc: 0.117431640625 Test Acc: 0.10546875\n",
      "Step 273 -- Train loss: 2.6585757732391357, Train Acc: 0.123046875 Test Acc: 0.1142578125\n",
      "Step 274 -- Train loss: 2.6581878662109375, Train Acc: 0.12060546875 Test Acc: 0.125\n",
      "Step 275 -- Train loss: 2.6575465202331543, Train Acc: 0.12451171875 Test Acc: 0.123046875\n",
      "Step 276 -- Train loss: 2.659123182296753, Train Acc: 0.1181640625 Test Acc: 0.1025390625\n",
      "Step 277 -- Train loss: 2.6555941104888916, Train Acc: 0.113525390625 Test Acc: 0.1171875\n",
      "Step 278 -- Train loss: 2.6570816040039062, Train Acc: 0.1240234375 Test Acc: 0.111328125\n",
      "Step 279 -- Train loss: 2.6566712856292725, Train Acc: 0.119873046875 Test Acc: 0.123046875\n",
      "Step 280 -- Train loss: 2.658545732498169, Train Acc: 0.117431640625 Test Acc: 0.123046875\n",
      "Step 281 -- Train loss: 2.657585859298706, Train Acc: 0.11572265625 Test Acc: 0.11328125\n",
      "Step 282 -- Train loss: 2.6576662063598633, Train Acc: 0.119384765625 Test Acc: 0.1171875\n",
      "Step 283 -- Train loss: 2.65647554397583, Train Acc: 0.1220703125 Test Acc: 0.1181640625\n",
      "Step 284 -- Train loss: 2.6563379764556885, Train Acc: 0.125732421875 Test Acc: 0.1015625\n",
      "Step 285 -- Train loss: 2.6574974060058594, Train Acc: 0.112548828125 Test Acc: 0.1142578125\n",
      "Step 286 -- Train loss: 2.6583290100097656, Train Acc: 0.11865234375 Test Acc: 0.095703125\n",
      "Step 287 -- Train loss: 2.6577610969543457, Train Acc: 0.121826171875 Test Acc: 0.123046875\n",
      "Step 288 -- Train loss: 2.6578469276428223, Train Acc: 0.121826171875 Test Acc: 0.1240234375\n",
      "Step 289 -- Train loss: 2.6568100452423096, Train Acc: 0.118408203125 Test Acc: 0.1240234375\n",
      "Step 290 -- Train loss: 2.6597139835357666, Train Acc: 0.116943359375 Test Acc: 0.1220703125\n",
      "Step 291 -- Train loss: 2.655313730239868, Train Acc: 0.125 Test Acc: 0.1103515625\n",
      "Step 292 -- Train loss: 2.6569035053253174, Train Acc: 0.1201171875 Test Acc: 0.1142578125\n",
      "Step 293 -- Train loss: 2.657644033432007, Train Acc: 0.123779296875 Test Acc: 0.119140625\n",
      "Step 294 -- Train loss: 2.6566925048828125, Train Acc: 0.11669921875 Test Acc: 0.1220703125\n",
      "Step 295 -- Train loss: 2.657179355621338, Train Acc: 0.115966796875 Test Acc: 0.1220703125\n",
      "Step 296 -- Train loss: 2.657867908477783, Train Acc: 0.1142578125 Test Acc: 0.1181640625\n",
      "Step 297 -- Train loss: 2.6579537391662598, Train Acc: 0.116943359375 Test Acc: 0.1220703125\n",
      "Step 298 -- Train loss: 2.658372163772583, Train Acc: 0.120849609375 Test Acc: 0.1337890625\n",
      "Step 299 -- Train loss: 2.6578776836395264, Train Acc: 0.115234375 Test Acc: 0.123046875\n",
      "Step 300 -- Train loss: 2.6581380367279053, Train Acc: 0.114990234375 Test Acc: 0.1171875\n",
      "Step 301 -- Train loss: 2.6563141345977783, Train Acc: 0.12353515625 Test Acc: 0.11328125\n",
      "Step 302 -- Train loss: 2.6584432125091553, Train Acc: 0.1171875 Test Acc: 0.123046875\n",
      "Step 303 -- Train loss: 2.656508207321167, Train Acc: 0.1240234375 Test Acc: 0.125\n",
      "Step 304 -- Train loss: 2.6571693420410156, Train Acc: 0.123046875 Test Acc: 0.11328125\n",
      "Step 305 -- Train loss: 2.6583142280578613, Train Acc: 0.11962890625 Test Acc: 0.11328125\n",
      "Step 306 -- Train loss: 2.6571314334869385, Train Acc: 0.11083984375 Test Acc: 0.125\n",
      "Step 307 -- Train loss: 2.658144474029541, Train Acc: 0.118896484375 Test Acc: 0.1181640625\n",
      "Step 308 -- Train loss: 2.658167839050293, Train Acc: 0.117431640625 Test Acc: 0.126953125\n",
      "Step 309 -- Train loss: 2.657639503479004, Train Acc: 0.116943359375 Test Acc: 0.12109375\n",
      "Step 310 -- Train loss: 2.658041477203369, Train Acc: 0.118408203125 Test Acc: 0.109375\n",
      "Step 311 -- Train loss: 2.6589815616607666, Train Acc: 0.1162109375 Test Acc: 0.1259765625\n",
      "Step 312 -- Train loss: 2.657672166824341, Train Acc: 0.12158203125 Test Acc: 0.1259765625\n",
      "Step 313 -- Train loss: 2.6573991775512695, Train Acc: 0.119140625 Test Acc: 0.1240234375\n",
      "Step 314 -- Train loss: 2.656606674194336, Train Acc: 0.119873046875 Test Acc: 0.115234375\n",
      "Step 315 -- Train loss: 2.6556475162506104, Train Acc: 0.13037109375 Test Acc: 0.1171875\n",
      "Step 316 -- Train loss: 2.6586196422576904, Train Acc: 0.117431640625 Test Acc: 0.1181640625\n",
      "Step 317 -- Train loss: 2.65751314163208, Train Acc: 0.12109375 Test Acc: 0.1181640625\n",
      "Step 318 -- Train loss: 2.6564722061157227, Train Acc: 0.118896484375 Test Acc: 0.107421875\n",
      "Step 319 -- Train loss: 2.656602144241333, Train Acc: 0.114990234375 Test Acc: 0.1240234375\n",
      "Step 320 -- Train loss: 2.6563076972961426, Train Acc: 0.113037109375 Test Acc: 0.1103515625\n",
      "Step 321 -- Train loss: 2.6569106578826904, Train Acc: 0.123779296875 Test Acc: 0.12109375\n",
      "Step 322 -- Train loss: 2.6576907634735107, Train Acc: 0.111083984375 Test Acc: 0.115234375\n",
      "Step 323 -- Train loss: 2.6566600799560547, Train Acc: 0.114990234375 Test Acc: 0.12890625\n",
      "Step 324 -- Train loss: 2.6557393074035645, Train Acc: 0.119873046875 Test Acc: 0.1279296875\n",
      "Step 325 -- Train loss: 2.6577048301696777, Train Acc: 0.118896484375 Test Acc: 0.1220703125\n",
      "Step 326 -- Train loss: 2.657308578491211, Train Acc: 0.115966796875 Test Acc: 0.1240234375\n",
      "Step 327 -- Train loss: 2.657900810241699, Train Acc: 0.11669921875 Test Acc: 0.1279296875\n",
      "Step 328 -- Train loss: 2.658414363861084, Train Acc: 0.11572265625 Test Acc: 0.107421875\n",
      "Step 329 -- Train loss: 2.6577446460723877, Train Acc: 0.120361328125 Test Acc: 0.115234375\n",
      "Step 330 -- Train loss: 2.6576812267303467, Train Acc: 0.1171875 Test Acc: 0.1103515625\n",
      "Step 331 -- Train loss: 2.656956434249878, Train Acc: 0.117919921875 Test Acc: 0.10546875\n",
      "Step 332 -- Train loss: 2.65674090385437, Train Acc: 0.117431640625 Test Acc: 0.119140625\n",
      "Step 333 -- Train loss: 2.6568763256073, Train Acc: 0.11474609375 Test Acc: 0.1142578125\n",
      "Step 334 -- Train loss: 2.6564090251922607, Train Acc: 0.114990234375 Test Acc: 0.1279296875\n",
      "Step 335 -- Train loss: 2.6573448181152344, Train Acc: 0.115966796875 Test Acc: 0.125\n",
      "Step 336 -- Train loss: 2.6583290100097656, Train Acc: 0.114013671875 Test Acc: 0.119140625\n",
      "Step 337 -- Train loss: 2.6578965187072754, Train Acc: 0.1201171875 Test Acc: 0.123046875\n",
      "Step 338 -- Train loss: 2.6567025184631348, Train Acc: 0.1123046875 Test Acc: 0.12109375\n",
      "Step 339 -- Train loss: 2.6575183868408203, Train Acc: 0.117919921875 Test Acc: 0.109375\n",
      "Step 340 -- Train loss: 2.6565046310424805, Train Acc: 0.1259765625 Test Acc: 0.1171875\n",
      "Step 341 -- Train loss: 2.656219482421875, Train Acc: 0.1171875 Test Acc: 0.1142578125\n",
      "Step 342 -- Train loss: 2.65836238861084, Train Acc: 0.11669921875 Test Acc: 0.1328125\n",
      "Step 343 -- Train loss: 2.658395290374756, Train Acc: 0.12109375 Test Acc: 0.12890625\n",
      "Step 344 -- Train loss: 2.657421827316284, Train Acc: 0.120849609375 Test Acc: 0.1142578125\n",
      "Step 345 -- Train loss: 2.6578011512756348, Train Acc: 0.11083984375 Test Acc: 0.111328125\n",
      "Step 346 -- Train loss: 2.6570448875427246, Train Acc: 0.124267578125 Test Acc: 0.1103515625\n",
      "Step 347 -- Train loss: 2.6574478149414062, Train Acc: 0.121826171875 Test Acc: 0.1181640625\n",
      "Step 348 -- Train loss: 2.657062530517578, Train Acc: 0.116455078125 Test Acc: 0.11328125\n",
      "Step 349 -- Train loss: 2.6581459045410156, Train Acc: 0.11181640625 Test Acc: 0.130859375\n",
      "Step 350 -- Train loss: 2.6590960025787354, Train Acc: 0.118896484375 Test Acc: 0.1142578125\n",
      "Step 351 -- Train loss: 2.657573938369751, Train Acc: 0.11865234375 Test Acc: 0.1181640625\n",
      "Step 352 -- Train loss: 2.6586670875549316, Train Acc: 0.115478515625 Test Acc: 0.1181640625\n",
      "Step 353 -- Train loss: 2.655653238296509, Train Acc: 0.119140625 Test Acc: 0.1201171875\n",
      "Step 354 -- Train loss: 2.65848970413208, Train Acc: 0.1181640625 Test Acc: 0.119140625\n",
      "Step 355 -- Train loss: 2.6576128005981445, Train Acc: 0.11962890625 Test Acc: 0.11328125\n",
      "Step 356 -- Train loss: 2.657773494720459, Train Acc: 0.121826171875 Test Acc: 0.11328125\n",
      "Step 357 -- Train loss: 2.657688856124878, Train Acc: 0.114013671875 Test Acc: 0.119140625\n",
      "Step 358 -- Train loss: 2.657240152359009, Train Acc: 0.117431640625 Test Acc: 0.1162109375\n",
      "Step 359 -- Train loss: 2.6573190689086914, Train Acc: 0.1220703125 Test Acc: 0.111328125\n",
      "Step 360 -- Train loss: 2.6562604904174805, Train Acc: 0.122802734375 Test Acc: 0.123046875\n",
      "Step 361 -- Train loss: 2.6563479900360107, Train Acc: 0.123779296875 Test Acc: 0.1240234375\n",
      "Step 362 -- Train loss: 2.6563425064086914, Train Acc: 0.1220703125 Test Acc: 0.1162109375\n",
      "Step 363 -- Train loss: 2.657782554626465, Train Acc: 0.11962890625 Test Acc: 0.123046875\n",
      "Step 364 -- Train loss: 2.657968759536743, Train Acc: 0.114990234375 Test Acc: 0.115234375\n",
      "Step 365 -- Train loss: 2.6559653282165527, Train Acc: 0.115478515625 Test Acc: 0.1123046875\n",
      "Step 366 -- Train loss: 2.658834934234619, Train Acc: 0.11669921875 Test Acc: 0.1162109375\n",
      "Step 367 -- Train loss: 2.6572203636169434, Train Acc: 0.119384765625 Test Acc: 0.1123046875\n",
      "Step 368 -- Train loss: 2.6576638221740723, Train Acc: 0.1142578125 Test Acc: 0.1259765625\n",
      "Step 369 -- Train loss: 2.657362222671509, Train Acc: 0.120849609375 Test Acc: 0.11328125\n",
      "Step 370 -- Train loss: 2.658781051635742, Train Acc: 0.11865234375 Test Acc: 0.109375\n",
      "Step 371 -- Train loss: 2.655437707901001, Train Acc: 0.1201171875 Test Acc: 0.12109375\n",
      "Step 372 -- Train loss: 2.661029815673828, Train Acc: 0.119384765625 Test Acc: 0.12890625\n",
      "Step 373 -- Train loss: 2.66020131111145, Train Acc: 0.1142578125 Test Acc: 0.115234375\n",
      "Step 374 -- Train loss: 2.6568994522094727, Train Acc: 0.12109375 Test Acc: 0.1220703125\n",
      "Step 375 -- Train loss: 2.657195568084717, Train Acc: 0.119384765625 Test Acc: 0.115234375\n",
      "Step 376 -- Train loss: 2.6594302654266357, Train Acc: 0.114501953125 Test Acc: 0.11328125\n",
      "Step 377 -- Train loss: 2.6585471630096436, Train Acc: 0.113037109375 Test Acc: 0.1123046875\n",
      "Step 378 -- Train loss: 2.6574182510375977, Train Acc: 0.115478515625 Test Acc: 0.1240234375\n",
      "Step 379 -- Train loss: 2.6593801975250244, Train Acc: 0.119384765625 Test Acc: 0.1298828125\n",
      "Step 380 -- Train loss: 2.658189058303833, Train Acc: 0.115234375 Test Acc: 0.1083984375\n",
      "Step 381 -- Train loss: 2.657027244567871, Train Acc: 0.114013671875 Test Acc: 0.1103515625\n",
      "Step 382 -- Train loss: 2.6556012630462646, Train Acc: 0.115966796875 Test Acc: 0.1103515625\n",
      "Step 383 -- Train loss: 2.657355308532715, Train Acc: 0.119140625 Test Acc: 0.12109375\n",
      "Step 384 -- Train loss: 2.6570239067077637, Train Acc: 0.121337890625 Test Acc: 0.123046875\n",
      "Step 385 -- Train loss: 2.659804344177246, Train Acc: 0.116455078125 Test Acc: 0.1142578125\n",
      "Step 386 -- Train loss: 2.658745050430298, Train Acc: 0.1201171875 Test Acc: 0.1103515625\n",
      "Step 387 -- Train loss: 2.658691167831421, Train Acc: 0.1171875 Test Acc: 0.119140625\n",
      "Step 388 -- Train loss: 2.657935380935669, Train Acc: 0.117919921875 Test Acc: 0.12109375\n",
      "Step 389 -- Train loss: 2.656449556350708, Train Acc: 0.12451171875 Test Acc: 0.126953125\n",
      "Step 390 -- Train loss: 2.6571803092956543, Train Acc: 0.1171875 Test Acc: 0.119140625\n",
      "Step 391 -- Train loss: 2.6589365005493164, Train Acc: 0.11279296875 Test Acc: 0.1171875\n",
      "Step 392 -- Train loss: 2.6589467525482178, Train Acc: 0.119140625 Test Acc: 0.1171875\n",
      "Step 393 -- Train loss: 2.6570379734039307, Train Acc: 0.11767578125 Test Acc: 0.1220703125\n",
      "Step 394 -- Train loss: 2.6579437255859375, Train Acc: 0.111083984375 Test Acc: 0.109375\n",
      "Step 395 -- Train loss: 2.656808376312256, Train Acc: 0.11669921875 Test Acc: 0.109375\n",
      "Step 396 -- Train loss: 2.657168388366699, Train Acc: 0.125 Test Acc: 0.125\n",
      "Step 397 -- Train loss: 2.6572582721710205, Train Acc: 0.114013671875 Test Acc: 0.109375\n",
      "Step 398 -- Train loss: 2.6564958095550537, Train Acc: 0.12255859375 Test Acc: 0.1142578125\n",
      "Step 399 -- Train loss: 2.657155990600586, Train Acc: 0.119384765625 Test Acc: 0.1123046875\n",
      "Step 400 -- Train loss: 2.6566290855407715, Train Acc: 0.12353515625 Test Acc: 0.10546875\n",
      "Step 401 -- Train loss: 2.657588005065918, Train Acc: 0.118896484375 Test Acc: 0.1240234375\n",
      "Step 402 -- Train loss: 2.6567838191986084, Train Acc: 0.117431640625 Test Acc: 0.1181640625\n",
      "Step 403 -- Train loss: 2.656853675842285, Train Acc: 0.120849609375 Test Acc: 0.1142578125\n",
      "Step 404 -- Train loss: 2.6570370197296143, Train Acc: 0.125732421875 Test Acc: 0.1220703125\n",
      "Step 405 -- Train loss: 2.6564278602600098, Train Acc: 0.1162109375 Test Acc: 0.1201171875\n",
      "Step 406 -- Train loss: 2.658188581466675, Train Acc: 0.119140625 Test Acc: 0.12890625\n",
      "Step 407 -- Train loss: 2.657827615737915, Train Acc: 0.115478515625 Test Acc: 0.1142578125\n",
      "Step 408 -- Train loss: 2.6559746265411377, Train Acc: 0.115234375 Test Acc: 0.1201171875\n",
      "Step 409 -- Train loss: 2.656609535217285, Train Acc: 0.121337890625 Test Acc: 0.1201171875\n",
      "Step 410 -- Train loss: 2.658114433288574, Train Acc: 0.1162109375 Test Acc: 0.1103515625\n",
      "Step 411 -- Train loss: 2.658034563064575, Train Acc: 0.115966796875 Test Acc: 0.1162109375\n",
      "Step 412 -- Train loss: 2.6572699546813965, Train Acc: 0.112060546875 Test Acc: 0.1171875\n",
      "Step 413 -- Train loss: 2.656482219696045, Train Acc: 0.123291015625 Test Acc: 0.1240234375\n",
      "Step 414 -- Train loss: 2.6567182540893555, Train Acc: 0.121337890625 Test Acc: 0.1171875\n",
      "Step 415 -- Train loss: 2.655327558517456, Train Acc: 0.125 Test Acc: 0.1142578125\n",
      "Step 416 -- Train loss: 2.657979965209961, Train Acc: 0.113037109375 Test Acc: 0.1279296875\n",
      "Step 417 -- Train loss: 2.657496929168701, Train Acc: 0.1220703125 Test Acc: 0.1123046875\n",
      "Step 418 -- Train loss: 2.657783031463623, Train Acc: 0.1123046875 Test Acc: 0.1181640625\n",
      "Step 419 -- Train loss: 2.658101797103882, Train Acc: 0.115966796875 Test Acc: 0.1123046875\n",
      "Step 420 -- Train loss: 2.657762050628662, Train Acc: 0.1103515625 Test Acc: 0.1123046875\n",
      "Step 421 -- Train loss: 2.657029628753662, Train Acc: 0.1220703125 Test Acc: 0.1220703125\n",
      "Step 422 -- Train loss: 2.658799171447754, Train Acc: 0.11181640625 Test Acc: 0.1298828125\n",
      "Step 423 -- Train loss: 2.657360553741455, Train Acc: 0.1181640625 Test Acc: 0.1279296875\n",
      "Step 424 -- Train loss: 2.6570839881896973, Train Acc: 0.112548828125 Test Acc: 0.1103515625\n",
      "Step 425 -- Train loss: 2.657121181488037, Train Acc: 0.116455078125 Test Acc: 0.1142578125\n",
      "Step 426 -- Train loss: 2.6583516597747803, Train Acc: 0.11865234375 Test Acc: 0.125\n",
      "Step 427 -- Train loss: 2.658353090286255, Train Acc: 0.1123046875 Test Acc: 0.11328125\n",
      "Step 428 -- Train loss: 2.6557230949401855, Train Acc: 0.13134765625 Test Acc: 0.1142578125\n",
      "Step 429 -- Train loss: 2.656785726547241, Train Acc: 0.116943359375 Test Acc: 0.1279296875\n",
      "Step 430 -- Train loss: 2.6569652557373047, Train Acc: 0.12109375 Test Acc: 0.1298828125\n",
      "Step 431 -- Train loss: 2.657727003097534, Train Acc: 0.11669921875 Test Acc: 0.119140625\n",
      "Step 432 -- Train loss: 2.6571836471557617, Train Acc: 0.12109375 Test Acc: 0.12109375\n",
      "Step 433 -- Train loss: 2.657719373703003, Train Acc: 0.114013671875 Test Acc: 0.1201171875\n",
      "Step 434 -- Train loss: 2.6577672958374023, Train Acc: 0.112548828125 Test Acc: 0.123046875\n",
      "Step 435 -- Train loss: 2.6557109355926514, Train Acc: 0.119384765625 Test Acc: 0.1142578125\n",
      "Step 436 -- Train loss: 2.6556477546691895, Train Acc: 0.11767578125 Test Acc: 0.12109375\n",
      "Step 437 -- Train loss: 2.657719373703003, Train Acc: 0.1162109375 Test Acc: 0.107421875\n",
      "Step 438 -- Train loss: 2.6572675704956055, Train Acc: 0.115234375 Test Acc: 0.1103515625\n",
      "Step 439 -- Train loss: 2.6552789211273193, Train Acc: 0.126220703125 Test Acc: 0.115234375\n",
      "Step 440 -- Train loss: 2.657054901123047, Train Acc: 0.1240234375 Test Acc: 0.12109375\n",
      "Step 441 -- Train loss: 2.655902862548828, Train Acc: 0.125732421875 Test Acc: 0.125\n",
      "Step 442 -- Train loss: 2.656350612640381, Train Acc: 0.125732421875 Test Acc: 0.111328125\n",
      "Step 443 -- Train loss: 2.65792179107666, Train Acc: 0.126220703125 Test Acc: 0.109375\n",
      "Step 444 -- Train loss: 2.6581103801727295, Train Acc: 0.119873046875 Test Acc: 0.1181640625\n",
      "Step 445 -- Train loss: 2.6587893962860107, Train Acc: 0.119140625 Test Acc: 0.12109375\n",
      "Step 446 -- Train loss: 2.6582980155944824, Train Acc: 0.123291015625 Test Acc: 0.1142578125\n",
      "Step 447 -- Train loss: 2.6582212448120117, Train Acc: 0.11962890625 Test Acc: 0.1328125\n",
      "Step 448 -- Train loss: 2.6567203998565674, Train Acc: 0.1162109375 Test Acc: 0.1201171875\n",
      "Step 449 -- Train loss: 2.6580114364624023, Train Acc: 0.1181640625 Test Acc: 0.1171875\n",
      "Step 450 -- Train loss: 2.657562017440796, Train Acc: 0.117919921875 Test Acc: 0.1064453125\n",
      "Step 451 -- Train loss: 2.6585042476654053, Train Acc: 0.115478515625 Test Acc: 0.107421875\n",
      "Step 452 -- Train loss: 2.6558175086975098, Train Acc: 0.11572265625 Test Acc: 0.125\n",
      "Step 453 -- Train loss: 2.657546043395996, Train Acc: 0.116455078125 Test Acc: 0.1181640625\n",
      "Step 454 -- Train loss: 2.6568048000335693, Train Acc: 0.122802734375 Test Acc: 0.1220703125\n",
      "Step 455 -- Train loss: 2.6586973667144775, Train Acc: 0.11376953125 Test Acc: 0.123046875\n",
      "Step 456 -- Train loss: 2.6565909385681152, Train Acc: 0.11474609375 Test Acc: 0.123046875\n",
      "Step 457 -- Train loss: 2.658334732055664, Train Acc: 0.115234375 Test Acc: 0.1240234375\n",
      "Step 458 -- Train loss: 2.658663034439087, Train Acc: 0.117431640625 Test Acc: 0.115234375\n",
      "Step 459 -- Train loss: 2.655177116394043, Train Acc: 0.12548828125 Test Acc: 0.10546875\n",
      "Step 460 -- Train loss: 2.657501220703125, Train Acc: 0.117919921875 Test Acc: 0.1220703125\n",
      "Step 461 -- Train loss: 2.6574456691741943, Train Acc: 0.117431640625 Test Acc: 0.1181640625\n",
      "Step 462 -- Train loss: 2.6570963859558105, Train Acc: 0.121826171875 Test Acc: 0.12109375\n",
      "Step 463 -- Train loss: 2.6578125953674316, Train Acc: 0.123779296875 Test Acc: 0.1201171875\n",
      "Step 464 -- Train loss: 2.656933307647705, Train Acc: 0.127197265625 Test Acc: 0.126953125\n",
      "Step 465 -- Train loss: 2.656418561935425, Train Acc: 0.125 Test Acc: 0.123046875\n",
      "Step 466 -- Train loss: 2.6564037799835205, Train Acc: 0.1259765625 Test Acc: 0.1318359375\n",
      "Step 467 -- Train loss: 2.6573150157928467, Train Acc: 0.123291015625 Test Acc: 0.1337890625\n",
      "Step 468 -- Train loss: 2.6585123538970947, Train Acc: 0.122802734375 Test Acc: 0.1220703125\n",
      "Step 469 -- Train loss: 2.6571803092956543, Train Acc: 0.118896484375 Test Acc: 0.123046875\n",
      "Step 470 -- Train loss: 2.658053398132324, Train Acc: 0.117919921875 Test Acc: 0.1142578125\n",
      "Step 471 -- Train loss: 2.656653881072998, Train Acc: 0.114013671875 Test Acc: 0.12109375\n",
      "Step 472 -- Train loss: 2.657487392425537, Train Acc: 0.11962890625 Test Acc: 0.12109375\n",
      "Step 473 -- Train loss: 2.6569712162017822, Train Acc: 0.118896484375 Test Acc: 0.1171875\n",
      "Step 474 -- Train loss: 2.6574952602386475, Train Acc: 0.11474609375 Test Acc: 0.1201171875\n",
      "Step 475 -- Train loss: 2.658165693283081, Train Acc: 0.11474609375 Test Acc: 0.1220703125\n",
      "Step 476 -- Train loss: 2.657775640487671, Train Acc: 0.120361328125 Test Acc: 0.1181640625\n",
      "Step 477 -- Train loss: 2.6567330360412598, Train Acc: 0.126708984375 Test Acc: 0.12109375\n",
      "Step 478 -- Train loss: 2.6599948406219482, Train Acc: 0.11376953125 Test Acc: 0.11328125\n",
      "Step 479 -- Train loss: 2.6582648754119873, Train Acc: 0.11279296875 Test Acc: 0.1142578125\n",
      "Step 480 -- Train loss: 2.658215284347534, Train Acc: 0.11962890625 Test Acc: 0.11328125\n",
      "Step 481 -- Train loss: 2.6570844650268555, Train Acc: 0.11669921875 Test Acc: 0.12109375\n",
      "Step 482 -- Train loss: 2.656972646713257, Train Acc: 0.12158203125 Test Acc: 0.11328125\n",
      "Step 483 -- Train loss: 2.658149003982544, Train Acc: 0.122802734375 Test Acc: 0.10546875\n",
      "Step 484 -- Train loss: 2.657850742340088, Train Acc: 0.120361328125 Test Acc: 0.1201171875\n",
      "Step 485 -- Train loss: 2.6582064628601074, Train Acc: 0.120849609375 Test Acc: 0.1201171875\n",
      "Step 486 -- Train loss: 2.6586620807647705, Train Acc: 0.1171875 Test Acc: 0.1181640625\n",
      "Step 487 -- Train loss: 2.657357931137085, Train Acc: 0.117919921875 Test Acc: 0.1083984375\n",
      "Step 488 -- Train loss: 2.6573424339294434, Train Acc: 0.124755859375 Test Acc: 0.12109375\n",
      "Step 489 -- Train loss: 2.6568431854248047, Train Acc: 0.119140625 Test Acc: 0.1142578125\n",
      "Step 490 -- Train loss: 2.657362222671509, Train Acc: 0.1220703125 Test Acc: 0.12109375\n",
      "Step 491 -- Train loss: 2.658330202102661, Train Acc: 0.11962890625 Test Acc: 0.1181640625\n",
      "Step 492 -- Train loss: 2.6588549613952637, Train Acc: 0.115234375 Test Acc: 0.11328125\n",
      "Step 493 -- Train loss: 2.658308982849121, Train Acc: 0.115966796875 Test Acc: 0.111328125\n",
      "Step 494 -- Train loss: 2.6570136547088623, Train Acc: 0.119384765625 Test Acc: 0.119140625\n",
      "Step 495 -- Train loss: 2.6553192138671875, Train Acc: 0.125244140625 Test Acc: 0.1162109375\n",
      "Step 496 -- Train loss: 2.657107353210449, Train Acc: 0.1162109375 Test Acc: 0.11328125\n",
      "Step 497 -- Train loss: 2.657696485519409, Train Acc: 0.1201171875 Test Acc: 0.11328125\n",
      "Step 498 -- Train loss: 2.6589813232421875, Train Acc: 0.116943359375 Test Acc: 0.130859375\n",
      "Step 499 -- Train loss: 2.6570847034454346, Train Acc: 0.118896484375 Test Acc: 0.115234375\n",
      "Step 500 -- Train loss: 2.6581270694732666, Train Acc: 0.11865234375 Test Acc: 0.119140625\n",
      "Step 501 -- Train loss: 2.6567494869232178, Train Acc: 0.117431640625 Test Acc: 0.126953125\n",
      "Step 502 -- Train loss: 2.6571178436279297, Train Acc: 0.118896484375 Test Acc: 0.123046875\n",
      "Step 503 -- Train loss: 2.6574320793151855, Train Acc: 0.11962890625 Test Acc: 0.111328125\n",
      "Step 504 -- Train loss: 2.6600406169891357, Train Acc: 0.115234375 Test Acc: 0.1171875\n",
      "Step 505 -- Train loss: 2.657182216644287, Train Acc: 0.114990234375 Test Acc: 0.1357421875\n",
      "Step 506 -- Train loss: 2.6572866439819336, Train Acc: 0.122802734375 Test Acc: 0.1171875\n",
      "Step 507 -- Train loss: 2.6563220024108887, Train Acc: 0.124267578125 Test Acc: 0.1240234375\n",
      "Step 508 -- Train loss: 2.6571006774902344, Train Acc: 0.117919921875 Test Acc: 0.1328125\n",
      "Step 509 -- Train loss: 2.6564433574676514, Train Acc: 0.1220703125 Test Acc: 0.1162109375\n",
      "Step 510 -- Train loss: 2.656717300415039, Train Acc: 0.124267578125 Test Acc: 0.119140625\n",
      "Step 511 -- Train loss: 2.6585793495178223, Train Acc: 0.111328125 Test Acc: 0.12109375\n",
      "Step 512 -- Train loss: 2.6567208766937256, Train Acc: 0.121337890625 Test Acc: 0.111328125\n",
      "Step 513 -- Train loss: 2.6564629077911377, Train Acc: 0.122314453125 Test Acc: 0.1298828125\n",
      "Step 514 -- Train loss: 2.658371925354004, Train Acc: 0.118896484375 Test Acc: 0.1181640625\n",
      "Step 515 -- Train loss: 2.657094717025757, Train Acc: 0.123046875 Test Acc: 0.1201171875\n",
      "Step 516 -- Train loss: 2.656379461288452, Train Acc: 0.1171875 Test Acc: 0.1171875\n",
      "Step 517 -- Train loss: 2.6578333377838135, Train Acc: 0.11865234375 Test Acc: 0.1201171875\n",
      "Step 518 -- Train loss: 2.65728497505188, Train Acc: 0.11865234375 Test Acc: 0.123046875\n",
      "Step 519 -- Train loss: 2.6560657024383545, Train Acc: 0.126220703125 Test Acc: 0.1103515625\n",
      "Step 520 -- Train loss: 2.659398317337036, Train Acc: 0.115478515625 Test Acc: 0.1201171875\n",
      "Step 521 -- Train loss: 2.6579785346984863, Train Acc: 0.1142578125 Test Acc: 0.1220703125\n",
      "Step 522 -- Train loss: 2.6572537422180176, Train Acc: 0.1220703125 Test Acc: 0.1142578125\n",
      "Step 523 -- Train loss: 2.6575095653533936, Train Acc: 0.11669921875 Test Acc: 0.11328125\n",
      "Step 524 -- Train loss: 2.6580922603607178, Train Acc: 0.123779296875 Test Acc: 0.12109375\n",
      "Step 525 -- Train loss: 2.6582727432250977, Train Acc: 0.113525390625 Test Acc: 0.111328125\n",
      "Step 526 -- Train loss: 2.6608099937438965, Train Acc: 0.112060546875 Test Acc: 0.1064453125\n",
      "Step 527 -- Train loss: 2.657979726791382, Train Acc: 0.113037109375 Test Acc: 0.12109375\n",
      "Step 528 -- Train loss: 2.6560821533203125, Train Acc: 0.1259765625 Test Acc: 0.125\n",
      "Step 529 -- Train loss: 2.656982660293579, Train Acc: 0.121826171875 Test Acc: 0.1064453125\n",
      "Step 530 -- Train loss: 2.65852689743042, Train Acc: 0.11767578125 Test Acc: 0.1318359375\n",
      "Step 531 -- Train loss: 2.6571903228759766, Train Acc: 0.117919921875 Test Acc: 0.115234375\n",
      "Step 532 -- Train loss: 2.657900810241699, Train Acc: 0.123779296875 Test Acc: 0.1220703125\n",
      "Step 533 -- Train loss: 2.6581361293792725, Train Acc: 0.120849609375 Test Acc: 0.1259765625\n",
      "Step 534 -- Train loss: 2.656343460083008, Train Acc: 0.1181640625 Test Acc: 0.126953125\n",
      "Step 535 -- Train loss: 2.6573338508605957, Train Acc: 0.11328125 Test Acc: 0.1259765625\n",
      "Step 536 -- Train loss: 2.65608549118042, Train Acc: 0.11865234375 Test Acc: 0.1259765625\n",
      "Step 537 -- Train loss: 2.6590728759765625, Train Acc: 0.111083984375 Test Acc: 0.1005859375\n",
      "Step 538 -- Train loss: 2.657803535461426, Train Acc: 0.11376953125 Test Acc: 0.111328125\n",
      "Step 539 -- Train loss: 2.657224416732788, Train Acc: 0.1142578125 Test Acc: 0.1240234375\n",
      "Step 540 -- Train loss: 2.6568984985351562, Train Acc: 0.11083984375 Test Acc: 0.1318359375\n",
      "Step 541 -- Train loss: 2.656665802001953, Train Acc: 0.121337890625 Test Acc: 0.115234375\n",
      "Step 542 -- Train loss: 2.656925678253174, Train Acc: 0.12158203125 Test Acc: 0.1181640625\n",
      "Step 543 -- Train loss: 2.65675687789917, Train Acc: 0.119384765625 Test Acc: 0.1162109375\n",
      "Step 544 -- Train loss: 2.658964157104492, Train Acc: 0.116943359375 Test Acc: 0.119140625\n",
      "Step 545 -- Train loss: 2.659492015838623, Train Acc: 0.113525390625 Test Acc: 0.12109375\n",
      "Step 546 -- Train loss: 2.6564483642578125, Train Acc: 0.11962890625 Test Acc: 0.119140625\n",
      "Step 547 -- Train loss: 2.65653395652771, Train Acc: 0.119873046875 Test Acc: 0.1162109375\n",
      "Step 548 -- Train loss: 2.6563751697540283, Train Acc: 0.123046875 Test Acc: 0.1240234375\n",
      "Step 549 -- Train loss: 2.657783031463623, Train Acc: 0.111328125 Test Acc: 0.123046875\n",
      "Step 550 -- Train loss: 2.65775990486145, Train Acc: 0.1220703125 Test Acc: 0.125\n",
      "Step 551 -- Train loss: 2.656804084777832, Train Acc: 0.123291015625 Test Acc: 0.1259765625\n",
      "Step 552 -- Train loss: 2.658910036087036, Train Acc: 0.11865234375 Test Acc: 0.123046875\n",
      "Step 553 -- Train loss: 2.656765937805176, Train Acc: 0.123291015625 Test Acc: 0.140625\n",
      "Step 554 -- Train loss: 2.6555895805358887, Train Acc: 0.12548828125 Test Acc: 0.1044921875\n",
      "Step 555 -- Train loss: 2.65815806388855, Train Acc: 0.1162109375 Test Acc: 0.125\n",
      "Step 556 -- Train loss: 2.6560475826263428, Train Acc: 0.125 Test Acc: 0.1181640625\n",
      "Step 557 -- Train loss: 2.658242702484131, Train Acc: 0.11376953125 Test Acc: 0.125\n",
      "Step 558 -- Train loss: 2.6565678119659424, Train Acc: 0.125244140625 Test Acc: 0.11328125\n",
      "Step 559 -- Train loss: 2.6575329303741455, Train Acc: 0.1162109375 Test Acc: 0.125\n",
      "Step 560 -- Train loss: 2.656116008758545, Train Acc: 0.119873046875 Test Acc: 0.1025390625\n",
      "Step 561 -- Train loss: 2.6578633785247803, Train Acc: 0.11865234375 Test Acc: 0.125\n",
      "Step 562 -- Train loss: 2.659106969833374, Train Acc: 0.115966796875 Test Acc: 0.1259765625\n",
      "Step 563 -- Train loss: 2.6589388847351074, Train Acc: 0.120849609375 Test Acc: 0.1201171875\n",
      "Step 564 -- Train loss: 2.6571390628814697, Train Acc: 0.11328125 Test Acc: 0.107421875\n",
      "Step 565 -- Train loss: 2.656376600265503, Train Acc: 0.125244140625 Test Acc: 0.1103515625\n",
      "Step 566 -- Train loss: 2.65800404548645, Train Acc: 0.11962890625 Test Acc: 0.1103515625\n",
      "Step 567 -- Train loss: 2.6579692363739014, Train Acc: 0.11767578125 Test Acc: 0.1171875\n",
      "Step 568 -- Train loss: 2.658214807510376, Train Acc: 0.112548828125 Test Acc: 0.125\n",
      "Step 569 -- Train loss: 2.6570358276367188, Train Acc: 0.119140625 Test Acc: 0.1181640625\n",
      "Step 570 -- Train loss: 2.6563127040863037, Train Acc: 0.12646484375 Test Acc: 0.1064453125\n",
      "Step 571 -- Train loss: 2.65859317779541, Train Acc: 0.113525390625 Test Acc: 0.1142578125\n",
      "Step 572 -- Train loss: 2.6584632396698, Train Acc: 0.11865234375 Test Acc: 0.1044921875\n",
      "Step 573 -- Train loss: 2.6574573516845703, Train Acc: 0.12451171875 Test Acc: 0.115234375\n",
      "Step 574 -- Train loss: 2.657609462738037, Train Acc: 0.123046875 Test Acc: 0.1220703125\n",
      "Step 575 -- Train loss: 2.6571195125579834, Train Acc: 0.1181640625 Test Acc: 0.1162109375\n",
      "Step 576 -- Train loss: 2.6594178676605225, Train Acc: 0.11376953125 Test Acc: 0.126953125\n",
      "Step 577 -- Train loss: 2.656010866165161, Train Acc: 0.125732421875 Test Acc: 0.12109375\n",
      "Step 578 -- Train loss: 2.655750036239624, Train Acc: 0.1240234375 Test Acc: 0.1162109375\n",
      "Step 579 -- Train loss: 2.656907320022583, Train Acc: 0.12060546875 Test Acc: 0.1142578125\n",
      "Step 580 -- Train loss: 2.6584243774414062, Train Acc: 0.114501953125 Test Acc: 0.11328125\n",
      "Step 581 -- Train loss: 2.6570324897766113, Train Acc: 0.1171875 Test Acc: 0.115234375\n",
      "Step 582 -- Train loss: 2.655555486679077, Train Acc: 0.12353515625 Test Acc: 0.1123046875\n",
      "Step 583 -- Train loss: 2.656463623046875, Train Acc: 0.12109375 Test Acc: 0.126953125\n",
      "Step 584 -- Train loss: 2.657123565673828, Train Acc: 0.124267578125 Test Acc: 0.1298828125\n",
      "Step 585 -- Train loss: 2.6579959392547607, Train Acc: 0.114013671875 Test Acc: 0.130859375\n",
      "Step 586 -- Train loss: 2.657736301422119, Train Acc: 0.11865234375 Test Acc: 0.1279296875\n",
      "Step 587 -- Train loss: 2.655869245529175, Train Acc: 0.120361328125 Test Acc: 0.1142578125\n",
      "Step 588 -- Train loss: 2.656951665878296, Train Acc: 0.11865234375 Test Acc: 0.1220703125\n",
      "Step 589 -- Train loss: 2.6598799228668213, Train Acc: 0.110595703125 Test Acc: 0.126953125\n",
      "Step 590 -- Train loss: 2.657184362411499, Train Acc: 0.121337890625 Test Acc: 0.1298828125\n",
      "Step 591 -- Train loss: 2.6580557823181152, Train Acc: 0.122314453125 Test Acc: 0.1259765625\n",
      "Step 592 -- Train loss: 2.6572651863098145, Train Acc: 0.12158203125 Test Acc: 0.134765625\n",
      "Step 593 -- Train loss: 2.6577532291412354, Train Acc: 0.118896484375 Test Acc: 0.1259765625\n",
      "Step 594 -- Train loss: 2.656554698944092, Train Acc: 0.120849609375 Test Acc: 0.11328125\n",
      "Step 595 -- Train loss: 2.6585021018981934, Train Acc: 0.11669921875 Test Acc: 0.12109375\n",
      "Step 596 -- Train loss: 2.658531904220581, Train Acc: 0.11669921875 Test Acc: 0.10546875\n",
      "Step 597 -- Train loss: 2.6560733318328857, Train Acc: 0.120849609375 Test Acc: 0.1328125\n",
      "Step 598 -- Train loss: 2.6582190990448, Train Acc: 0.11181640625 Test Acc: 0.119140625\n",
      "Step 599 -- Train loss: 2.657860279083252, Train Acc: 0.117919921875 Test Acc: 0.109375\n",
      "Step 600 -- Train loss: 2.6578750610351562, Train Acc: 0.117431640625 Test Acc: 0.1181640625\n",
      "Step 601 -- Train loss: 2.6572959423065186, Train Acc: 0.119140625 Test Acc: 0.115234375\n",
      "Step 602 -- Train loss: 2.656581163406372, Train Acc: 0.119140625 Test Acc: 0.1181640625\n",
      "Step 603 -- Train loss: 2.65781569480896, Train Acc: 0.12158203125 Test Acc: 0.1259765625\n",
      "Step 604 -- Train loss: 2.657334566116333, Train Acc: 0.116943359375 Test Acc: 0.1181640625\n",
      "Step 605 -- Train loss: 2.6568760871887207, Train Acc: 0.123046875 Test Acc: 0.1201171875\n",
      "Step 606 -- Train loss: 2.6582226753234863, Train Acc: 0.115234375 Test Acc: 0.126953125\n",
      "Step 607 -- Train loss: 2.6576247215270996, Train Acc: 0.123291015625 Test Acc: 0.1318359375\n",
      "Step 608 -- Train loss: 2.658045768737793, Train Acc: 0.123046875 Test Acc: 0.125\n",
      "Step 609 -- Train loss: 2.656952142715454, Train Acc: 0.123046875 Test Acc: 0.1142578125\n",
      "Step 610 -- Train loss: 2.6573097705841064, Train Acc: 0.12060546875 Test Acc: 0.1171875\n",
      "Step 611 -- Train loss: 2.6558070182800293, Train Acc: 0.115966796875 Test Acc: 0.1064453125\n",
      "Step 612 -- Train loss: 2.6574172973632812, Train Acc: 0.127197265625 Test Acc: 0.123046875\n",
      "Step 613 -- Train loss: 2.6572957038879395, Train Acc: 0.11767578125 Test Acc: 0.1083984375\n",
      "Step 614 -- Train loss: 2.6574811935424805, Train Acc: 0.117431640625 Test Acc: 0.1201171875\n",
      "Step 615 -- Train loss: 2.656035900115967, Train Acc: 0.123291015625 Test Acc: 0.1220703125\n",
      "Step 616 -- Train loss: 2.656322956085205, Train Acc: 0.119384765625 Test Acc: 0.107421875\n",
      "Step 617 -- Train loss: 2.6542840003967285, Train Acc: 0.122314453125 Test Acc: 0.1220703125\n",
      "Step 618 -- Train loss: 2.6566359996795654, Train Acc: 0.122314453125 Test Acc: 0.1201171875\n",
      "Step 619 -- Train loss: 2.6826741695404053, Train Acc: 0.117431640625 Test Acc: 0.109375\n",
      "Step 620 -- Train loss: 3.014146327972412, Train Acc: 0.056640625 Test Acc: 0.0419921875\n",
      "Step 621 -- Train loss: 2.9967474937438965, Train Acc: 0.062255859375 Test Acc: 0.0625\n",
      "Step 622 -- Train loss: 2.9094951152801514, Train Acc: 0.059326171875 Test Acc: 0.0751953125\n",
      "Step 623 -- Train loss: 2.7975709438323975, Train Acc: 0.053955078125 Test Acc: 0.0556640625\n",
      "Step 624 -- Train loss: 2.7436163425445557, Train Acc: 0.119873046875 Test Acc: 0.1142578125\n",
      "Step 625 -- Train loss: 2.728029251098633, Train Acc: 0.114013671875 Test Acc: 0.1357421875\n",
      "Step 626 -- Train loss: 2.7117860317230225, Train Acc: 0.123291015625 Test Acc: 0.1298828125\n",
      "Step 627 -- Train loss: 2.691209316253662, Train Acc: 0.1220703125 Test Acc: 0.12109375\n",
      "Step 628 -- Train loss: 2.6839911937713623, Train Acc: 0.11474609375 Test Acc: 0.1162109375\n",
      "Step 629 -- Train loss: 2.672631025314331, Train Acc: 0.120361328125 Test Acc: 0.1201171875\n",
      "Step 630 -- Train loss: 2.6692042350769043, Train Acc: 0.113037109375 Test Acc: 0.1259765625\n",
      "Step 631 -- Train loss: 2.664820671081543, Train Acc: 0.12939453125 Test Acc: 0.1220703125\n",
      "Step 632 -- Train loss: 2.6693997383117676, Train Acc: 0.1171875 Test Acc: 0.1279296875\n",
      "Step 633 -- Train loss: 2.6721549034118652, Train Acc: 0.109375 Test Acc: 0.1103515625\n",
      "Step 634 -- Train loss: 2.663778066635132, Train Acc: 0.1162109375 Test Acc: 0.1083984375\n",
      "Step 635 -- Train loss: 2.669095516204834, Train Acc: 0.11669921875 Test Acc: 0.11328125\n",
      "Step 636 -- Train loss: 2.666721820831299, Train Acc: 0.1162109375 Test Acc: 0.1083984375\n",
      "Step 637 -- Train loss: 2.664645195007324, Train Acc: 0.118896484375 Test Acc: 0.126953125\n",
      "Step 638 -- Train loss: 2.6632847785949707, Train Acc: 0.120361328125 Test Acc: 0.119140625\n",
      "Step 639 -- Train loss: 2.6658809185028076, Train Acc: 0.1220703125 Test Acc: 0.125\n",
      "Step 640 -- Train loss: 2.664257764816284, Train Acc: 0.114990234375 Test Acc: 0.12890625\n",
      "Step 641 -- Train loss: 2.661390542984009, Train Acc: 0.12060546875 Test Acc: 0.115234375\n",
      "Step 642 -- Train loss: 2.661069869995117, Train Acc: 0.11669921875 Test Acc: 0.1220703125\n",
      "Step 643 -- Train loss: 2.661376953125, Train Acc: 0.11669921875 Test Acc: 0.125\n",
      "Step 644 -- Train loss: 2.6586527824401855, Train Acc: 0.1201171875 Test Acc: 0.1240234375\n",
      "Step 645 -- Train loss: 2.6631674766540527, Train Acc: 0.1181640625 Test Acc: 0.1044921875\n",
      "Step 646 -- Train loss: 2.662184000015259, Train Acc: 0.115234375 Test Acc: 0.115234375\n",
      "Step 647 -- Train loss: 2.6589901447296143, Train Acc: 0.116455078125 Test Acc: 0.1142578125\n",
      "Step 648 -- Train loss: 2.657637119293213, Train Acc: 0.123046875 Test Acc: 0.1259765625\n",
      "Step 649 -- Train loss: 2.659163475036621, Train Acc: 0.12060546875 Test Acc: 0.109375\n",
      "Step 650 -- Train loss: 2.661935329437256, Train Acc: 0.11767578125 Test Acc: 0.1220703125\n",
      "Step 651 -- Train loss: 2.6608712673187256, Train Acc: 0.115234375 Test Acc: 0.1240234375\n",
      "Step 652 -- Train loss: 2.659960985183716, Train Acc: 0.121337890625 Test Acc: 0.123046875\n",
      "Step 653 -- Train loss: 2.658010244369507, Train Acc: 0.1201171875 Test Acc: 0.1357421875\n",
      "Step 654 -- Train loss: 2.658543348312378, Train Acc: 0.11865234375 Test Acc: 0.115234375\n",
      "Step 655 -- Train loss: 2.660392999649048, Train Acc: 0.114990234375 Test Acc: 0.12109375\n",
      "Step 656 -- Train loss: 2.6596126556396484, Train Acc: 0.1103515625 Test Acc: 0.1220703125\n",
      "Step 657 -- Train loss: 2.658336639404297, Train Acc: 0.1220703125 Test Acc: 0.11328125\n",
      "Step 658 -- Train loss: 2.6594276428222656, Train Acc: 0.116455078125 Test Acc: 0.1083984375\n",
      "Step 659 -- Train loss: 2.6588850021362305, Train Acc: 0.12060546875 Test Acc: 0.1240234375\n",
      "Step 660 -- Train loss: 2.6573352813720703, Train Acc: 0.12158203125 Test Acc: 0.1220703125\n",
      "Step 661 -- Train loss: 2.65708065032959, Train Acc: 0.120361328125 Test Acc: 0.1220703125\n",
      "Step 662 -- Train loss: 2.6585474014282227, Train Acc: 0.121826171875 Test Acc: 0.1220703125\n",
      "Step 663 -- Train loss: 2.6610562801361084, Train Acc: 0.1201171875 Test Acc: 0.12890625\n",
      "Step 664 -- Train loss: 2.659418821334839, Train Acc: 0.113525390625 Test Acc: 0.12890625\n",
      "Step 665 -- Train loss: 2.657587766647339, Train Acc: 0.118408203125 Test Acc: 0.119140625\n",
      "Step 666 -- Train loss: 2.657339572906494, Train Acc: 0.127685546875 Test Acc: 0.1123046875\n",
      "Step 667 -- Train loss: 2.6626176834106445, Train Acc: 0.115478515625 Test Acc: 0.11328125\n",
      "Step 668 -- Train loss: 2.661191463470459, Train Acc: 0.117431640625 Test Acc: 0.1123046875\n",
      "Step 669 -- Train loss: 2.658747434616089, Train Acc: 0.109375 Test Acc: 0.11328125\n",
      "Step 670 -- Train loss: 2.6582133769989014, Train Acc: 0.11669921875 Test Acc: 0.1064453125\n",
      "Step 671 -- Train loss: 2.6596105098724365, Train Acc: 0.126708984375 Test Acc: 0.111328125\n",
      "Step 672 -- Train loss: 2.660954475402832, Train Acc: 0.12353515625 Test Acc: 0.109375\n",
      "Step 673 -- Train loss: 2.6610965728759766, Train Acc: 0.119873046875 Test Acc: 0.119140625\n",
      "Step 674 -- Train loss: 2.6583480834960938, Train Acc: 0.12158203125 Test Acc: 0.12109375\n",
      "Step 675 -- Train loss: 2.661658763885498, Train Acc: 0.114990234375 Test Acc: 0.115234375\n",
      "Step 676 -- Train loss: 2.661071300506592, Train Acc: 0.118896484375 Test Acc: 0.1220703125\n",
      "Step 677 -- Train loss: 2.6598141193389893, Train Acc: 0.11962890625 Test Acc: 0.1201171875\n",
      "Step 678 -- Train loss: 2.661226749420166, Train Acc: 0.114990234375 Test Acc: 0.12109375\n",
      "Step 679 -- Train loss: 2.6607093811035156, Train Acc: 0.120361328125 Test Acc: 0.130859375\n",
      "Step 680 -- Train loss: 2.6603376865386963, Train Acc: 0.120849609375 Test Acc: 0.111328125\n",
      "Step 681 -- Train loss: 2.657869338989258, Train Acc: 0.117431640625 Test Acc: 0.1181640625\n",
      "Step 682 -- Train loss: 2.6603236198425293, Train Acc: 0.121337890625 Test Acc: 0.111328125\n",
      "Step 683 -- Train loss: 2.6625521183013916, Train Acc: 0.113037109375 Test Acc: 0.12109375\n",
      "Step 684 -- Train loss: 2.6587271690368652, Train Acc: 0.123046875 Test Acc: 0.1162109375\n",
      "Step 685 -- Train loss: 2.661222219467163, Train Acc: 0.121337890625 Test Acc: 0.123046875\n",
      "Step 686 -- Train loss: 2.6573383808135986, Train Acc: 0.12109375 Test Acc: 0.1083984375\n",
      "Step 687 -- Train loss: 2.658662796020508, Train Acc: 0.128173828125 Test Acc: 0.12109375\n",
      "Step 688 -- Train loss: 2.6614229679107666, Train Acc: 0.12451171875 Test Acc: 0.107421875\n",
      "Step 689 -- Train loss: 2.6625866889953613, Train Acc: 0.119873046875 Test Acc: 0.1083984375\n",
      "Step 690 -- Train loss: 2.6595938205718994, Train Acc: 0.116455078125 Test Acc: 0.109375\n",
      "Step 691 -- Train loss: 2.6578893661499023, Train Acc: 0.119384765625 Test Acc: 0.12890625\n",
      "Step 692 -- Train loss: 2.657278537750244, Train Acc: 0.120361328125 Test Acc: 0.1171875\n",
      "Step 693 -- Train loss: 2.6582908630371094, Train Acc: 0.1201171875 Test Acc: 0.126953125\n",
      "Step 694 -- Train loss: 2.6595356464385986, Train Acc: 0.116455078125 Test Acc: 0.1103515625\n",
      "Step 695 -- Train loss: 2.6600918769836426, Train Acc: 0.1201171875 Test Acc: 0.115234375\n",
      "Step 696 -- Train loss: 2.6588492393493652, Train Acc: 0.116455078125 Test Acc: 0.1181640625\n",
      "Step 697 -- Train loss: 2.659646511077881, Train Acc: 0.120849609375 Test Acc: 0.1162109375\n",
      "Step 698 -- Train loss: 2.660322904586792, Train Acc: 0.118408203125 Test Acc: 0.123046875\n",
      "Step 699 -- Train loss: 2.6609983444213867, Train Acc: 0.1171875 Test Acc: 0.1201171875\n",
      "Step 700 -- Train loss: 2.65842342376709, Train Acc: 0.121337890625 Test Acc: 0.1103515625\n",
      "Step 701 -- Train loss: 2.6568775177001953, Train Acc: 0.11572265625 Test Acc: 0.1162109375\n",
      "Step 702 -- Train loss: 2.657623291015625, Train Acc: 0.11328125 Test Acc: 0.111328125\n",
      "Step 703 -- Train loss: 2.6585288047790527, Train Acc: 0.123046875 Test Acc: 0.115234375\n",
      "Step 704 -- Train loss: 2.6600470542907715, Train Acc: 0.121826171875 Test Acc: 0.126953125\n",
      "Step 705 -- Train loss: 2.659900426864624, Train Acc: 0.117431640625 Test Acc: 0.115234375\n",
      "Step 706 -- Train loss: 2.658350944519043, Train Acc: 0.121337890625 Test Acc: 0.115234375\n",
      "Step 707 -- Train loss: 2.6574079990386963, Train Acc: 0.11865234375 Test Acc: 0.125\n",
      "Step 708 -- Train loss: 2.658498764038086, Train Acc: 0.1171875 Test Acc: 0.115234375\n",
      "Step 709 -- Train loss: 2.6580822467803955, Train Acc: 0.1181640625 Test Acc: 0.111328125\n",
      "Step 710 -- Train loss: 2.658083200454712, Train Acc: 0.12109375 Test Acc: 0.1240234375\n",
      "Step 711 -- Train loss: 2.6589627265930176, Train Acc: 0.117919921875 Test Acc: 0.1201171875\n",
      "Step 712 -- Train loss: 2.65787410736084, Train Acc: 0.1171875 Test Acc: 0.1044921875\n",
      "Step 713 -- Train loss: 2.6582789421081543, Train Acc: 0.11669921875 Test Acc: 0.1328125\n",
      "Step 714 -- Train loss: 2.658367156982422, Train Acc: 0.119384765625 Test Acc: 0.1220703125\n",
      "Step 715 -- Train loss: 2.657291889190674, Train Acc: 0.118408203125 Test Acc: 0.111328125\n",
      "Step 716 -- Train loss: 2.6580402851104736, Train Acc: 0.1181640625 Test Acc: 0.1025390625\n",
      "Step 717 -- Train loss: 2.6580147743225098, Train Acc: 0.1220703125 Test Acc: 0.126953125\n",
      "Step 718 -- Train loss: 2.658867359161377, Train Acc: 0.113037109375 Test Acc: 0.123046875\n",
      "Step 719 -- Train loss: 2.658334255218506, Train Acc: 0.119140625 Test Acc: 0.1171875\n",
      "Step 720 -- Train loss: 2.659118175506592, Train Acc: 0.1181640625 Test Acc: 0.1201171875\n",
      "Step 721 -- Train loss: 2.6574947834014893, Train Acc: 0.125 Test Acc: 0.123046875\n",
      "Step 722 -- Train loss: 2.6564717292785645, Train Acc: 0.12353515625 Test Acc: 0.1220703125\n",
      "Step 723 -- Train loss: 2.658475399017334, Train Acc: 0.122314453125 Test Acc: 0.123046875\n",
      "Step 724 -- Train loss: 2.6594173908233643, Train Acc: 0.11962890625 Test Acc: 0.125\n",
      "Step 725 -- Train loss: 2.6577162742614746, Train Acc: 0.1171875 Test Acc: 0.1142578125\n",
      "Step 726 -- Train loss: 2.6570427417755127, Train Acc: 0.122314453125 Test Acc: 0.1171875\n",
      "Step 727 -- Train loss: 2.656075954437256, Train Acc: 0.116943359375 Test Acc: 0.1142578125\n",
      "Step 728 -- Train loss: 2.658526659011841, Train Acc: 0.114501953125 Test Acc: 0.125\n",
      "Step 729 -- Train loss: 2.6594033241271973, Train Acc: 0.11474609375 Test Acc: 0.1171875\n",
      "Step 730 -- Train loss: 2.6584763526916504, Train Acc: 0.114013671875 Test Acc: 0.1220703125\n",
      "Step 731 -- Train loss: 2.6578516960144043, Train Acc: 0.119873046875 Test Acc: 0.109375\n",
      "Step 732 -- Train loss: 2.6569719314575195, Train Acc: 0.11669921875 Test Acc: 0.1201171875\n",
      "Step 733 -- Train loss: 2.6571147441864014, Train Acc: 0.115966796875 Test Acc: 0.1376953125\n",
      "Step 734 -- Train loss: 2.6578919887542725, Train Acc: 0.123046875 Test Acc: 0.107421875\n",
      "Step 735 -- Train loss: 2.659735918045044, Train Acc: 0.117431640625 Test Acc: 0.109375\n",
      "Step 736 -- Train loss: 2.6572861671447754, Train Acc: 0.122314453125 Test Acc: 0.1220703125\n",
      "Step 737 -- Train loss: 2.6581625938415527, Train Acc: 0.118896484375 Test Acc: 0.1025390625\n",
      "Step 738 -- Train loss: 2.6588845252990723, Train Acc: 0.11474609375 Test Acc: 0.1279296875\n",
      "Step 739 -- Train loss: 2.65641450881958, Train Acc: 0.1220703125 Test Acc: 0.1279296875\n",
      "Step 740 -- Train loss: 2.6564817428588867, Train Acc: 0.124755859375 Test Acc: 0.11328125\n",
      "Step 741 -- Train loss: 2.6576476097106934, Train Acc: 0.117431640625 Test Acc: 0.1328125\n",
      "Step 742 -- Train loss: 2.660261869430542, Train Acc: 0.118408203125 Test Acc: 0.115234375\n",
      "Step 743 -- Train loss: 2.659359931945801, Train Acc: 0.123291015625 Test Acc: 0.12890625\n",
      "Step 744 -- Train loss: 2.6568119525909424, Train Acc: 0.120361328125 Test Acc: 0.119140625\n",
      "Step 745 -- Train loss: 2.658292531967163, Train Acc: 0.12158203125 Test Acc: 0.1142578125\n",
      "Step 746 -- Train loss: 2.6586198806762695, Train Acc: 0.12451171875 Test Acc: 0.126953125\n",
      "Step 747 -- Train loss: 2.6577343940734863, Train Acc: 0.123046875 Test Acc: 0.115234375\n",
      "Step 748 -- Train loss: 2.6591320037841797, Train Acc: 0.119384765625 Test Acc: 0.1201171875\n",
      "Step 749 -- Train loss: 2.6583173274993896, Train Acc: 0.125732421875 Test Acc: 0.1171875\n",
      "Step 750 -- Train loss: 2.6575708389282227, Train Acc: 0.118896484375 Test Acc: 0.125\n",
      "Step 751 -- Train loss: 2.6573266983032227, Train Acc: 0.119873046875 Test Acc: 0.1279296875\n",
      "Step 752 -- Train loss: 2.6580071449279785, Train Acc: 0.1171875 Test Acc: 0.1142578125\n",
      "Step 753 -- Train loss: 2.656322717666626, Train Acc: 0.118896484375 Test Acc: 0.1279296875\n",
      "Step 754 -- Train loss: 2.657869815826416, Train Acc: 0.115966796875 Test Acc: 0.1171875\n",
      "Step 755 -- Train loss: 2.6569793224334717, Train Acc: 0.1220703125 Test Acc: 0.126953125\n",
      "Step 756 -- Train loss: 2.6582107543945312, Train Acc: 0.121337890625 Test Acc: 0.1259765625\n",
      "Step 757 -- Train loss: 2.658343553543091, Train Acc: 0.11865234375 Test Acc: 0.107421875\n",
      "Step 758 -- Train loss: 2.6564323902130127, Train Acc: 0.12353515625 Test Acc: 0.126953125\n",
      "Step 759 -- Train loss: 2.657052993774414, Train Acc: 0.115234375 Test Acc: 0.115234375\n",
      "Step 760 -- Train loss: 2.658298969268799, Train Acc: 0.12060546875 Test Acc: 0.1220703125\n",
      "Step 761 -- Train loss: 2.65736985206604, Train Acc: 0.117431640625 Test Acc: 0.115234375\n",
      "Step 762 -- Train loss: 2.6562836170196533, Train Acc: 0.12939453125 Test Acc: 0.11328125\n",
      "Step 763 -- Train loss: 2.6585023403167725, Train Acc: 0.1162109375 Test Acc: 0.1142578125\n",
      "Step 764 -- Train loss: 2.6572420597076416, Train Acc: 0.124755859375 Test Acc: 0.10546875\n",
      "Step 765 -- Train loss: 2.658090353012085, Train Acc: 0.125 Test Acc: 0.1044921875\n",
      "Step 766 -- Train loss: 2.6567437648773193, Train Acc: 0.118896484375 Test Acc: 0.123046875\n",
      "Step 767 -- Train loss: 2.6570820808410645, Train Acc: 0.11572265625 Test Acc: 0.123046875\n",
      "Step 768 -- Train loss: 2.6565890312194824, Train Acc: 0.119140625 Test Acc: 0.1279296875\n",
      "Step 769 -- Train loss: 2.65743350982666, Train Acc: 0.1220703125 Test Acc: 0.1201171875\n",
      "Step 770 -- Train loss: 2.6583430767059326, Train Acc: 0.120849609375 Test Acc: 0.1171875\n",
      "Step 771 -- Train loss: 2.657470226287842, Train Acc: 0.126220703125 Test Acc: 0.1171875\n",
      "Step 772 -- Train loss: 2.6570887565612793, Train Acc: 0.115234375 Test Acc: 0.1181640625\n",
      "Step 773 -- Train loss: 2.65761399269104, Train Acc: 0.117919921875 Test Acc: 0.111328125\n",
      "Step 774 -- Train loss: 2.6586172580718994, Train Acc: 0.1201171875 Test Acc: 0.1171875\n",
      "Step 775 -- Train loss: 2.657059907913208, Train Acc: 0.126220703125 Test Acc: 0.1220703125\n",
      "Step 776 -- Train loss: 2.6579484939575195, Train Acc: 0.119140625 Test Acc: 0.12109375\n",
      "Step 777 -- Train loss: 2.656681776046753, Train Acc: 0.13037109375 Test Acc: 0.11328125\n",
      "Step 778 -- Train loss: 2.6574528217315674, Train Acc: 0.122802734375 Test Acc: 0.111328125\n",
      "Step 779 -- Train loss: 2.65647029876709, Train Acc: 0.123291015625 Test Acc: 0.115234375\n",
      "Step 780 -- Train loss: 2.6577324867248535, Train Acc: 0.1240234375 Test Acc: 0.1318359375\n",
      "Step 781 -- Train loss: 2.657996654510498, Train Acc: 0.118896484375 Test Acc: 0.1103515625\n",
      "Step 782 -- Train loss: 2.657703399658203, Train Acc: 0.117919921875 Test Acc: 0.126953125\n",
      "Step 783 -- Train loss: 2.6583662033081055, Train Acc: 0.120849609375 Test Acc: 0.10546875\n",
      "Step 784 -- Train loss: 2.656871795654297, Train Acc: 0.11767578125 Test Acc: 0.107421875\n",
      "Step 785 -- Train loss: 2.6584277153015137, Train Acc: 0.1181640625 Test Acc: 0.1044921875\n",
      "Step 786 -- Train loss: 2.6577131748199463, Train Acc: 0.12548828125 Test Acc: 0.1201171875\n",
      "Step 787 -- Train loss: 2.6572391986846924, Train Acc: 0.1220703125 Test Acc: 0.107421875\n",
      "Step 788 -- Train loss: 2.658936023712158, Train Acc: 0.11572265625 Test Acc: 0.1123046875\n",
      "Step 789 -- Train loss: 2.657541275024414, Train Acc: 0.122314453125 Test Acc: 0.1162109375\n",
      "Step 790 -- Train loss: 2.657750368118286, Train Acc: 0.117919921875 Test Acc: 0.1162109375\n",
      "Step 791 -- Train loss: 2.6568193435668945, Train Acc: 0.12353515625 Test Acc: 0.1103515625\n",
      "Step 792 -- Train loss: 2.6574318408966064, Train Acc: 0.127197265625 Test Acc: 0.12109375\n",
      "Step 793 -- Train loss: 2.6578588485717773, Train Acc: 0.12060546875 Test Acc: 0.1142578125\n",
      "Step 794 -- Train loss: 2.65865421295166, Train Acc: 0.1123046875 Test Acc: 0.109375\n",
      "Step 795 -- Train loss: 2.656095027923584, Train Acc: 0.127197265625 Test Acc: 0.119140625\n",
      "Step 796 -- Train loss: 2.6596519947052, Train Acc: 0.116455078125 Test Acc: 0.1416015625\n",
      "Step 797 -- Train loss: 2.657323122024536, Train Acc: 0.122314453125 Test Acc: 0.1240234375\n",
      "Step 798 -- Train loss: 2.6594715118408203, Train Acc: 0.11865234375 Test Acc: 0.1181640625\n",
      "Step 799 -- Train loss: 2.657837152481079, Train Acc: 0.11767578125 Test Acc: 0.1259765625\n",
      "Step 800 -- Train loss: 2.6572864055633545, Train Acc: 0.1259765625 Test Acc: 0.111328125\n",
      "Step 801 -- Train loss: 2.657846689224243, Train Acc: 0.122802734375 Test Acc: 0.1123046875\n",
      "Step 802 -- Train loss: 2.6548244953155518, Train Acc: 0.12353515625 Test Acc: 0.1181640625\n",
      "Step 803 -- Train loss: 2.658514976501465, Train Acc: 0.11962890625 Test Acc: 0.1083984375\n",
      "Step 804 -- Train loss: 2.6602225303649902, Train Acc: 0.118408203125 Test Acc: 0.1142578125\n",
      "Step 805 -- Train loss: 2.658052444458008, Train Acc: 0.116455078125 Test Acc: 0.12890625\n",
      "Step 806 -- Train loss: 2.656583309173584, Train Acc: 0.116455078125 Test Acc: 0.126953125\n",
      "Step 807 -- Train loss: 2.658705949783325, Train Acc: 0.117431640625 Test Acc: 0.1259765625\n",
      "Step 808 -- Train loss: 2.6578168869018555, Train Acc: 0.1181640625 Test Acc: 0.1201171875\n",
      "Step 809 -- Train loss: 2.6580705642700195, Train Acc: 0.11083984375 Test Acc: 0.11328125\n",
      "Step 810 -- Train loss: 2.6574504375457764, Train Acc: 0.1142578125 Test Acc: 0.107421875\n",
      "Step 811 -- Train loss: 2.658761501312256, Train Acc: 0.11669921875 Test Acc: 0.12109375\n",
      "Step 812 -- Train loss: 2.65824294090271, Train Acc: 0.1181640625 Test Acc: 0.1162109375\n",
      "Step 813 -- Train loss: 2.656136989593506, Train Acc: 0.114501953125 Test Acc: 0.119140625\n",
      "Step 814 -- Train loss: 2.6580240726470947, Train Acc: 0.116943359375 Test Acc: 0.1220703125\n",
      "Step 815 -- Train loss: 2.6571238040924072, Train Acc: 0.11962890625 Test Acc: 0.1142578125\n",
      "Step 816 -- Train loss: 2.660609245300293, Train Acc: 0.118896484375 Test Acc: 0.10546875\n",
      "Step 817 -- Train loss: 2.6584720611572266, Train Acc: 0.119384765625 Test Acc: 0.0986328125\n",
      "Step 818 -- Train loss: 2.657344102859497, Train Acc: 0.123046875 Test Acc: 0.1318359375\n",
      "Step 819 -- Train loss: 2.6567299365997314, Train Acc: 0.12744140625 Test Acc: 0.1181640625\n",
      "Step 820 -- Train loss: 2.657089948654175, Train Acc: 0.1220703125 Test Acc: 0.1220703125\n",
      "Step 821 -- Train loss: 2.6587510108947754, Train Acc: 0.122314453125 Test Acc: 0.1171875\n",
      "Step 822 -- Train loss: 2.659172773361206, Train Acc: 0.1181640625 Test Acc: 0.125\n",
      "Step 823 -- Train loss: 2.6583149433135986, Train Acc: 0.115966796875 Test Acc: 0.1171875\n",
      "Step 824 -- Train loss: 2.6584880352020264, Train Acc: 0.117431640625 Test Acc: 0.119140625\n",
      "Step 825 -- Train loss: 2.6557602882385254, Train Acc: 0.114990234375 Test Acc: 0.115234375\n",
      "Step 826 -- Train loss: 2.658277750015259, Train Acc: 0.119384765625 Test Acc: 0.123046875\n",
      "Step 827 -- Train loss: 2.6565561294555664, Train Acc: 0.12060546875 Test Acc: 0.1220703125\n",
      "Step 828 -- Train loss: 2.6600472927093506, Train Acc: 0.11572265625 Test Acc: 0.1279296875\n",
      "Step 829 -- Train loss: 2.658451557159424, Train Acc: 0.11572265625 Test Acc: 0.1220703125\n",
      "Step 830 -- Train loss: 2.656572103500366, Train Acc: 0.123046875 Test Acc: 0.1083984375\n",
      "Step 831 -- Train loss: 2.6587798595428467, Train Acc: 0.123291015625 Test Acc: 0.123046875\n",
      "Step 832 -- Train loss: 2.6571106910705566, Train Acc: 0.121826171875 Test Acc: 0.1181640625\n",
      "Step 833 -- Train loss: 2.6579527854919434, Train Acc: 0.11767578125 Test Acc: 0.1123046875\n",
      "Step 834 -- Train loss: 2.6565282344818115, Train Acc: 0.126953125 Test Acc: 0.11328125\n",
      "Step 835 -- Train loss: 2.6579041481018066, Train Acc: 0.116455078125 Test Acc: 0.109375\n",
      "Step 836 -- Train loss: 2.6586480140686035, Train Acc: 0.115966796875 Test Acc: 0.1064453125\n",
      "Step 837 -- Train loss: 2.6558022499084473, Train Acc: 0.125244140625 Test Acc: 0.109375\n",
      "Step 838 -- Train loss: 2.658215284347534, Train Acc: 0.11572265625 Test Acc: 0.111328125\n",
      "Step 839 -- Train loss: 2.658267021179199, Train Acc: 0.11865234375 Test Acc: 0.1162109375\n",
      "Step 840 -- Train loss: 2.6569511890411377, Train Acc: 0.119384765625 Test Acc: 0.1240234375\n",
      "Step 841 -- Train loss: 2.6602210998535156, Train Acc: 0.109130859375 Test Acc: 0.1123046875\n",
      "Step 842 -- Train loss: 2.6585593223571777, Train Acc: 0.116455078125 Test Acc: 0.115234375\n",
      "Step 843 -- Train loss: 2.6573987007141113, Train Acc: 0.114990234375 Test Acc: 0.1181640625\n",
      "Step 844 -- Train loss: 2.6573071479797363, Train Acc: 0.11669921875 Test Acc: 0.1220703125\n",
      "Step 845 -- Train loss: 2.656698703765869, Train Acc: 0.116943359375 Test Acc: 0.123046875\n",
      "Step 846 -- Train loss: 2.657316207885742, Train Acc: 0.12158203125 Test Acc: 0.103515625\n",
      "Step 847 -- Train loss: 2.6568868160247803, Train Acc: 0.121337890625 Test Acc: 0.123046875\n",
      "Step 848 -- Train loss: 2.657388925552368, Train Acc: 0.117919921875 Test Acc: 0.1162109375\n",
      "Step 849 -- Train loss: 2.657663583755493, Train Acc: 0.11279296875 Test Acc: 0.1220703125\n",
      "Step 850 -- Train loss: 2.6571390628814697, Train Acc: 0.1171875 Test Acc: 0.12109375\n",
      "Step 851 -- Train loss: 2.658163547515869, Train Acc: 0.1181640625 Test Acc: 0.115234375\n",
      "Step 852 -- Train loss: 2.657609462738037, Train Acc: 0.116943359375 Test Acc: 0.1328125\n",
      "Step 853 -- Train loss: 2.6551411151885986, Train Acc: 0.12158203125 Test Acc: 0.1162109375\n",
      "Step 854 -- Train loss: 2.6577305793762207, Train Acc: 0.125732421875 Test Acc: 0.1181640625\n",
      "Step 855 -- Train loss: 2.6588387489318848, Train Acc: 0.119384765625 Test Acc: 0.12109375\n",
      "Step 856 -- Train loss: 2.656982421875, Train Acc: 0.124755859375 Test Acc: 0.1279296875\n",
      "Step 857 -- Train loss: 2.656796455383301, Train Acc: 0.12255859375 Test Acc: 0.1064453125\n",
      "Step 858 -- Train loss: 2.657468318939209, Train Acc: 0.1240234375 Test Acc: 0.125\n",
      "Step 859 -- Train loss: 2.6569974422454834, Train Acc: 0.1240234375 Test Acc: 0.12890625\n",
      "Step 860 -- Train loss: 2.6589019298553467, Train Acc: 0.115966796875 Test Acc: 0.1162109375\n",
      "Step 861 -- Train loss: 2.658050060272217, Train Acc: 0.116943359375 Test Acc: 0.1201171875\n",
      "Step 862 -- Train loss: 2.6580049991607666, Train Acc: 0.12158203125 Test Acc: 0.1181640625\n",
      "Step 863 -- Train loss: 2.6577720642089844, Train Acc: 0.120361328125 Test Acc: 0.119140625\n",
      "Step 864 -- Train loss: 2.659958839416504, Train Acc: 0.115234375 Test Acc: 0.12109375\n",
      "Step 865 -- Train loss: 2.657808303833008, Train Acc: 0.117431640625 Test Acc: 0.1162109375\n",
      "Step 866 -- Train loss: 2.6569902896881104, Train Acc: 0.122802734375 Test Acc: 0.1103515625\n",
      "Step 867 -- Train loss: 2.658942699432373, Train Acc: 0.11328125 Test Acc: 0.12890625\n",
      "Step 868 -- Train loss: 2.6568448543548584, Train Acc: 0.11767578125 Test Acc: 0.119140625\n",
      "Step 869 -- Train loss: 2.6585278511047363, Train Acc: 0.11865234375 Test Acc: 0.123046875\n",
      "Step 870 -- Train loss: 2.657435894012451, Train Acc: 0.120361328125 Test Acc: 0.1201171875\n",
      "Step 871 -- Train loss: 2.6566014289855957, Train Acc: 0.123046875 Test Acc: 0.1103515625\n",
      "Step 872 -- Train loss: 2.6574649810791016, Train Acc: 0.1181640625 Test Acc: 0.111328125\n",
      "Step 873 -- Train loss: 2.6559929847717285, Train Acc: 0.124267578125 Test Acc: 0.123046875\n",
      "Step 874 -- Train loss: 2.6574594974517822, Train Acc: 0.12109375 Test Acc: 0.1240234375\n",
      "Step 875 -- Train loss: 2.657147169113159, Train Acc: 0.120849609375 Test Acc: 0.115234375\n",
      "Step 876 -- Train loss: 2.658233880996704, Train Acc: 0.11669921875 Test Acc: 0.109375\n",
      "Step 877 -- Train loss: 2.656456708908081, Train Acc: 0.128662109375 Test Acc: 0.126953125\n",
      "Step 878 -- Train loss: 2.658876895904541, Train Acc: 0.112548828125 Test Acc: 0.1201171875\n",
      "Step 879 -- Train loss: 2.6582155227661133, Train Acc: 0.115478515625 Test Acc: 0.1201171875\n",
      "Step 880 -- Train loss: 2.657571792602539, Train Acc: 0.11328125 Test Acc: 0.125\n",
      "Step 881 -- Train loss: 2.657750368118286, Train Acc: 0.116455078125 Test Acc: 0.1201171875\n",
      "Step 882 -- Train loss: 2.6560821533203125, Train Acc: 0.12060546875 Test Acc: 0.1123046875\n",
      "Step 883 -- Train loss: 2.658386707305908, Train Acc: 0.1201171875 Test Acc: 0.111328125\n",
      "Step 884 -- Train loss: 2.658364772796631, Train Acc: 0.12353515625 Test Acc: 0.1240234375\n",
      "Step 885 -- Train loss: 2.658482551574707, Train Acc: 0.12158203125 Test Acc: 0.1103515625\n",
      "Step 886 -- Train loss: 2.6575405597686768, Train Acc: 0.119384765625 Test Acc: 0.1181640625\n",
      "Step 887 -- Train loss: 2.6570558547973633, Train Acc: 0.123779296875 Test Acc: 0.1240234375\n",
      "Step 888 -- Train loss: 2.6573450565338135, Train Acc: 0.122314453125 Test Acc: 0.12109375\n",
      "Step 889 -- Train loss: 2.658106803894043, Train Acc: 0.118408203125 Test Acc: 0.10546875\n",
      "Step 890 -- Train loss: 2.65801739692688, Train Acc: 0.1201171875 Test Acc: 0.115234375\n",
      "Step 891 -- Train loss: 2.657841682434082, Train Acc: 0.11669921875 Test Acc: 0.109375\n",
      "Step 892 -- Train loss: 2.659186840057373, Train Acc: 0.11767578125 Test Acc: 0.1259765625\n",
      "Step 893 -- Train loss: 2.658946990966797, Train Acc: 0.117431640625 Test Acc: 0.1298828125\n",
      "Step 894 -- Train loss: 2.658770799636841, Train Acc: 0.121337890625 Test Acc: 0.1181640625\n",
      "Step 895 -- Train loss: 2.6569440364837646, Train Acc: 0.122314453125 Test Acc: 0.1220703125\n",
      "Step 896 -- Train loss: 2.6566102504730225, Train Acc: 0.12158203125 Test Acc: 0.1142578125\n",
      "Step 897 -- Train loss: 2.6568472385406494, Train Acc: 0.121826171875 Test Acc: 0.1142578125\n",
      "Step 898 -- Train loss: 2.6578688621520996, Train Acc: 0.11669921875 Test Acc: 0.1318359375\n",
      "Step 899 -- Train loss: 2.65665864944458, Train Acc: 0.123046875 Test Acc: 0.1201171875\n",
      "Step 900 -- Train loss: 2.657182455062866, Train Acc: 0.1171875 Test Acc: 0.1162109375\n",
      "Step 901 -- Train loss: 2.6569814682006836, Train Acc: 0.123291015625 Test Acc: 0.10546875\n",
      "Step 902 -- Train loss: 2.6587612628936768, Train Acc: 0.1220703125 Test Acc: 0.109375\n",
      "Step 903 -- Train loss: 2.657531976699829, Train Acc: 0.114990234375 Test Acc: 0.119140625\n",
      "Step 904 -- Train loss: 2.6570608615875244, Train Acc: 0.1220703125 Test Acc: 0.1181640625\n",
      "Step 905 -- Train loss: 2.6563050746917725, Train Acc: 0.1201171875 Test Acc: 0.12109375\n",
      "Step 906 -- Train loss: 2.656982898712158, Train Acc: 0.122802734375 Test Acc: 0.1220703125\n",
      "Step 907 -- Train loss: 2.656599760055542, Train Acc: 0.124267578125 Test Acc: 0.12109375\n",
      "Step 908 -- Train loss: 2.656524419784546, Train Acc: 0.11865234375 Test Acc: 0.111328125\n",
      "Step 909 -- Train loss: 2.6578497886657715, Train Acc: 0.117431640625 Test Acc: 0.11328125\n",
      "Step 910 -- Train loss: 2.6581451892852783, Train Acc: 0.11962890625 Test Acc: 0.125\n",
      "Step 911 -- Train loss: 2.6577329635620117, Train Acc: 0.122314453125 Test Acc: 0.1142578125\n",
      "Step 912 -- Train loss: 2.6567742824554443, Train Acc: 0.11572265625 Test Acc: 0.1123046875\n",
      "Step 913 -- Train loss: 2.6575801372528076, Train Acc: 0.123779296875 Test Acc: 0.109375\n",
      "Step 914 -- Train loss: 2.6577341556549072, Train Acc: 0.1162109375 Test Acc: 0.1259765625\n",
      "Step 915 -- Train loss: 2.6557350158691406, Train Acc: 0.1201171875 Test Acc: 0.1279296875\n",
      "Step 916 -- Train loss: 2.657057762145996, Train Acc: 0.1201171875 Test Acc: 0.1123046875\n",
      "Step 917 -- Train loss: 2.656270742416382, Train Acc: 0.12255859375 Test Acc: 0.11328125\n",
      "Step 918 -- Train loss: 2.6586968898773193, Train Acc: 0.1142578125 Test Acc: 0.119140625\n",
      "Step 919 -- Train loss: 2.6577374935150146, Train Acc: 0.121826171875 Test Acc: 0.125\n",
      "Step 920 -- Train loss: 2.656630277633667, Train Acc: 0.118408203125 Test Acc: 0.109375\n",
      "Step 921 -- Train loss: 2.6573312282562256, Train Acc: 0.1259765625 Test Acc: 0.1162109375\n",
      "Step 922 -- Train loss: 2.658029317855835, Train Acc: 0.116943359375 Test Acc: 0.1181640625\n",
      "Step 923 -- Train loss: 2.6580026149749756, Train Acc: 0.116455078125 Test Acc: 0.1142578125\n",
      "Step 924 -- Train loss: 2.656308174133301, Train Acc: 0.12158203125 Test Acc: 0.1103515625\n",
      "Step 925 -- Train loss: 2.657109260559082, Train Acc: 0.116943359375 Test Acc: 0.1181640625\n",
      "Step 926 -- Train loss: 2.657435178756714, Train Acc: 0.119140625 Test Acc: 0.1064453125\n",
      "Step 927 -- Train loss: 2.657594919204712, Train Acc: 0.117919921875 Test Acc: 0.119140625\n",
      "Step 928 -- Train loss: 2.6574018001556396, Train Acc: 0.118408203125 Test Acc: 0.111328125\n",
      "Step 929 -- Train loss: 2.658090829849243, Train Acc: 0.122802734375 Test Acc: 0.1103515625\n",
      "Step 930 -- Train loss: 2.6566061973571777, Train Acc: 0.12548828125 Test Acc: 0.119140625\n",
      "Step 931 -- Train loss: 2.6552343368530273, Train Acc: 0.12841796875 Test Acc: 0.10546875\n",
      "Step 932 -- Train loss: 2.6578993797302246, Train Acc: 0.12109375 Test Acc: 0.1220703125\n",
      "Step 933 -- Train loss: 2.6574556827545166, Train Acc: 0.118408203125 Test Acc: 0.1240234375\n",
      "Step 934 -- Train loss: 2.6560676097869873, Train Acc: 0.1240234375 Test Acc: 0.1201171875\n",
      "Step 935 -- Train loss: 2.657501459121704, Train Acc: 0.12451171875 Test Acc: 0.111328125\n",
      "Step 936 -- Train loss: 2.6574954986572266, Train Acc: 0.1201171875 Test Acc: 0.1171875\n",
      "Step 937 -- Train loss: 2.6576015949249268, Train Acc: 0.11767578125 Test Acc: 0.1220703125\n",
      "Step 938 -- Train loss: 2.6564297676086426, Train Acc: 0.11767578125 Test Acc: 0.115234375\n",
      "Step 939 -- Train loss: 2.656621217727661, Train Acc: 0.1201171875 Test Acc: 0.1201171875\n",
      "Step 940 -- Train loss: 2.6564948558807373, Train Acc: 0.12109375 Test Acc: 0.1123046875\n",
      "Step 941 -- Train loss: 2.6577513217926025, Train Acc: 0.113525390625 Test Acc: 0.1044921875\n",
      "Step 942 -- Train loss: 2.6576321125030518, Train Acc: 0.119873046875 Test Acc: 0.111328125\n",
      "Step 943 -- Train loss: 2.65667986869812, Train Acc: 0.1162109375 Test Acc: 0.130859375\n",
      "Step 944 -- Train loss: 2.657043218612671, Train Acc: 0.1220703125 Test Acc: 0.1171875\n",
      "Step 945 -- Train loss: 2.656996011734009, Train Acc: 0.119140625 Test Acc: 0.11328125\n",
      "Step 946 -- Train loss: 2.657370090484619, Train Acc: 0.121337890625 Test Acc: 0.1015625\n",
      "Step 947 -- Train loss: 2.657508373260498, Train Acc: 0.114990234375 Test Acc: 0.1142578125\n",
      "Step 948 -- Train loss: 2.6568856239318848, Train Acc: 0.1259765625 Test Acc: 0.1123046875\n",
      "Step 949 -- Train loss: 2.6573426723480225, Train Acc: 0.118408203125 Test Acc: 0.1103515625\n",
      "Step 950 -- Train loss: 2.6562340259552, Train Acc: 0.122314453125 Test Acc: 0.1298828125\n",
      "Step 951 -- Train loss: 2.656881093978882, Train Acc: 0.12353515625 Test Acc: 0.1337890625\n",
      "Step 952 -- Train loss: 2.6567118167877197, Train Acc: 0.12353515625 Test Acc: 0.1162109375\n",
      "Step 953 -- Train loss: 2.6588242053985596, Train Acc: 0.1201171875 Test Acc: 0.1220703125\n",
      "Step 954 -- Train loss: 2.6573123931884766, Train Acc: 0.119384765625 Test Acc: 0.1162109375\n",
      "Step 955 -- Train loss: 2.656351327896118, Train Acc: 0.12060546875 Test Acc: 0.1259765625\n",
      "Step 956 -- Train loss: 2.6569929122924805, Train Acc: 0.122802734375 Test Acc: 0.123046875\n",
      "Step 957 -- Train loss: 2.656273603439331, Train Acc: 0.1201171875 Test Acc: 0.1201171875\n",
      "Step 958 -- Train loss: 2.6590781211853027, Train Acc: 0.114501953125 Test Acc: 0.111328125\n",
      "Step 959 -- Train loss: 2.6595346927642822, Train Acc: 0.116943359375 Test Acc: 0.11328125\n",
      "Step 960 -- Train loss: 2.659083604812622, Train Acc: 0.118408203125 Test Acc: 0.11328125\n",
      "Step 961 -- Train loss: 2.6578121185302734, Train Acc: 0.1201171875 Test Acc: 0.125\n",
      "Step 962 -- Train loss: 2.6567535400390625, Train Acc: 0.11572265625 Test Acc: 0.11328125\n",
      "Step 963 -- Train loss: 2.657579183578491, Train Acc: 0.1240234375 Test Acc: 0.1318359375\n",
      "Step 964 -- Train loss: 2.656968832015991, Train Acc: 0.12451171875 Test Acc: 0.119140625\n",
      "Step 965 -- Train loss: 2.6564273834228516, Train Acc: 0.125732421875 Test Acc: 0.1171875\n",
      "Step 966 -- Train loss: 2.6578474044799805, Train Acc: 0.121337890625 Test Acc: 0.123046875\n",
      "Step 967 -- Train loss: 2.657486915588379, Train Acc: 0.123046875 Test Acc: 0.1123046875\n",
      "Step 968 -- Train loss: 2.658060073852539, Train Acc: 0.114501953125 Test Acc: 0.1162109375\n",
      "Step 969 -- Train loss: 2.655696392059326, Train Acc: 0.123046875 Test Acc: 0.111328125\n",
      "Step 970 -- Train loss: 2.6576719284057617, Train Acc: 0.11669921875 Test Acc: 0.138671875\n",
      "Step 971 -- Train loss: 2.657303810119629, Train Acc: 0.114990234375 Test Acc: 0.1162109375\n",
      "Step 972 -- Train loss: 2.656244993209839, Train Acc: 0.12255859375 Test Acc: 0.1142578125\n",
      "Step 973 -- Train loss: 2.6574718952178955, Train Acc: 0.12060546875 Test Acc: 0.1337890625\n",
      "Step 974 -- Train loss: 2.658226728439331, Train Acc: 0.110595703125 Test Acc: 0.1318359375\n",
      "Step 975 -- Train loss: 2.6560184955596924, Train Acc: 0.128173828125 Test Acc: 0.119140625\n",
      "Step 976 -- Train loss: 2.6568515300750732, Train Acc: 0.11767578125 Test Acc: 0.11328125\n",
      "Step 977 -- Train loss: 2.656747341156006, Train Acc: 0.121826171875 Test Acc: 0.1142578125\n",
      "Step 978 -- Train loss: 2.656470537185669, Train Acc: 0.122314453125 Test Acc: 0.11328125\n",
      "Step 979 -- Train loss: 2.6581742763519287, Train Acc: 0.116943359375 Test Acc: 0.10546875\n",
      "Step 980 -- Train loss: 2.6581759452819824, Train Acc: 0.116943359375 Test Acc: 0.12109375\n",
      "Step 981 -- Train loss: 2.657895088195801, Train Acc: 0.1162109375 Test Acc: 0.1181640625\n",
      "Step 982 -- Train loss: 2.655348300933838, Train Acc: 0.1220703125 Test Acc: 0.1142578125\n",
      "Step 983 -- Train loss: 2.6581995487213135, Train Acc: 0.12646484375 Test Acc: 0.1201171875\n",
      "Step 984 -- Train loss: 2.6574032306671143, Train Acc: 0.119873046875 Test Acc: 0.107421875\n",
      "Step 985 -- Train loss: 2.6569809913635254, Train Acc: 0.1201171875 Test Acc: 0.1259765625\n",
      "Step 986 -- Train loss: 2.657743215560913, Train Acc: 0.1162109375 Test Acc: 0.1220703125\n",
      "Step 987 -- Train loss: 2.655355930328369, Train Acc: 0.123046875 Test Acc: 0.1181640625\n",
      "Step 988 -- Train loss: 2.6593687534332275, Train Acc: 0.11865234375 Test Acc: 0.1201171875\n",
      "Step 989 -- Train loss: 2.6572749614715576, Train Acc: 0.12158203125 Test Acc: 0.1083984375\n",
      "Step 990 -- Train loss: 2.659867286682129, Train Acc: 0.11767578125 Test Acc: 0.123046875\n",
      "Step 991 -- Train loss: 2.656545400619507, Train Acc: 0.11962890625 Test Acc: 0.099609375\n",
      "Step 992 -- Train loss: 2.6580045223236084, Train Acc: 0.123291015625 Test Acc: 0.125\n",
      "Step 993 -- Train loss: 2.6583263874053955, Train Acc: 0.119384765625 Test Acc: 0.1171875\n",
      "Step 994 -- Train loss: 2.6577794551849365, Train Acc: 0.1142578125 Test Acc: 0.109375\n",
      "Step 995 -- Train loss: 2.6557109355926514, Train Acc: 0.122802734375 Test Acc: 0.125\n",
      "Step 996 -- Train loss: 2.658095598220825, Train Acc: 0.114501953125 Test Acc: 0.1201171875\n",
      "Step 997 -- Train loss: 2.6585099697113037, Train Acc: 0.119140625 Test Acc: 0.1201171875\n",
      "Step 998 -- Train loss: 2.6577467918395996, Train Acc: 0.117919921875 Test Acc: 0.1142578125\n",
      "Step 999 -- Train loss: 2.657869338989258, Train Acc: 0.12353515625 Test Acc: 0.1201171875\n"
     ]
    }
   ],
   "source": [
    "### Training on all\n",
    "for step in range(1000):\n",
    "    train_step(\n",
    "        model=model,\n",
    "        optim=optim,\n",
    "        data_sampler=data_sampler,\n",
    "        step=step,\n",
    "        config=config,\n",
    "        mask_idx=None\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>att_prog_measure</td><td></td></tr><tr><td>data_repeat_frac</td><td></td></tr><tr><td>idx0_check</td><td></td></tr><tr><td>idx10_check</td><td></td></tr><tr><td>idx11_check</td><td></td></tr><tr><td>idx12_check</td><td></td></tr><tr><td>idx13_check</td><td></td></tr><tr><td>idx14_check</td><td></td></tr><tr><td>idx15_check</td><td></td></tr><tr><td>idx1_check</td><td></td></tr><tr><td>idx2_check</td><td></td></tr><tr><td>idx3_check</td><td></td></tr><tr><td>idx4_check</td><td></td></tr><tr><td>idx5_check</td><td></td></tr><tr><td>idx6_check</td><td></td></tr><tr><td>idx7_check</td><td></td></tr><tr><td>idx8_check</td><td></td></tr><tr><td>idx9_check</td><td></td></tr><tr><td>mean_cosine_sim</td><td></td></tr><tr><td>mean_cosine_sim_0</td><td></td></tr><tr><td>mean_cosine_sim_1</td><td></td></tr><tr><td>mean_cosine_sim_10</td><td></td></tr><tr><td>mean_cosine_sim_11</td><td></td></tr><tr><td>mean_cosine_sim_12</td><td></td></tr><tr><td>mean_cosine_sim_13</td><td></td></tr><tr><td>mean_cosine_sim_14</td><td></td></tr><tr><td>mean_cosine_sim_2</td><td></td></tr><tr><td>mean_cosine_sim_3</td><td></td></tr><tr><td>mean_cosine_sim_4</td><td></td></tr><tr><td>mean_cosine_sim_5</td><td></td></tr><tr><td>mean_cosine_sim_6</td><td></td></tr><tr><td>mean_cosine_sim_7</td><td></td></tr><tr><td>mean_cosine_sim_8</td><td></td></tr><tr><td>mean_cosine_sim_9</td><td></td></tr><tr><td>model_repeat_frac</td><td></td></tr><tr><td>test_acc</td><td></td></tr><tr><td>train_acc</td><td></td></tr><tr><td>train_loss</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>att_prog_measure</td><td>0.06269</td></tr><tr><td>data_repeat_frac</td><td>0.06875</td></tr><tr><td>idx0_check</td><td>1.0</td></tr><tr><td>idx10_check</td><td>0.04297</td></tr><tr><td>idx11_check</td><td>0.09375</td></tr><tr><td>idx12_check</td><td>0.05469</td></tr><tr><td>idx13_check</td><td>0.0625</td></tr><tr><td>idx14_check</td><td>0.07422</td></tr><tr><td>idx15_check</td><td>0.03906</td></tr><tr><td>idx1_check</td><td>0.05469</td></tr><tr><td>idx2_check</td><td>0.07031</td></tr><tr><td>idx3_check</td><td>0.06641</td></tr><tr><td>idx4_check</td><td>0.07422</td></tr><tr><td>idx5_check</td><td>0.08594</td></tr><tr><td>idx6_check</td><td>0.05859</td></tr><tr><td>idx7_check</td><td>0.07422</td></tr><tr><td>idx8_check</td><td>0.05469</td></tr><tr><td>idx9_check</td><td>0.07031</td></tr><tr><td>mean_cosine_sim</td><td>0.99994</td></tr><tr><td>mean_cosine_sim_0</td><td>0.02853</td></tr><tr><td>mean_cosine_sim_1</td><td>0.99988</td></tr><tr><td>mean_cosine_sim_10</td><td>0.99996</td></tr><tr><td>mean_cosine_sim_11</td><td>0.99996</td></tr><tr><td>mean_cosine_sim_12</td><td>0.99996</td></tr><tr><td>mean_cosine_sim_13</td><td>0.99996</td></tr><tr><td>mean_cosine_sim_14</td><td>0.99996</td></tr><tr><td>mean_cosine_sim_2</td><td>0.99995</td></tr><tr><td>mean_cosine_sim_3</td><td>0.99995</td></tr><tr><td>mean_cosine_sim_4</td><td>0.99996</td></tr><tr><td>mean_cosine_sim_5</td><td>0.99995</td></tr><tr><td>mean_cosine_sim_6</td><td>0.99995</td></tr><tr><td>mean_cosine_sim_7</td><td>0.99995</td></tr><tr><td>mean_cosine_sim_8</td><td>0.99992</td></tr><tr><td>mean_cosine_sim_9</td><td>0.99996</td></tr><tr><td>model_repeat_frac</td><td>0.66563</td></tr><tr><td>test_acc</td><td>0.12012</td></tr><tr><td>train_acc</td><td>0.12354</td></tr><tr><td>train_loss</td><td>2.65787</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">mws_softmax_attention_loss_noinput_indexTrainTest</strong> at: <a href='https://wandb.ai/wth_ucsd/loss_plateau_tf/runs/gei2zj11' target=\"_blank\">https://wandb.ai/wth_ucsd/loss_plateau_tf/runs/gei2zj11</a><br/>Synced 6 W&B file(s), 1114 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251104_022613-gei2zj11/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
