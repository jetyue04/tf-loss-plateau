model:
  n_layer: 1
  n_head: 1
  n_embd: 256
  linear: True

data:
  tasks:
    - name: 'MovingWindowSum'
      sep: 17
      n_train: 256
      n_test: 64
  min_num: 1
  max_num: 16
  k: 2 # Window size
  p: 17 # Mod p - Dicates Vocab Size
  cot: False # No idea what this is - can delete
  num_tokens: 16
  n_train: 256
  n_test: 64
  fixed_len: True
  mix: 'random'

train:
  lr: 0.0001
  grad_clip: -1
  num_steps: 500
  norm_type: "none_rank"
  wandb: True
  wandb_run_name: 'mws_linear_embeddingFrozen'
  wandb_project: 'loss_plateau_tf'
  save_ckpt: False
  ckpt_freq: 20
  seed: 67
  mask_input: True # If we are masking the loss on the input sequence
  freeze_embedding: True