{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "YiF5Vq1LGhEw"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import yaml\n",
    "import argparse\n",
    "from dotmap import DotMap\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.optim import Adam\n",
    "from torch.nn.functional import cosine_similarity\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"./src\")  # make sure Python can find src/\n",
    "from model_linear import GPTLinear\n",
    "from model_softmax import GPTSoftmax\n",
    "from data import MovingWindowSum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "def train_step(\n",
    "    model,\n",
    "    optim,\n",
    "    data_sampler,\n",
    "    step,\n",
    "    config,\n",
    "):\n",
    "    n_train, n_test, num_tokens = (\n",
    "        config.data.n_train,\n",
    "        config.data.n_test,\n",
    "        config.data.num_tokens,\n",
    "    )\n",
    "\n",
    "    data = data_sampler.sample(\n",
    "        num_samples=n_train + n_test,\n",
    "        num_tokens=num_tokens,\n",
    "    )\n",
    "\n",
    "    train_data = data[:n_train, :]\n",
    "    test_data = data[n_train:, :]\n",
    "\n",
    "    prompt_len = num_tokens + 1\n",
    "    gen_len = num_tokens\n",
    "    acc_start = num_tokens + 1\n",
    "\n",
    "    model.train()\n",
    "    optim.zero_grad(set_to_none=True)\n",
    "\n",
    "    _, _, _, loss = model(\n",
    "        train_data[:, :-1], targets=train_data[:, 1:], prompt_len =prompt_len,\n",
    "    )\n",
    "    loss.backward()\n",
    "\n",
    "    if config.train.grad_clip > 0:\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), config.train.grad_clip)\n",
    "\n",
    "    optim.step()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Log train loss, train / test acc, repetition frequency\n",
    "        attn_map, pre_lm_h, _, train_loss = model(train_data[:, :-1], targets=train_data[:, 1:], prompt_len =prompt_len,)\n",
    "\n",
    "        train_pred = model.generate(\n",
    "            idx=train_data[:, :prompt_len],\n",
    "            max_new_tokens=gen_len,\n",
    "            prompt_len =prompt_len,\n",
    "        )\n",
    "        test_pred = model.generate(\n",
    "            idx=test_data[:, :prompt_len],\n",
    "            max_new_tokens=gen_len,\n",
    "            prompt_len =prompt_len,\n",
    "        )\n",
    "\n",
    "        train_acc = torch.mean(\n",
    "            (train_pred[:, acc_start:] == train_data[:, acc_start:]).to(float)\n",
    "        ).item()\n",
    "        test_acc = torch.mean(\n",
    "            (test_pred[:, acc_start:] == test_data[:, acc_start:]).to(float)\n",
    "        ).item()\n",
    "\n",
    "        data_repeat_frac = torch.mean((test_data[:, acc_start:-1] == test_data[:, acc_start+1:]).to(float))\n",
    "        model_repeat_frac = torch.mean((test_pred[:, acc_start:-1] == test_pred[:, acc_start+1:]).to(float))\n",
    "\n",
    "        # Log attention progress measure\n",
    "        attn_map_output_seq = attn_map[:, :, acc_start-1:]\n",
    "        att_mask = torch.zeros_like(attn_map_output_seq).to(device)\n",
    "\n",
    "        att_mask[:, :, 0, 0] = 1\n",
    "        for i in range(num_tokens - 1):\n",
    "            att_mask[:, :, i + 1, i : i + 2] = 1\n",
    "\n",
    "        att_prog_measure = torch.mean(\n",
    "            torch.sum(torch.abs(attn_map_output_seq) * att_mask, dim=(-3, -2, -1)) /\n",
    "            torch.sum(torch.abs(attn_map_output_seq), dim=(-3, -2, -1)),\n",
    "            dim=0\n",
    "        )\n",
    "\n",
    "        # Log pair-wise cosine similarity between hidden states\n",
    "        embed_start = acc_start - 1\n",
    "        embed_len = gen_len\n",
    "\n",
    "        logit_cs = torch.zeros((embed_len, embed_len))\n",
    "\n",
    "        for i_1 in range(embed_start, embed_start + embed_len):\n",
    "            for i_2 in range(embed_start, i_1):\n",
    "                logit_cs[i_1 - embed_start, i_2 - embed_start] = torch.mean(\n",
    "                    (\n",
    "                        cosine_similarity(\n",
    "                            pre_lm_h[:, i_1, :], pre_lm_h[:, i_2, :], dim=-1\n",
    "                        )\n",
    "                    ), dim=0\n",
    "                )\n",
    "\n",
    "        # Log plots for cosine similarity, attention map\n",
    "        logit_fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(30, 15))\n",
    "\n",
    "        im1 = ax[0].imshow(logit_cs)\n",
    "        ax[0].set_title(\"avg pre_lm_h cosine sim\")\n",
    "        cb1 = logit_fig.colorbar(im1, location=\"right\", shrink=0.99, pad=0.02, ax=ax[0])\n",
    "\n",
    "        avg_attn_map = torch.mean(attn_map, dim=0).squeeze().detach().cpu().numpy()\n",
    "\n",
    "        im2 = ax[1].imshow(avg_attn_map)\n",
    "        ax[1].set_title(\"att map\")\n",
    "        cb4 = logit_fig.colorbar(im2, location=\"right\", shrink=0.99, pad=0.02, ax=ax[1])\n",
    "        ax[1].set_xticks(range(avg_attn_map.shape[-1]))\n",
    "        ax[1].set_yticks(range(avg_attn_map.shape[-2]))\n",
    "\n",
    "        for i1 in range(embed_len):\n",
    "            for i2 in range(embed_len):\n",
    "                text1 = ax[0].text(\n",
    "                    i2,\n",
    "                    i1,\n",
    "                    round(logit_cs[i1, i2].item(), 2),\n",
    "                    ha=\"center\",\n",
    "                    va=\"center\",\n",
    "                    color=\"w\",\n",
    "                )\n",
    "\n",
    "\n",
    "        print(\n",
    "            f\"Step {step} -- Train loss: {train_loss}, Train Acc: {train_acc} Test Acc: {test_acc}\"\n",
    "        )\n",
    "        # print(f\"input: {test_data[0]} \\n predicted:{test_pred[0]}\")\n",
    "\n",
    "        if config.train.wandb:\n",
    "\n",
    "            log_data = {\n",
    "                \"train_loss\": train_loss,\n",
    "                \"train_acc\": train_acc,\n",
    "                \"test_acc\": test_acc,\n",
    "                \"data_repeat_frac\": data_repeat_frac,\n",
    "                \"model_repeat_frac\": model_repeat_frac,\n",
    "                \"att_prog_measure\": att_prog_measure,\n",
    "                \"pre_lm_h_cosine_sim\": logit_fig,\n",
    "                \"mean_cosine_sim\": torch.sum(logit_cs[:, 1:]) / (0.5 * (gen_len-1) * (gen_len-2))\n",
    "            }\n",
    "\n",
    "            for output_pos in range(gen_len):\n",
    "                log_data.update(\n",
    "                    {\n",
    "                        f\"idx{output_pos}_check\": torch.mean(\n",
    "                            (train_pred[:, acc_start + output_pos] == train_data[:, acc_start + output_pos]).to(float)\n",
    "                        ).item()\n",
    "                    }\n",
    "                )\n",
    "\n",
    "                if output_pos < gen_len-1:\n",
    "                    log_data.update(\n",
    "                        {\n",
    "                            f\"mean_cosine_sim_{output_pos}\": torch.sum(logit_cs[:, output_pos]) / (gen_len-1-output_pos)\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "            wandb.log(log_data)\n",
    "\n",
    "        plt.close()\n",
    "        del (\n",
    "            logit_fig,\n",
    "            ax,\n",
    "            logit_cs,\n",
    "        )\n",
    "\n",
    "        if config.train.save_ckpt:\n",
    "            if (step == 0) or ((step + 1) % config.train.ckpt_freq == 0):\n",
    "                model.train()\n",
    "                torch.save(\n",
    "                    {\n",
    "                        \"epoch\": step,\n",
    "                        \"model\": model.state_dict(),\n",
    "                        \"optim\": optim.state_dict(),\n",
    "                        \"train_loss\": train_loss,\n",
    "                        \"test_acc\": test_acc,\n",
    "                    },\n",
    "                    \"./mws_k2_l1_h1_a16_n16.tar\",\n",
    "                )\n",
    "                print(f\"saved state at epoch {step} to {f'./mws_k2_l1_h1_a16_n16.tar'}\")\n",
    "\n",
    "                if config.train.wandb:\n",
    "                    model_wandb = wandb.Artifact(\n",
    "                        f\"model_step{step}\", type=\"model\"\n",
    "                    )\n",
    "                    model_wandb.add_file(f\"./mws_k2_l1_h1_a16_n16.tar\")\n",
    "                    wandb.log_artifact(model_wandb)\n",
    "                    print(\"model uploaded to wandb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# device = \"cpu\"\n",
    "\n",
    "config = {\n",
    "'model':\n",
    "  {\n",
    "    'n_layer': 1,\n",
    "    'n_head': 1,\n",
    "    'n_embd': 256,\n",
    "    'linear': True,\n",
    "  },\n",
    "\n",
    "'data':\n",
    "  {\n",
    "    'name': 'window',\n",
    "    'min_num': 1,\n",
    "    'max_num': 16,\n",
    "    'k': 2,\n",
    "    'p': 17,\n",
    "    'sep': 17,\n",
    "    'cot': False,\n",
    "    'num_tokens': 16,\n",
    "    'n_train': 256,\n",
    "    'n_test': 64,\n",
    "    'fixed_len': True,\n",
    "  },\n",
    "\n",
    "'train':\n",
    "  {\n",
    "    'lr': 0.0001,\n",
    "    'grad_clip': -1,\n",
    "    'num_steps': 500,\n",
    "    'norm_type': \"none_rank\",\n",
    "    'wandb': True,\n",
    "    'save_ckpt': False,\n",
    "    'ckpt_freq': 20,\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:h7bzaxc4) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">mws_linear_frozen_embedding_test</strong> at: <a href='https://wandb.ai/wth_ucsd/loss_plateau_tf/runs/h7bzaxc4' target=\"_blank\">https://wandb.ai/wth_ucsd/loss_plateau_tf/runs/h7bzaxc4</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251024_004754-h7bzaxc4/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:h7bzaxc4). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.22.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jyue/private/tf-loss-plateau/wandb/run-20251024_004829-xwbyyp9g</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/wth_ucsd/loss_plateau_tf/runs/xwbyyp9g' target=\"_blank\">mws_linear_frozen_embedding_test</a></strong> to <a href='https://wandb.ai/wth_ucsd/loss_plateau_tf' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/wth_ucsd/loss_plateau_tf' target=\"_blank\">https://wandb.ai/wth_ucsd/loss_plateau_tf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/wth_ucsd/loss_plateau_tf/runs/xwbyyp9g' target=\"_blank\">https://wandb.ai/wth_ucsd/loss_plateau_tf/runs/xwbyyp9g</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0 -- Train loss: 2.875150680541992, Train Acc: 0.0693359375 Test Acc: 0.0615234375\n",
      "Step 1 -- Train loss: 2.8703408241271973, Train Acc: 0.069091796875 Test Acc: 0.0634765625\n",
      "Step 2 -- Train loss: 2.86405086517334, Train Acc: 0.066162109375 Test Acc: 0.0556640625\n",
      "Step 3 -- Train loss: 2.849264621734619, Train Acc: 0.067138671875 Test Acc: 0.0673828125\n",
      "Step 4 -- Train loss: 2.8544020652770996, Train Acc: 0.074462890625 Test Acc: 0.0634765625\n",
      "Step 5 -- Train loss: 2.8404016494750977, Train Acc: 0.076416015625 Test Acc: 0.06640625\n",
      "Step 6 -- Train loss: 2.841900110244751, Train Acc: 0.077880859375 Test Acc: 0.0771484375\n",
      "Step 7 -- Train loss: 2.8252975940704346, Train Acc: 0.0830078125 Test Acc: 0.0791015625\n",
      "Step 8 -- Train loss: 2.825171709060669, Train Acc: 0.092041015625 Test Acc: 0.0791015625\n",
      "Step 9 -- Train loss: 2.8165249824523926, Train Acc: 0.09619140625 Test Acc: 0.0947265625\n",
      "Step 10 -- Train loss: 2.813645362854004, Train Acc: 0.10546875 Test Acc: 0.1083984375\n",
      "Step 11 -- Train loss: 2.810664415359497, Train Acc: 0.112548828125 Test Acc: 0.103515625\n",
      "Step 12 -- Train loss: 2.7990307807922363, Train Acc: 0.1171875 Test Acc: 0.1103515625\n",
      "Step 13 -- Train loss: 2.7927629947662354, Train Acc: 0.114990234375 Test Acc: 0.1171875\n",
      "Step 14 -- Train loss: 2.7856078147888184, Train Acc: 0.1171875 Test Acc: 0.1142578125\n",
      "Step 15 -- Train loss: 2.7734711170196533, Train Acc: 0.11083984375 Test Acc: 0.1220703125\n",
      "Step 16 -- Train loss: 2.7614498138427734, Train Acc: 0.11572265625 Test Acc: 0.1181640625\n",
      "Step 17 -- Train loss: 2.756711959838867, Train Acc: 0.10986328125 Test Acc: 0.1083984375\n",
      "Step 18 -- Train loss: 2.7455835342407227, Train Acc: 0.114990234375 Test Acc: 0.111328125\n",
      "Step 19 -- Train loss: 2.7366714477539062, Train Acc: 0.11181640625 Test Acc: 0.109375\n",
      "Step 20 -- Train loss: 2.736407995223999, Train Acc: 0.111572265625 Test Acc: 0.1181640625\n",
      "Step 21 -- Train loss: 2.722336769104004, Train Acc: 0.1171875 Test Acc: 0.109375\n",
      "Step 22 -- Train loss: 2.720768928527832, Train Acc: 0.110595703125 Test Acc: 0.1201171875\n",
      "Step 23 -- Train loss: 2.7142491340637207, Train Acc: 0.107421875 Test Acc: 0.1123046875\n",
      "Step 24 -- Train loss: 2.7105841636657715, Train Acc: 0.1142578125 Test Acc: 0.111328125\n",
      "Step 25 -- Train loss: 2.7005558013916016, Train Acc: 0.119384765625 Test Acc: 0.109375\n",
      "Step 26 -- Train loss: 2.70035982131958, Train Acc: 0.1162109375 Test Acc: 0.11328125\n",
      "Step 27 -- Train loss: 2.6946535110473633, Train Acc: 0.11962890625 Test Acc: 0.1103515625\n",
      "Step 28 -- Train loss: 2.691153049468994, Train Acc: 0.119384765625 Test Acc: 0.1064453125\n",
      "Step 29 -- Train loss: 2.685351610183716, Train Acc: 0.1201171875 Test Acc: 0.1318359375\n",
      "Step 30 -- Train loss: 2.6802420616149902, Train Acc: 0.11328125 Test Acc: 0.1220703125\n",
      "Step 31 -- Train loss: 2.6808016300201416, Train Acc: 0.118408203125 Test Acc: 0.126953125\n",
      "Step 32 -- Train loss: 2.6768336296081543, Train Acc: 0.11572265625 Test Acc: 0.109375\n",
      "Step 33 -- Train loss: 2.6735572814941406, Train Acc: 0.113037109375 Test Acc: 0.1220703125\n",
      "Step 34 -- Train loss: 2.6720852851867676, Train Acc: 0.114990234375 Test Acc: 0.1103515625\n",
      "Step 35 -- Train loss: 2.67262864112854, Train Acc: 0.116455078125 Test Acc: 0.134765625\n",
      "Step 36 -- Train loss: 2.6674134731292725, Train Acc: 0.119873046875 Test Acc: 0.1220703125\n",
      "Step 37 -- Train loss: 2.67172908782959, Train Acc: 0.11962890625 Test Acc: 0.1142578125\n",
      "Step 38 -- Train loss: 2.6736364364624023, Train Acc: 0.11865234375 Test Acc: 0.1064453125\n",
      "Step 39 -- Train loss: 2.6685123443603516, Train Acc: 0.12353515625 Test Acc: 0.12109375\n",
      "Step 40 -- Train loss: 2.6621768474578857, Train Acc: 0.12158203125 Test Acc: 0.1279296875\n",
      "Step 41 -- Train loss: 2.660905361175537, Train Acc: 0.12109375 Test Acc: 0.126953125\n",
      "Step 42 -- Train loss: 2.6623311042785645, Train Acc: 0.12255859375 Test Acc: 0.1298828125\n",
      "Step 43 -- Train loss: 2.659080982208252, Train Acc: 0.125 Test Acc: 0.1376953125\n",
      "Step 44 -- Train loss: 2.657505750656128, Train Acc: 0.120849609375 Test Acc: 0.12109375\n",
      "Step 45 -- Train loss: 2.6588711738586426, Train Acc: 0.11669921875 Test Acc: 0.119140625\n",
      "Step 46 -- Train loss: 2.660402774810791, Train Acc: 0.11767578125 Test Acc: 0.1337890625\n",
      "Step 47 -- Train loss: 2.6527254581451416, Train Acc: 0.128662109375 Test Acc: 0.126953125\n",
      "Step 48 -- Train loss: 2.6472809314727783, Train Acc: 0.121337890625 Test Acc: 0.125\n",
      "Step 49 -- Train loss: 2.6529135704040527, Train Acc: 0.128662109375 Test Acc: 0.119140625\n",
      "Step 50 -- Train loss: 2.6547975540161133, Train Acc: 0.123779296875 Test Acc: 0.1259765625\n",
      "Step 51 -- Train loss: 2.6588988304138184, Train Acc: 0.125 Test Acc: 0.123046875\n",
      "Step 52 -- Train loss: 2.6521525382995605, Train Acc: 0.125244140625 Test Acc: 0.1259765625\n",
      "Step 53 -- Train loss: 2.649733543395996, Train Acc: 0.124755859375 Test Acc: 0.1279296875\n",
      "Step 54 -- Train loss: 2.652594566345215, Train Acc: 0.1181640625 Test Acc: 0.1201171875\n",
      "Step 55 -- Train loss: 2.6476328372955322, Train Acc: 0.131591796875 Test Acc: 0.111328125\n",
      "Step 56 -- Train loss: 2.6484663486480713, Train Acc: 0.1201171875 Test Acc: 0.1337890625\n",
      "Step 57 -- Train loss: 2.6460957527160645, Train Acc: 0.125732421875 Test Acc: 0.1201171875\n",
      "Step 58 -- Train loss: 2.6462206840515137, Train Acc: 0.123046875 Test Acc: 0.1357421875\n",
      "Step 59 -- Train loss: 2.648172616958618, Train Acc: 0.128173828125 Test Acc: 0.12109375\n",
      "Step 60 -- Train loss: 2.6415514945983887, Train Acc: 0.130126953125 Test Acc: 0.130859375\n",
      "Step 61 -- Train loss: 2.645538806915283, Train Acc: 0.131103515625 Test Acc: 0.1357421875\n",
      "Step 62 -- Train loss: 2.6403050422668457, Train Acc: 0.12255859375 Test Acc: 0.1318359375\n",
      "Step 63 -- Train loss: 2.6423842906951904, Train Acc: 0.127197265625 Test Acc: 0.126953125\n",
      "Step 64 -- Train loss: 2.6442980766296387, Train Acc: 0.122314453125 Test Acc: 0.125\n",
      "Step 65 -- Train loss: 2.6419801712036133, Train Acc: 0.135009765625 Test Acc: 0.1220703125\n",
      "Step 66 -- Train loss: 2.6363394260406494, Train Acc: 0.130859375 Test Acc: 0.126953125\n",
      "Step 67 -- Train loss: 2.635171890258789, Train Acc: 0.125244140625 Test Acc: 0.1376953125\n",
      "Step 68 -- Train loss: 2.6321427822113037, Train Acc: 0.129150390625 Test Acc: 0.1171875\n",
      "Step 69 -- Train loss: 2.6372811794281006, Train Acc: 0.127197265625 Test Acc: 0.119140625\n",
      "Step 70 -- Train loss: 2.632657766342163, Train Acc: 0.133544921875 Test Acc: 0.1455078125\n",
      "Step 71 -- Train loss: 2.630819320678711, Train Acc: 0.127197265625 Test Acc: 0.1240234375\n",
      "Step 72 -- Train loss: 2.62966251373291, Train Acc: 0.1259765625 Test Acc: 0.1240234375\n",
      "Step 73 -- Train loss: 2.6311213970184326, Train Acc: 0.1279296875 Test Acc: 0.11328125\n",
      "Step 74 -- Train loss: 2.623741865158081, Train Acc: 0.128173828125 Test Acc: 0.1259765625\n",
      "Step 75 -- Train loss: 2.624110698699951, Train Acc: 0.1376953125 Test Acc: 0.125\n",
      "Step 76 -- Train loss: 2.612480640411377, Train Acc: 0.130126953125 Test Acc: 0.1357421875\n",
      "Step 77 -- Train loss: 2.622525691986084, Train Acc: 0.1318359375 Test Acc: 0.130859375\n",
      "Step 78 -- Train loss: 2.6116678714752197, Train Acc: 0.126708984375 Test Acc: 0.12109375\n",
      "Step 79 -- Train loss: 2.6065523624420166, Train Acc: 0.141357421875 Test Acc: 0.138671875\n",
      "Step 80 -- Train loss: 2.609459161758423, Train Acc: 0.138916015625 Test Acc: 0.123046875\n",
      "Step 81 -- Train loss: 2.607414722442627, Train Acc: 0.134521484375 Test Acc: 0.1328125\n",
      "Step 82 -- Train loss: 2.603729724884033, Train Acc: 0.134033203125 Test Acc: 0.138671875\n",
      "Step 83 -- Train loss: 2.607404947280884, Train Acc: 0.139892578125 Test Acc: 0.138671875\n",
      "Step 84 -- Train loss: 2.6006479263305664, Train Acc: 0.14306640625 Test Acc: 0.1337890625\n",
      "Step 85 -- Train loss: 2.5961413383483887, Train Acc: 0.135009765625 Test Acc: 0.1328125\n",
      "Step 86 -- Train loss: 2.587374687194824, Train Acc: 0.146484375 Test Acc: 0.16796875\n",
      "Step 87 -- Train loss: 2.573333978652954, Train Acc: 0.141357421875 Test Acc: 0.1513671875\n",
      "Step 88 -- Train loss: 2.5728869438171387, Train Acc: 0.1455078125 Test Acc: 0.142578125\n",
      "Step 89 -- Train loss: 2.5659520626068115, Train Acc: 0.143798828125 Test Acc: 0.1396484375\n",
      "Step 90 -- Train loss: 2.5679311752319336, Train Acc: 0.156982421875 Test Acc: 0.1591796875\n",
      "Step 91 -- Train loss: 2.566075325012207, Train Acc: 0.159423828125 Test Acc: 0.1533203125\n",
      "Step 92 -- Train loss: 2.5536293983459473, Train Acc: 0.1611328125 Test Acc: 0.138671875\n",
      "Step 93 -- Train loss: 2.5332839488983154, Train Acc: 0.170654296875 Test Acc: 0.1806640625\n",
      "Step 94 -- Train loss: 2.522251605987549, Train Acc: 0.184326171875 Test Acc: 0.1796875\n",
      "Step 95 -- Train loss: 2.5172524452209473, Train Acc: 0.17822265625 Test Acc: 0.2060546875\n",
      "Step 96 -- Train loss: 2.499601364135742, Train Acc: 0.19921875 Test Acc: 0.2109375\n",
      "Step 97 -- Train loss: 2.4778051376342773, Train Acc: 0.20654296875 Test Acc: 0.201171875\n",
      "Step 98 -- Train loss: 2.4623677730560303, Train Acc: 0.213134765625 Test Acc: 0.1982421875\n",
      "Step 99 -- Train loss: 2.449263095855713, Train Acc: 0.22412109375 Test Acc: 0.23828125\n",
      "Step 100 -- Train loss: 2.4132561683654785, Train Acc: 0.250732421875 Test Acc: 0.2373046875\n",
      "Step 101 -- Train loss: 2.381268262863159, Train Acc: 0.2763671875 Test Acc: 0.2548828125\n",
      "Step 102 -- Train loss: 2.357062339782715, Train Acc: 0.296630859375 Test Acc: 0.30078125\n",
      "Step 103 -- Train loss: 2.318127393722534, Train Acc: 0.32861328125 Test Acc: 0.3232421875\n",
      "Step 104 -- Train loss: 2.2899160385131836, Train Acc: 0.355224609375 Test Acc: 0.3642578125\n",
      "Step 105 -- Train loss: 2.2400453090667725, Train Acc: 0.4248046875 Test Acc: 0.4091796875\n",
      "Step 106 -- Train loss: 2.1921069622039795, Train Acc: 0.494384765625 Test Acc: 0.490234375\n",
      "Step 107 -- Train loss: 2.142489194869995, Train Acc: 0.550048828125 Test Acc: 0.529296875\n",
      "Step 108 -- Train loss: 2.0840044021606445, Train Acc: 0.60693359375 Test Acc: 0.578125\n",
      "Step 109 -- Train loss: 2.021815776824951, Train Acc: 0.669189453125 Test Acc: 0.6669921875\n",
      "Step 110 -- Train loss: 1.9354432821273804, Train Acc: 0.73828125 Test Acc: 0.716796875\n",
      "Step 111 -- Train loss: 1.8539172410964966, Train Acc: 0.790283203125 Test Acc: 0.7919921875\n",
      "Step 112 -- Train loss: 1.7697038650512695, Train Acc: 0.8291015625 Test Acc: 0.828125\n",
      "Step 113 -- Train loss: 1.6492348909378052, Train Acc: 0.86669921875 Test Acc: 0.8486328125\n"
     ]
    }
   ],
   "source": [
    "config = DotMap(config)\n",
    "\n",
    "config.model.vocab_size = max(config.data.p, config.data.max_num) + 1\n",
    "config.model.block_size = 2 * config.data.num_tokens + 1\n",
    "config.train.n_steps = 5\n",
    "\n",
    "data_sampler = MovingWindowSum(\n",
    "    min_num=config.data.min_num,\n",
    "    max_num=config.data.max_num,\n",
    "    k=config.data.k,\n",
    "    p=config.data.p,\n",
    "    device=device\n",
    ")\n",
    "model = GPTLinear(config.model, return_att=True).to(device)\n",
    "\n",
    "## Freeze embedding layer weights\n",
    "for param in model.transformer.wte.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in model.transformer.wpe.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Make sure optimizer only updates trainable parameters\n",
    "optim = Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=config.train.lr)\n",
    "\n",
    "if config.train.wandb:\n",
    "    wandb_run_name = 'mws_linear_frozen_embedding_test'\n",
    "    wandb.login(key=\"\")\n",
    "    wandb.init(project=\"loss_plateau_tf\", name=wandb_run_name, config=config)\n",
    "    wandb.watch(model)\n",
    "\n",
    "for step in range(config.train.num_steps):\n",
    "    train_step(\n",
    "        model=model,\n",
    "        optim=optim,\n",
    "        data_sampler=data_sampler,\n",
    "        step=step,\n",
    "        config=config,\n",
    "    )\n",
    "\n",
    "if config.train.wandb:\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
