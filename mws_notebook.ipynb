{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "YiF5Vq1LGhEw"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import yaml\n",
    "import argparse\n",
    "from dotmap import DotMap\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.optim import Adam\n",
    "from torch.nn.functional import cosine_similarity\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "pNvJqb60GnE3"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 19\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m(Modified version of) Andrej Karpathy's minGPT implementation (https://github.com/karpathy/minGPT/blob/master/mingpt/model.py)\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;124;03mhttps://github.com/huggingface/transformers/blob/main/src/transformers/models/gpt2/modeling_gpt2.py\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# from mingpt.utils import CfgNode as CN\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# -----------------------------------------------------------------------------\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mNewGELU\u001b[39;00m(\u001b[43mnn\u001b[49m\u001b[38;5;241m.\u001b[39mModule):\n\u001b[1;32m     20\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;124;03m    Implementation of the GELU activation function currently in Google BERT repo (identical to OpenAI GPT).\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;124;03m    Reference: Gaussian Error Linear Units (GELU) paper: https://arxiv.org/abs/1606.08415\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "# Model definition\n",
    "\"\"\"\n",
    "(Modified version of) Andrej Karpathy's minGPT implementation (https://github.com/karpathy/minGPT/blob/master/mingpt/model.py)\n",
    "\n",
    "Full definition of a GPT Language Model, all of it in this single file.\n",
    "\n",
    "References:\n",
    "1) the official GPT-2 TensorFlow implementation released by OpenAI:\n",
    "https://github.com/openai/gpt-2/blob/master/src/model.py\n",
    "2) huggingface/transformers PyTorch implementation:\n",
    "https://github.com/huggingface/transformers/blob/main/src/transformers/models/gpt2/modeling_gpt2.py\n",
    "\"\"\"\n",
    "\n",
    "# from mingpt.utils import CfgNode as CN\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "class NewGELU(nn.Module):\n",
    "    \"\"\"\n",
    "    Implementation of the GELU activation function currently in Google BERT repo (identical to OpenAI GPT).\n",
    "    Reference: Gaussian Error Linear Units (GELU) paper: https://arxiv.org/abs/1606.08415\n",
    "    \"\"\"\n",
    "\n",
    "    def forward(self, x):\n",
    "        return (\n",
    "            0.5\n",
    "            * x\n",
    "            * (\n",
    "                1.0\n",
    "                + torch.tanh(\n",
    "                    math.sqrt(2.0 / math.pi) * (x + 0.044715 * torch.pow(x, 3.0))\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    A vanilla multi-head masked self-attention layer with a projection at the end.\n",
    "    It is possible to use torch.nn.MultiheadAttention here but I am including an\n",
    "    explicit implementation here to show that there is nothing too scary here.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config, return_att=False):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "\n",
    "        # key, query, value projections for all heads\n",
    "        self.k = nn.Linear(config.n_embd, config.n_embd)\n",
    "        self.q = nn.Linear(config.n_embd, config.n_embd)\n",
    "        self.v = nn.Linear(config.n_embd, config.n_embd)\n",
    "\n",
    "        # output projection\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
    "\n",
    "        # causal mask to ensure that attention is only applied to the left in the input sequence\n",
    "        self.register_buffer(\n",
    "            \"bias\",\n",
    "            torch.tril(torch.ones(config.block_size, config.block_size)).view(\n",
    "                1, 1, config.block_size, config.block_size\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        self.return_att = return_att\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = (\n",
    "            x.size()\n",
    "        )  # batch size, sequence length, embedding dimensionality (n_embd)\n",
    "\n",
    "        # calculate query, key, values for all heads\n",
    "        q = self.q(x)\n",
    "        k = self.k(x)\n",
    "        v = self.v(x)\n",
    "\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(\n",
    "            1, 2\n",
    "        )  # (B, nh, T, hs)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(\n",
    "            1, 2\n",
    "        )  # (B, nh, T, hs)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(\n",
    "            1, 2\n",
    "        )  # (B, nh, T, hs)\n",
    "\n",
    "        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
    "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "        att = att.masked_fill(self.bias[:, :, :T, :T] == 0, 0.0)\n",
    "\n",
    "        att_copy = att.clone().detach()\n",
    "\n",
    "        y = att @ v  # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "        y = (\n",
    "            y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        )  # re-assemble all head outputs side by side\n",
    "\n",
    "        # output projection\n",
    "        y = self.c_proj(y)\n",
    "\n",
    "        if self.return_att:\n",
    "            return y, att_copy\n",
    "\n",
    "        return y\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\"an unassuming Transformer block\"\"\"\n",
    "\n",
    "    def __init__(self, config, return_att=False):\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
    "        self.attn = CausalSelfAttention(config, return_att=return_att)\n",
    "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
    "        self.mlp = nn.ModuleDict(\n",
    "            dict(\n",
    "                c_fc=nn.Linear(config.n_embd, 4 * config.n_embd),\n",
    "                c_proj=nn.Linear(4 * config.n_embd, config.n_embd),\n",
    "                act=NewGELU(),\n",
    "                # dropout=nn.Dropout(config.resid_pdrop),\n",
    "            )\n",
    "        )\n",
    "\n",
    "        m = self.mlp\n",
    "        self.mlpf = lambda x: m.c_proj(m.act(m.c_fc(x)))  # MLP forward\n",
    "        self.return_att = return_att\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.return_att:\n",
    "            x_prev, att = self.attn(self.ln_1(x))\n",
    "            x = x + x_prev\n",
    "\n",
    "            x = x + self.mlpf(self.ln_2(x))\n",
    "\n",
    "            return x, att\n",
    "\n",
    "        else:\n",
    "            x = x + self.attn(self.ln_1(x))\n",
    "            x = x + self.mlpf(self.ln_2(x))\n",
    "            return x\n",
    "\n",
    "\n",
    "class GPTLinear(nn.Module):\n",
    "    \"\"\"GPT Language Model\"\"\"\n",
    "\n",
    "    def __init__(self, config, return_att=False):\n",
    "        super().__init__()\n",
    "        assert config.vocab_size is not None\n",
    "        assert config.block_size is not None\n",
    "        self.block_size = config.block_size\n",
    "        self.return_att = return_att\n",
    "\n",
    "        self.transformer = nn.ModuleDict(\n",
    "            dict(\n",
    "                wte=nn.Embedding(config.vocab_size, config.n_embd),\n",
    "                wpe=nn.Embedding(config.block_size, config.n_embd),\n",
    "                h=nn.ModuleList(\n",
    "                    [\n",
    "                        Block(config, return_att=self.return_att)\n",
    "                        for _ in range(config.n_layer)\n",
    "                    ]\n",
    "                ),\n",
    "                ln_f=nn.LayerNorm(config.n_embd),\n",
    "            )\n",
    "        )\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "\n",
    "        # init all weights, and apply a special scaled init to the residual projections, per GPT-2 paper\n",
    "        self.apply(self._init_weights)\n",
    "        for pn, p in self.named_parameters():\n",
    "            if pn.endswith(\"c_proj.weight\"):\n",
    "                torch.nn.init.normal_(\n",
    "                    p, mean=0.0, std=0.02 / math.sqrt(2 * config.n_layer)\n",
    "                )\n",
    "\n",
    "        # report number of parameters (note we don't count the decoder parameters in lm_head)\n",
    "        n_params = sum(p.numel() for p in self.transformer.parameters())\n",
    "        # print(\"number of parameters: %.2fM\" % (n_params / 1e6,))\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            torch.nn.init.zeros_(module.bias)\n",
    "            torch.nn.init.ones_(module.weight)\n",
    "\n",
    "\n",
    "    # Only used for weight decay experiments -------------------------------------------\n",
    "    def configure_optimizers(self, train_config):\n",
    "        \"\"\"\n",
    "        This long function is unfortunately doing something very simple and is being very defensive:\n",
    "        We are separating out all parameters of the model into two buckets: those that will experience\n",
    "        weight decay for regularization and those that won't (biases, and layernorm/embedding weights).\n",
    "        We are then returning the PyTorch optimizer object.\n",
    "        \"\"\"\n",
    "\n",
    "        # separate out all parameters to those that will and won't experience regularizing weight decay\n",
    "        decay = set()\n",
    "        no_decay = set()\n",
    "        whitelist_weight_modules = (torch.nn.Linear, )\n",
    "        blacklist_weight_modules = (torch.nn.LayerNorm, torch.nn.Embedding)\n",
    "        for mn, m in self.named_modules():\n",
    "            for pn, p in m.named_parameters():\n",
    "                fpn = '%s.%s' % (mn, pn) if mn else pn # full param name\n",
    "                # random note: because named_modules and named_parameters are recursive\n",
    "                # we will see the same tensors p many many times. but doing it this way\n",
    "                # allows us to know which parent module any tensor p belongs to...\n",
    "                if pn.endswith('bias'):\n",
    "                    # all biases will not be decayed\n",
    "                    no_decay.add(fpn)\n",
    "                elif pn.endswith('weight') and isinstance(m, whitelist_weight_modules):\n",
    "                    # weights of whitelist modules will be weight decayed\n",
    "                    decay.add(fpn)\n",
    "                elif pn.endswith('weight') and isinstance(m, blacklist_weight_modules):\n",
    "                    # weights of blacklist modules will NOT be weight decayed\n",
    "                    no_decay.add(fpn)\n",
    "\n",
    "        # validate that we considered every parameter\n",
    "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
    "        inter_params = decay & no_decay\n",
    "        union_params = decay | no_decay\n",
    "        assert len(inter_params) == 0, \"parameters %s made it into both decay/no_decay sets!\" % (str(inter_params), )\n",
    "        assert len(param_dict.keys() - union_params) == 0, \"parameters %s were not separated into either decay/no_decay set!\" \\\n",
    "                                                    % (str(param_dict.keys() - union_params), )\n",
    "\n",
    "        # create the pytorch optimizer object\n",
    "        optim_groups = [\n",
    "            {\"params\": [param_dict[pn] for pn in sorted(list(decay))], \"weight_decay\": train_config.wd},\n",
    "            {\"params\": [param_dict[pn] for pn in sorted(list(no_decay))], \"weight_decay\": 0.0},\n",
    "        ]\n",
    "        optimizer = torch.optim.AdamW(optim_groups, lr=train_config.lr)\n",
    "        return optimizer\n",
    "    # -------------------------------------------\n",
    "\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        device = idx.device\n",
    "        b, t = idx.size()\n",
    "        assert (\n",
    "            t <= self.block_size\n",
    "        ), f\"Cannot forward sequence of length {t}, block size is only {self.block_size}\"\n",
    "        pos = torch.arange(0, t, dtype=torch.long, device=device).unsqueeze(\n",
    "            0\n",
    "        )  # shape (1, t)\n",
    "\n",
    "        # forward the GPT model itself\n",
    "        tok_emb = self.transformer.wte(idx)  # token embeddings of shape (b, t, n_embd)\n",
    "        pos_emb = self.transformer.wpe(pos)  # position embeddings of shape (1, t, n_embd)\n",
    "        x = tok_emb + pos_emb\n",
    "\n",
    "        for block in self.transformer.h:\n",
    "            if self.return_att:\n",
    "                x, attn_map = block(x)\n",
    "            else:\n",
    "                x = block(x)\n",
    "\n",
    "\n",
    "        x = self.transformer.ln_f(x)\n",
    "\n",
    "        # Track residual state before LM head for representation collapse\n",
    "        pre_lm_h = x.clone().detach()\n",
    "\n",
    "        # Final logits\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        # if we are given some desired targets also calculate the loss\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(\n",
    "                logits.reshape(-1, logits.size(-1)),\n",
    "                targets.reshape(-1),\n",
    "                ignore_index=-1,\n",
    "            )\n",
    "\n",
    "        if self.return_att:\n",
    "            return attn_map, pre_lm_h, logits, loss\n",
    "\n",
    "        return pre_lm_h, logits, loss\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(\n",
    "        self, idx, max_new_tokens, temperature=1.0, do_sample=False, top_k=None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Take a conditioning sequence of indices idx (LongTensor of shape (b,t)) and complete\n",
    "        the sequence max_new_tokens times, feeding the predictions back into the model each time.\n",
    "        Most likely you'll want to make sure to be in model.eval() mode of operation for this.\n",
    "        \"\"\"\n",
    "\n",
    "        for _ in range(max_new_tokens):\n",
    "            # if the sequence context is growing too long we must crop it at block_size\n",
    "            idx_cond = (\n",
    "                idx if idx.size(1) <= self.block_size else idx[:, -self.block_size :]\n",
    "            )\n",
    "            # forward the model to get the logits for the index in the sequence\n",
    "            _, _, logits, _ = self(idx_cond)\n",
    "            # pluck the logits at the final step and scale by desired temperature\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            # optionally crop the logits to only the top k options\n",
    "            if top_k is not None:\n",
    "                v, _ = torch.topk(logits, top_k)\n",
    "                logits[logits < v[:, [-1]]] = -float(\"Inf\")\n",
    "            # apply softmax to convert logits to (normalized) probabilities\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            # either sample from the distribution or take the most likely element\n",
    "            if do_sample:\n",
    "                idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            else:\n",
    "                _, idx_next = torch.topk(probs, k=1, dim=-1)\n",
    "            # append sampled index to the running sequence and continue\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "\n",
    "        return idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "Q4VrcEUrGndW"
   },
   "outputs": [],
   "source": [
    "# Data\n",
    "\n",
    "class MovingWindowSum:\n",
    "    def __init__(self, min_num=1, max_num=16, k=2, p=17, sep=17, device=\"cuda\"):\n",
    "        self.min_num = min_num\n",
    "        self.max_num = max_num\n",
    "        self.k = k\n",
    "        self.p = p\n",
    "        self.sep = sep\n",
    "        self.device = device\n",
    "        assert self.p > self.max_num\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def sample(\n",
    "        self,\n",
    "        num_samples,\n",
    "        num_tokens,\n",
    "    ):\n",
    "        random_ints = torch.randint(\n",
    "            low=self.min_num, high=self.max_num + 1, size=(num_samples, num_tokens)\n",
    "        ).to(self.device)\n",
    "\n",
    "        random_ints_np = random_ints.detach().cpu().numpy()\n",
    "        convolution = torch.stack(\n",
    "            [\n",
    "                torch.from_numpy(\n",
    "                    np.convolve(\n",
    "                        random_ints_np[i],\n",
    "                        np.ones(self.k),\n",
    "                        mode=\"valid\",\n",
    "                    )\n",
    "                )\n",
    "                for i in range(random_ints.shape[0])\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        moving_sum = random_ints.clone().detach()\n",
    "        moving_sum[:, self.k - 1 :] = convolution\n",
    "\n",
    "        # for i in range(num_samples):\n",
    "        #     for j in range(0, self.k - 1):\n",
    "        #         if moving_sum[i, j] != random_ints[i, j]:\n",
    "        #             print(f\"ERROR! {i} {j}\")\n",
    "        #     for j in range(self.k - 1, num_tokens):\n",
    "        #         if moving_sum[i, j] != torch.sum(random_ints[i, j-self.k+1:j+1]):\n",
    "        #             print(f\"ERROR! {i} {j}\")\n",
    "\n",
    "        # exit()\n",
    "        samples = (\n",
    "            torch.cat(\n",
    "                [\n",
    "                    random_ints,\n",
    "                    self.sep * torch.ones(size=(num_samples, 1)).to(self.device),\n",
    "                    torch.remainder(input=moving_sum, other=self.p),\n",
    "                ],\n",
    "                axis=-1,\n",
    "            )\n",
    "            .to(int)\n",
    "            .detach()\n",
    "        )\n",
    "\n",
    "        return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "GHF1-ylNGouE"
   },
   "outputs": [],
   "source": [
    "# Config\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "config = {\n",
    "'model':\n",
    "  {\n",
    "    'n_layer': 1,\n",
    "    'n_head': 1,\n",
    "    'n_embd': 256,\n",
    "    'linear': True,\n",
    "  },\n",
    "\n",
    "'data':\n",
    "  {\n",
    "    'name': 'window',\n",
    "    'min_num': 1,\n",
    "    'max_num': 16,\n",
    "    'k': 2,\n",
    "    'p': 17,\n",
    "    'sep': 17,\n",
    "    'cot': False,\n",
    "    'num_tokens': 16,\n",
    "    'n_train': 256,\n",
    "    'n_test': 64,\n",
    "    'fixed_len': True,\n",
    "  },\n",
    "\n",
    "'train':\n",
    "  {\n",
    "    'lr': 0.0001,\n",
    "    'grad_clip': -1,\n",
    "    'num_steps': 400,\n",
    "    'norm_type': \"none_rank\",\n",
    "    'wandb': True,\n",
    "    'save_ckpt': False,\n",
    "    'ckpt_freq': 20,\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "5HCPTLdZGqC2"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.22.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jyue/private/tf-loss-plateau/wandb/run-20251006_145338-cd58k54q</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jetyue04-uc-san-diego/tf-loss-plateau/runs/cd58k54q' target=\"_blank\">mws</a></strong> to <a href='https://wandb.ai/jetyue04-uc-san-diego/tf-loss-plateau' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jetyue04-uc-san-diego/tf-loss-plateau' target=\"_blank\">https://wandb.ai/jetyue04-uc-san-diego/tf-loss-plateau</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jetyue04-uc-san-diego/tf-loss-plateau/runs/cd58k54q' target=\"_blank\">https://wandb.ai/jetyue04-uc-san-diego/tf-loss-plateau/runs/cd58k54q</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0 -- Train loss: 2.886831521987915, Train Acc: 0.061279296875 Test Acc: 0.0595703125\n",
      "Step 1 -- Train loss: 2.8608810901641846, Train Acc: 0.05859375 Test Acc: 0.0693359375\n",
      "Step 2 -- Train loss: 2.8335483074188232, Train Acc: 0.0625 Test Acc: 0.0537109375\n",
      "Step 3 -- Train loss: 2.8156185150146484, Train Acc: 0.0625 Test Acc: 0.056640625\n",
      "Step 4 -- Train loss: 2.8014535903930664, Train Acc: 0.06298828125 Test Acc: 0.05859375\n",
      "Step 5 -- Train loss: 2.7881298065185547, Train Acc: 0.06591796875 Test Acc: 0.0732421875\n",
      "Step 6 -- Train loss: 2.771740436553955, Train Acc: 0.060546875 Test Acc: 0.0546875\n",
      "Step 7 -- Train loss: 2.765901803970337, Train Acc: 0.06494140625 Test Acc: 0.0556640625\n",
      "Step 8 -- Train loss: 2.753445863723755, Train Acc: 0.068603515625 Test Acc: 0.0517578125\n",
      "Step 9 -- Train loss: 2.7481772899627686, Train Acc: 0.066162109375 Test Acc: 0.0556640625\n",
      "Step 10 -- Train loss: 2.742149591445923, Train Acc: 0.06689453125 Test Acc: 0.0712890625\n",
      "Step 11 -- Train loss: 2.7395753860473633, Train Acc: 0.070556640625 Test Acc: 0.0703125\n",
      "Step 12 -- Train loss: 2.7365458011627197, Train Acc: 0.056640625 Test Acc: 0.0556640625\n",
      "Step 13 -- Train loss: 2.7350966930389404, Train Acc: 0.0556640625 Test Acc: 0.072265625\n",
      "Step 14 -- Train loss: 2.733011484146118, Train Acc: 0.060302734375 Test Acc: 0.0634765625\n",
      "Step 15 -- Train loss: 2.730802059173584, Train Acc: 0.074951171875 Test Acc: 0.0625\n",
      "Step 16 -- Train loss: 2.7293663024902344, Train Acc: 0.08203125 Test Acc: 0.0751953125\n",
      "Step 17 -- Train loss: 2.7287745475769043, Train Acc: 0.081298828125 Test Acc: 0.0751953125\n",
      "Step 18 -- Train loss: 2.7267253398895264, Train Acc: 0.083740234375 Test Acc: 0.0830078125\n",
      "Step 19 -- Train loss: 2.7243504524230957, Train Acc: 0.087646484375 Test Acc: 0.095703125\n",
      "Step 20 -- Train loss: 2.7230234146118164, Train Acc: 0.089111328125 Test Acc: 0.0810546875\n",
      "Step 21 -- Train loss: 2.722179889678955, Train Acc: 0.0859375 Test Acc: 0.0908203125\n",
      "Step 22 -- Train loss: 2.7212185859680176, Train Acc: 0.092529296875 Test Acc: 0.0810546875\n",
      "Step 23 -- Train loss: 2.717299222946167, Train Acc: 0.089111328125 Test Acc: 0.0771484375\n",
      "Step 24 -- Train loss: 2.7165329456329346, Train Acc: 0.09033203125 Test Acc: 0.091796875\n",
      "Step 25 -- Train loss: 2.71445369720459, Train Acc: 0.0966796875 Test Acc: 0.1015625\n",
      "Step 26 -- Train loss: 2.7118265628814697, Train Acc: 0.103759765625 Test Acc: 0.103515625\n",
      "Step 27 -- Train loss: 2.707918167114258, Train Acc: 0.112060546875 Test Acc: 0.0966796875\n",
      "Step 28 -- Train loss: 2.7107293605804443, Train Acc: 0.111572265625 Test Acc: 0.1240234375\n",
      "Step 29 -- Train loss: 2.706596612930298, Train Acc: 0.11572265625 Test Acc: 0.115234375\n",
      "Step 30 -- Train loss: 2.701869249343872, Train Acc: 0.10595703125 Test Acc: 0.1064453125\n",
      "Step 31 -- Train loss: 2.6978023052215576, Train Acc: 0.1123046875 Test Acc: 0.11328125\n",
      "Step 32 -- Train loss: 2.695035696029663, Train Acc: 0.1142578125 Test Acc: 0.1083984375\n",
      "Step 33 -- Train loss: 2.6913697719573975, Train Acc: 0.116455078125 Test Acc: 0.1044921875\n",
      "Step 34 -- Train loss: 2.6890103816986084, Train Acc: 0.1201171875 Test Acc: 0.123046875\n",
      "Step 35 -- Train loss: 2.68520188331604, Train Acc: 0.119140625 Test Acc: 0.1328125\n",
      "Step 36 -- Train loss: 2.6802597045898438, Train Acc: 0.1142578125 Test Acc: 0.1171875\n",
      "Step 37 -- Train loss: 2.6773948669433594, Train Acc: 0.120849609375 Test Acc: 0.1201171875\n",
      "Step 38 -- Train loss: 2.672149658203125, Train Acc: 0.1201171875 Test Acc: 0.115234375\n",
      "Step 39 -- Train loss: 2.672729969024658, Train Acc: 0.123291015625 Test Acc: 0.126953125\n",
      "Step 40 -- Train loss: 2.668592929840088, Train Acc: 0.125244140625 Test Acc: 0.125\n",
      "Step 41 -- Train loss: 2.663724660873413, Train Acc: 0.12353515625 Test Acc: 0.123046875\n",
      "Step 42 -- Train loss: 2.6652541160583496, Train Acc: 0.125732421875 Test Acc: 0.1181640625\n",
      "Step 43 -- Train loss: 2.6598246097564697, Train Acc: 0.11767578125 Test Acc: 0.1103515625\n",
      "Step 44 -- Train loss: 2.6575498580932617, Train Acc: 0.1240234375 Test Acc: 0.125\n",
      "Step 45 -- Train loss: 2.6533010005950928, Train Acc: 0.125732421875 Test Acc: 0.1357421875\n",
      "Step 46 -- Train loss: 2.654813766479492, Train Acc: 0.124267578125 Test Acc: 0.1240234375\n",
      "Step 47 -- Train loss: 2.65114688873291, Train Acc: 0.11865234375 Test Acc: 0.1328125\n",
      "Step 48 -- Train loss: 2.649439811706543, Train Acc: 0.131103515625 Test Acc: 0.119140625\n",
      "Step 49 -- Train loss: 2.651108741760254, Train Acc: 0.121826171875 Test Acc: 0.123046875\n",
      "Step 50 -- Train loss: 2.647040605545044, Train Acc: 0.121337890625 Test Acc: 0.1259765625\n",
      "Step 51 -- Train loss: 2.6493256092071533, Train Acc: 0.121826171875 Test Acc: 0.123046875\n",
      "Step 52 -- Train loss: 2.646664619445801, Train Acc: 0.12548828125 Test Acc: 0.1279296875\n",
      "Step 53 -- Train loss: 2.6463847160339355, Train Acc: 0.12353515625 Test Acc: 0.125\n",
      "Step 54 -- Train loss: 2.644078254699707, Train Acc: 0.13232421875 Test Acc: 0.1298828125\n",
      "Step 55 -- Train loss: 2.64176869392395, Train Acc: 0.1240234375 Test Acc: 0.1181640625\n",
      "Step 56 -- Train loss: 2.6404871940612793, Train Acc: 0.130126953125 Test Acc: 0.1220703125\n",
      "Step 57 -- Train loss: 2.641512393951416, Train Acc: 0.1162109375 Test Acc: 0.130859375\n",
      "Step 58 -- Train loss: 2.6417157649993896, Train Acc: 0.1201171875 Test Acc: 0.109375\n",
      "Step 59 -- Train loss: 2.641580820083618, Train Acc: 0.123779296875 Test Acc: 0.12109375\n",
      "Step 60 -- Train loss: 2.6386923789978027, Train Acc: 0.11962890625 Test Acc: 0.12109375\n",
      "Step 61 -- Train loss: 2.635284185409546, Train Acc: 0.129150390625 Test Acc: 0.134765625\n",
      "Step 62 -- Train loss: 2.6355292797088623, Train Acc: 0.1318359375 Test Acc: 0.1298828125\n",
      "Step 63 -- Train loss: 2.6398332118988037, Train Acc: 0.119140625 Test Acc: 0.134765625\n",
      "Step 64 -- Train loss: 2.6382291316986084, Train Acc: 0.124755859375 Test Acc: 0.1171875\n",
      "Step 65 -- Train loss: 2.6395435333251953, Train Acc: 0.125 Test Acc: 0.1259765625\n",
      "Step 66 -- Train loss: 2.6321170330047607, Train Acc: 0.124267578125 Test Acc: 0.130859375\n",
      "Step 67 -- Train loss: 2.634990930557251, Train Acc: 0.126220703125 Test Acc: 0.109375\n",
      "Step 68 -- Train loss: 2.6329574584960938, Train Acc: 0.126708984375 Test Acc: 0.1171875\n",
      "Step 69 -- Train loss: 2.6358420848846436, Train Acc: 0.122314453125 Test Acc: 0.1279296875\n",
      "Step 70 -- Train loss: 2.6337244510650635, Train Acc: 0.12158203125 Test Acc: 0.123046875\n",
      "Step 71 -- Train loss: 2.6314005851745605, Train Acc: 0.12939453125 Test Acc: 0.1259765625\n",
      "Step 72 -- Train loss: 2.633875846862793, Train Acc: 0.129150390625 Test Acc: 0.1220703125\n",
      "Step 73 -- Train loss: 2.6342859268188477, Train Acc: 0.128173828125 Test Acc: 0.10546875\n",
      "Step 74 -- Train loss: 2.6324872970581055, Train Acc: 0.125732421875 Test Acc: 0.119140625\n",
      "Step 75 -- Train loss: 2.6326658725738525, Train Acc: 0.125244140625 Test Acc: 0.130859375\n",
      "Step 76 -- Train loss: 2.6315882205963135, Train Acc: 0.12890625 Test Acc: 0.123046875\n",
      "Step 77 -- Train loss: 2.6314303874969482, Train Acc: 0.1279296875 Test Acc: 0.125\n",
      "Step 78 -- Train loss: 2.6322226524353027, Train Acc: 0.12060546875 Test Acc: 0.1103515625\n",
      "Step 79 -- Train loss: 2.6318795680999756, Train Acc: 0.12255859375 Test Acc: 0.12109375\n",
      "Step 80 -- Train loss: 2.6320712566375732, Train Acc: 0.122802734375 Test Acc: 0.1279296875\n",
      "Step 81 -- Train loss: 2.6313207149505615, Train Acc: 0.127197265625 Test Acc: 0.119140625\n",
      "Step 82 -- Train loss: 2.630122184753418, Train Acc: 0.126953125 Test Acc: 0.125\n",
      "Step 83 -- Train loss: 2.6319994926452637, Train Acc: 0.125732421875 Test Acc: 0.126953125\n",
      "Step 84 -- Train loss: 2.631510019302368, Train Acc: 0.12158203125 Test Acc: 0.1240234375\n",
      "Step 85 -- Train loss: 2.630556583404541, Train Acc: 0.1279296875 Test Acc: 0.123046875\n",
      "Step 86 -- Train loss: 2.6301817893981934, Train Acc: 0.123291015625 Test Acc: 0.130859375\n",
      "Step 87 -- Train loss: 2.627488136291504, Train Acc: 0.1220703125 Test Acc: 0.11328125\n",
      "Step 88 -- Train loss: 2.6304256916046143, Train Acc: 0.126220703125 Test Acc: 0.126953125\n",
      "Step 89 -- Train loss: 2.62955641746521, Train Acc: 0.125 Test Acc: 0.1376953125\n",
      "Step 90 -- Train loss: 2.6273343563079834, Train Acc: 0.11767578125 Test Acc: 0.1259765625\n",
      "Step 91 -- Train loss: 2.6292998790740967, Train Acc: 0.1220703125 Test Acc: 0.119140625\n",
      "Step 92 -- Train loss: 2.6290276050567627, Train Acc: 0.12451171875 Test Acc: 0.1142578125\n",
      "Step 93 -- Train loss: 2.6288106441497803, Train Acc: 0.125 Test Acc: 0.123046875\n",
      "Step 94 -- Train loss: 2.62583065032959, Train Acc: 0.130859375 Test Acc: 0.123046875\n",
      "Step 95 -- Train loss: 2.6269261837005615, Train Acc: 0.123046875 Test Acc: 0.1298828125\n",
      "Step 96 -- Train loss: 2.625401735305786, Train Acc: 0.125244140625 Test Acc: 0.1337890625\n",
      "Step 97 -- Train loss: 2.6257195472717285, Train Acc: 0.12353515625 Test Acc: 0.12890625\n",
      "Step 98 -- Train loss: 2.6248087882995605, Train Acc: 0.127685546875 Test Acc: 0.13671875\n",
      "Step 99 -- Train loss: 2.626131772994995, Train Acc: 0.1259765625 Test Acc: 0.1318359375\n",
      "Step 100 -- Train loss: 2.626500368118286, Train Acc: 0.126708984375 Test Acc: 0.1064453125\n",
      "Step 101 -- Train loss: 2.6225504875183105, Train Acc: 0.119140625 Test Acc: 0.13671875\n",
      "Step 102 -- Train loss: 2.624220132827759, Train Acc: 0.127685546875 Test Acc: 0.111328125\n",
      "Step 103 -- Train loss: 2.624974012374878, Train Acc: 0.119140625 Test Acc: 0.1181640625\n",
      "Step 104 -- Train loss: 2.6221604347229004, Train Acc: 0.12890625 Test Acc: 0.126953125\n",
      "Step 105 -- Train loss: 2.6248693466186523, Train Acc: 0.125244140625 Test Acc: 0.119140625\n",
      "Step 106 -- Train loss: 2.6204872131347656, Train Acc: 0.1298828125 Test Acc: 0.119140625\n",
      "Step 107 -- Train loss: 2.6202921867370605, Train Acc: 0.128662109375 Test Acc: 0.1142578125\n",
      "Step 108 -- Train loss: 2.618544816970825, Train Acc: 0.128173828125 Test Acc: 0.12109375\n",
      "Step 109 -- Train loss: 2.6188101768493652, Train Acc: 0.12548828125 Test Acc: 0.123046875\n",
      "Step 110 -- Train loss: 2.6170034408569336, Train Acc: 0.1318359375 Test Acc: 0.1220703125\n",
      "Step 111 -- Train loss: 2.619828224182129, Train Acc: 0.1240234375 Test Acc: 0.134765625\n",
      "Step 112 -- Train loss: 2.620316505432129, Train Acc: 0.116943359375 Test Acc: 0.134765625\n",
      "Step 113 -- Train loss: 2.6175789833068848, Train Acc: 0.126220703125 Test Acc: 0.125\n",
      "Step 114 -- Train loss: 2.613114595413208, Train Acc: 0.1337890625 Test Acc: 0.12890625\n",
      "Step 115 -- Train loss: 2.6148478984832764, Train Acc: 0.127685546875 Test Acc: 0.1357421875\n",
      "Step 116 -- Train loss: 2.6161158084869385, Train Acc: 0.12939453125 Test Acc: 0.1376953125\n",
      "Step 117 -- Train loss: 2.6154894828796387, Train Acc: 0.125244140625 Test Acc: 0.1201171875\n",
      "Step 118 -- Train loss: 2.6184098720550537, Train Acc: 0.12451171875 Test Acc: 0.1328125\n",
      "Step 119 -- Train loss: 2.6109261512756348, Train Acc: 0.135986328125 Test Acc: 0.1318359375\n",
      "Step 120 -- Train loss: 2.6122379302978516, Train Acc: 0.133056640625 Test Acc: 0.134765625\n",
      "Step 121 -- Train loss: 2.6073951721191406, Train Acc: 0.134521484375 Test Acc: 0.1416015625\n",
      "Step 122 -- Train loss: 2.610257863998413, Train Acc: 0.1357421875 Test Acc: 0.1484375\n",
      "Step 123 -- Train loss: 2.6053519248962402, Train Acc: 0.14208984375 Test Acc: 0.1416015625\n",
      "Step 124 -- Train loss: 2.6093223094940186, Train Acc: 0.139892578125 Test Acc: 0.1376953125\n",
      "Step 125 -- Train loss: 2.608060359954834, Train Acc: 0.135498046875 Test Acc: 0.1396484375\n",
      "Step 126 -- Train loss: 2.6046206951141357, Train Acc: 0.14013671875 Test Acc: 0.138671875\n",
      "Step 127 -- Train loss: 2.601851224899292, Train Acc: 0.1416015625 Test Acc: 0.123046875\n",
      "Step 128 -- Train loss: 2.6029458045959473, Train Acc: 0.13818359375 Test Acc: 0.1455078125\n",
      "Step 129 -- Train loss: 2.5967822074890137, Train Acc: 0.147705078125 Test Acc: 0.1357421875\n",
      "Step 130 -- Train loss: 2.5958166122436523, Train Acc: 0.1455078125 Test Acc: 0.13671875\n",
      "Step 131 -- Train loss: 2.594107151031494, Train Acc: 0.149169921875 Test Acc: 0.140625\n",
      "Step 132 -- Train loss: 2.5885910987854004, Train Acc: 0.153564453125 Test Acc: 0.1650390625\n",
      "Step 133 -- Train loss: 2.5874459743499756, Train Acc: 0.154052734375 Test Acc: 0.140625\n",
      "Step 134 -- Train loss: 2.580948829650879, Train Acc: 0.163818359375 Test Acc: 0.1513671875\n",
      "Step 135 -- Train loss: 2.584534168243408, Train Acc: 0.153564453125 Test Acc: 0.1533203125\n",
      "Step 136 -- Train loss: 2.572025775909424, Train Acc: 0.1708984375 Test Acc: 0.1669921875\n",
      "Step 137 -- Train loss: 2.5732133388519287, Train Acc: 0.177490234375 Test Acc: 0.177734375\n",
      "Step 138 -- Train loss: 2.5658457279205322, Train Acc: 0.1787109375 Test Acc: 0.19140625\n",
      "Step 139 -- Train loss: 2.557844400405884, Train Acc: 0.18310546875 Test Acc: 0.1953125\n",
      "Step 140 -- Train loss: 2.5517125129699707, Train Acc: 0.197998046875 Test Acc: 0.201171875\n",
      "Step 141 -- Train loss: 2.5420334339141846, Train Acc: 0.21923828125 Test Acc: 0.20703125\n",
      "Step 142 -- Train loss: 2.532439708709717, Train Acc: 0.23046875 Test Acc: 0.228515625\n",
      "Step 143 -- Train loss: 2.5173842906951904, Train Acc: 0.2568359375 Test Acc: 0.259765625\n",
      "Step 144 -- Train loss: 2.5069799423217773, Train Acc: 0.275390625 Test Acc: 0.2705078125\n",
      "Step 145 -- Train loss: 2.492335319519043, Train Acc: 0.310546875 Test Acc: 0.2939453125\n",
      "Step 146 -- Train loss: 2.4775257110595703, Train Acc: 0.33642578125 Test Acc: 0.322265625\n",
      "Step 147 -- Train loss: 2.4608869552612305, Train Acc: 0.375732421875 Test Acc: 0.369140625\n",
      "Step 148 -- Train loss: 2.4432263374328613, Train Acc: 0.42822265625 Test Acc: 0.396484375\n",
      "Step 149 -- Train loss: 2.424220323562622, Train Acc: 0.468505859375 Test Acc: 0.4853515625\n",
      "Step 150 -- Train loss: 2.4008450508117676, Train Acc: 0.5263671875 Test Acc: 0.490234375\n",
      "Step 151 -- Train loss: 2.3741025924682617, Train Acc: 0.565185546875 Test Acc: 0.5537109375\n",
      "Step 152 -- Train loss: 2.339230537414551, Train Acc: 0.6298828125 Test Acc: 0.6025390625\n",
      "Step 153 -- Train loss: 2.314342498779297, Train Acc: 0.692626953125 Test Acc: 0.6630859375\n",
      "Step 154 -- Train loss: 2.2719757556915283, Train Acc: 0.75146484375 Test Acc: 0.7490234375\n",
      "Step 155 -- Train loss: 2.2342865467071533, Train Acc: 0.792724609375 Test Acc: 0.783203125\n",
      "Step 156 -- Train loss: 2.196352481842041, Train Acc: 0.808349609375 Test Acc: 0.8193359375\n",
      "Step 157 -- Train loss: 2.13956618309021, Train Acc: 0.848876953125 Test Acc: 0.8232421875\n",
      "Step 158 -- Train loss: 2.095747947692871, Train Acc: 0.86865234375 Test Acc: 0.875\n",
      "Step 159 -- Train loss: 2.039093255996704, Train Acc: 0.89111328125 Test Acc: 0.8779296875\n",
      "Step 160 -- Train loss: 1.9862576723098755, Train Acc: 0.914306640625 Test Acc: 0.9228515625\n",
      "Step 161 -- Train loss: 1.9353059530258179, Train Acc: 0.935791015625 Test Acc: 0.9453125\n",
      "Step 162 -- Train loss: 1.8766872882843018, Train Acc: 0.94921875 Test Acc: 0.947265625\n",
      "Step 163 -- Train loss: 1.830573558807373, Train Acc: 0.9609375 Test Acc: 0.9609375\n",
      "Step 164 -- Train loss: 1.786850929260254, Train Acc: 0.967041015625 Test Acc: 0.9658203125\n",
      "Step 165 -- Train loss: 1.7483601570129395, Train Acc: 0.974853515625 Test Acc: 0.9677734375\n",
      "Step 166 -- Train loss: 1.7068222761154175, Train Acc: 0.978759765625 Test Acc: 0.9814453125\n",
      "Step 167 -- Train loss: 1.6693096160888672, Train Acc: 0.989990234375 Test Acc: 0.9892578125\n",
      "Step 168 -- Train loss: 1.637290358543396, Train Acc: 0.99267578125 Test Acc: 0.994140625\n",
      "Step 169 -- Train loss: 1.6065552234649658, Train Acc: 0.99755859375 Test Acc: 0.99609375\n",
      "Step 170 -- Train loss: 1.5797184705734253, Train Acc: 0.998291015625 Test Acc: 0.9990234375\n",
      "Step 171 -- Train loss: 1.5550692081451416, Train Acc: 0.99853515625 Test Acc: 0.9990234375\n",
      "Step 172 -- Train loss: 1.5346980094909668, Train Acc: 0.99951171875 Test Acc: 0.9990234375\n",
      "Step 173 -- Train loss: 1.5109076499938965, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 174 -- Train loss: 1.4925682544708252, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 175 -- Train loss: 1.4751518964767456, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 176 -- Train loss: 1.4636002779006958, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 177 -- Train loss: 1.4535613059997559, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 178 -- Train loss: 1.4429851770401, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 179 -- Train loss: 1.4295679330825806, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 180 -- Train loss: 1.4192578792572021, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 181 -- Train loss: 1.4167968034744263, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 182 -- Train loss: 1.4102578163146973, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 183 -- Train loss: 1.4002556800842285, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 184 -- Train loss: 1.3919646739959717, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 185 -- Train loss: 1.3888276815414429, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 186 -- Train loss: 1.3832144737243652, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 187 -- Train loss: 1.3817135095596313, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 188 -- Train loss: 1.3729629516601562, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 189 -- Train loss: 1.3725320100784302, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 190 -- Train loss: 1.3680686950683594, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 191 -- Train loss: 1.3654170036315918, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 192 -- Train loss: 1.3612890243530273, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 193 -- Train loss: 1.3599522113800049, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 194 -- Train loss: 1.3562167882919312, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 195 -- Train loss: 1.353096842765808, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 196 -- Train loss: 1.3531973361968994, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 197 -- Train loss: 1.3499181270599365, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 198 -- Train loss: 1.3479784727096558, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 199 -- Train loss: 1.346555233001709, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 200 -- Train loss: 1.345602035522461, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 201 -- Train loss: 1.3437068462371826, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 202 -- Train loss: 1.341501235961914, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 203 -- Train loss: 1.3399685621261597, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 204 -- Train loss: 1.3402961492538452, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 205 -- Train loss: 1.338316559791565, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 206 -- Train loss: 1.3382340669631958, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 207 -- Train loss: 1.3360103368759155, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 208 -- Train loss: 1.3361953496932983, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 209 -- Train loss: 1.334729790687561, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 210 -- Train loss: 1.3346970081329346, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 211 -- Train loss: 1.3333138227462769, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 212 -- Train loss: 1.3325450420379639, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 213 -- Train loss: 1.3321374654769897, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 214 -- Train loss: 1.3307414054870605, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 215 -- Train loss: 1.3284337520599365, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 216 -- Train loss: 1.3294545412063599, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 217 -- Train loss: 1.3305914402008057, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 218 -- Train loss: 1.329331398010254, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 219 -- Train loss: 1.328343391418457, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 220 -- Train loss: 1.328583002090454, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 221 -- Train loss: 1.3276304006576538, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 222 -- Train loss: 1.3260982036590576, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 223 -- Train loss: 1.3263745307922363, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 224 -- Train loss: 1.326476812362671, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 225 -- Train loss: 1.3250062465667725, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 226 -- Train loss: 1.3249191045761108, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 227 -- Train loss: 1.325339913368225, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 228 -- Train loss: 1.324876070022583, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 229 -- Train loss: 1.3243885040283203, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 230 -- Train loss: 1.3240875005722046, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 231 -- Train loss: 1.3218539953231812, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 232 -- Train loss: 1.3233476877212524, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 233 -- Train loss: 1.3224995136260986, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 234 -- Train loss: 1.323628544807434, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 235 -- Train loss: 1.3216032981872559, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 236 -- Train loss: 1.3231338262557983, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 237 -- Train loss: 1.3220611810684204, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 238 -- Train loss: 1.3207941055297852, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 239 -- Train loss: 1.3205478191375732, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 240 -- Train loss: 1.3214386701583862, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 241 -- Train loss: 1.3213911056518555, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 242 -- Train loss: 1.3200632333755493, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 243 -- Train loss: 1.3188323974609375, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 244 -- Train loss: 1.3187706470489502, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 245 -- Train loss: 1.319809913635254, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 246 -- Train loss: 1.3196498155593872, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 247 -- Train loss: 1.3205406665802002, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 248 -- Train loss: 1.3197001218795776, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 249 -- Train loss: 1.3191003799438477, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 250 -- Train loss: 1.31821608543396, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 251 -- Train loss: 1.317763090133667, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 252 -- Train loss: 1.318283200263977, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 253 -- Train loss: 1.3181284666061401, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 254 -- Train loss: 1.3174233436584473, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 255 -- Train loss: 1.31873619556427, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 256 -- Train loss: 1.3175928592681885, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 257 -- Train loss: 1.3174924850463867, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 258 -- Train loss: 1.3168885707855225, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 259 -- Train loss: 1.317080020904541, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 260 -- Train loss: 1.3165868520736694, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 261 -- Train loss: 1.3155517578125, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 262 -- Train loss: 1.3178846836090088, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 263 -- Train loss: 1.3171887397766113, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 264 -- Train loss: 1.3172558546066284, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 265 -- Train loss: 1.3155657052993774, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 266 -- Train loss: 1.3163068294525146, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 267 -- Train loss: 1.3156698942184448, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 268 -- Train loss: 1.3158613443374634, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 269 -- Train loss: 1.3152563571929932, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 270 -- Train loss: 1.3164812326431274, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 271 -- Train loss: 1.3159241676330566, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 272 -- Train loss: 1.3149255514144897, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 273 -- Train loss: 1.3151428699493408, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 274 -- Train loss: 1.315506935119629, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 275 -- Train loss: 1.3142679929733276, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 276 -- Train loss: 1.314277172088623, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 277 -- Train loss: 1.3136072158813477, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 278 -- Train loss: 1.3132407665252686, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 279 -- Train loss: 1.3149268627166748, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 280 -- Train loss: 1.3144464492797852, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 281 -- Train loss: 1.3139880895614624, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 282 -- Train loss: 1.3136919736862183, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 283 -- Train loss: 1.313334345817566, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 284 -- Train loss: 1.3143576383590698, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 285 -- Train loss: 1.3128130435943604, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 286 -- Train loss: 1.3139845132827759, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 287 -- Train loss: 1.3134602308273315, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 288 -- Train loss: 1.3135814666748047, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 289 -- Train loss: 1.3137749433517456, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 290 -- Train loss: 1.313240647315979, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 291 -- Train loss: 1.3122047185897827, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 292 -- Train loss: 1.313470721244812, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 293 -- Train loss: 1.3127853870391846, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 294 -- Train loss: 1.3127473592758179, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 295 -- Train loss: 1.3134074211120605, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 296 -- Train loss: 1.313590168952942, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 297 -- Train loss: 1.311460018157959, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 298 -- Train loss: 1.3133524656295776, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 299 -- Train loss: 1.3132482767105103, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 300 -- Train loss: 1.311159372329712, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 301 -- Train loss: 1.3119157552719116, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 302 -- Train loss: 1.3118630647659302, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 303 -- Train loss: 1.3122658729553223, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 304 -- Train loss: 1.3112378120422363, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 305 -- Train loss: 1.3121174573898315, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 306 -- Train loss: 1.312401533126831, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 307 -- Train loss: 1.311220645904541, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 308 -- Train loss: 1.3119072914123535, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 309 -- Train loss: 1.3116368055343628, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 310 -- Train loss: 1.312117576599121, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 311 -- Train loss: 1.3111340999603271, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 312 -- Train loss: 1.3120752573013306, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 313 -- Train loss: 1.3112990856170654, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 314 -- Train loss: 1.3113267421722412, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 315 -- Train loss: 1.310645580291748, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 316 -- Train loss: 1.3116267919540405, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 317 -- Train loss: 1.3109793663024902, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 318 -- Train loss: 1.3109735250473022, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 319 -- Train loss: 1.3106021881103516, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 320 -- Train loss: 1.3114999532699585, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 321 -- Train loss: 1.3103350400924683, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 322 -- Train loss: 1.3105882406234741, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 323 -- Train loss: 1.3106622695922852, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 324 -- Train loss: 1.3098174333572388, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 325 -- Train loss: 1.310541033744812, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 326 -- Train loss: 1.3099451065063477, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 327 -- Train loss: 1.311947226524353, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 328 -- Train loss: 1.3108172416687012, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 329 -- Train loss: 1.3106333017349243, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 330 -- Train loss: 1.309605360031128, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 331 -- Train loss: 1.3104926347732544, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 332 -- Train loss: 1.3097206354141235, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 333 -- Train loss: 1.309665322303772, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 334 -- Train loss: 1.3098845481872559, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 335 -- Train loss: 1.3089596033096313, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 336 -- Train loss: 1.3102970123291016, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 337 -- Train loss: 1.3095901012420654, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 338 -- Train loss: 1.3097554445266724, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 339 -- Train loss: 1.3095325231552124, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 340 -- Train loss: 1.3090869188308716, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 341 -- Train loss: 1.3095402717590332, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 342 -- Train loss: 1.3105758428573608, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 343 -- Train loss: 1.3088345527648926, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 344 -- Train loss: 1.3089839220046997, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 345 -- Train loss: 1.3089393377304077, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 346 -- Train loss: 1.3098498582839966, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 347 -- Train loss: 1.3089948892593384, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 348 -- Train loss: 1.3091672658920288, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 349 -- Train loss: 1.309707522392273, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 350 -- Train loss: 1.3088102340698242, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 351 -- Train loss: 1.3090592622756958, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 352 -- Train loss: 1.3090171813964844, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 353 -- Train loss: 1.3086155652999878, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 354 -- Train loss: 1.3086949586868286, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 355 -- Train loss: 1.308255910873413, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 356 -- Train loss: 1.3083457946777344, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 357 -- Train loss: 1.3088641166687012, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 358 -- Train loss: 1.3086119890213013, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 359 -- Train loss: 1.3094416856765747, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 360 -- Train loss: 1.3081459999084473, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 361 -- Train loss: 1.3078547716140747, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 362 -- Train loss: 1.3081071376800537, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 363 -- Train loss: 1.3074668645858765, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 364 -- Train loss: 1.3084564208984375, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 365 -- Train loss: 1.3082242012023926, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 366 -- Train loss: 1.3081482648849487, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 367 -- Train loss: 1.3075319528579712, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 368 -- Train loss: 1.3084776401519775, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 369 -- Train loss: 1.3072474002838135, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 370 -- Train loss: 1.3076690435409546, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 371 -- Train loss: 1.3082958459854126, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 372 -- Train loss: 1.3080956935882568, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 373 -- Train loss: 1.3077211380004883, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 374 -- Train loss: 1.3083947896957397, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 375 -- Train loss: 1.308825135231018, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 376 -- Train loss: 1.3074764013290405, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 377 -- Train loss: 1.307577133178711, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 378 -- Train loss: 1.3080545663833618, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 379 -- Train loss: 1.3075944185256958, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 380 -- Train loss: 1.3082873821258545, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 381 -- Train loss: 1.308642864227295, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 382 -- Train loss: 1.3076767921447754, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 383 -- Train loss: 1.3076450824737549, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 384 -- Train loss: 1.3074270486831665, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 385 -- Train loss: 1.3081021308898926, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 386 -- Train loss: 1.3080551624298096, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 387 -- Train loss: 1.3075493574142456, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 388 -- Train loss: 1.307213544845581, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 389 -- Train loss: 1.3080257177352905, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 390 -- Train loss: 1.308016061782837, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 391 -- Train loss: 1.3077083826065063, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 392 -- Train loss: 1.3071986436843872, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 393 -- Train loss: 1.3076908588409424, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 394 -- Train loss: 1.3074021339416504, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 395 -- Train loss: 1.3073172569274902, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 396 -- Train loss: 1.3068938255310059, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 397 -- Train loss: 1.306948184967041, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 398 -- Train loss: 1.3077043294906616, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 399 -- Train loss: 1.3075753450393677, Train Acc: 1.0 Test Acc: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>att_prog_measure</td><td></td></tr><tr><td>data_repeat_frac</td><td></td></tr><tr><td>idx0_check</td><td></td></tr><tr><td>idx10_check</td><td></td></tr><tr><td>idx11_check</td><td></td></tr><tr><td>idx12_check</td><td></td></tr><tr><td>idx13_check</td><td></td></tr><tr><td>idx14_check</td><td></td></tr><tr><td>idx15_check</td><td></td></tr><tr><td>idx1_check</td><td></td></tr><tr><td>idx2_check</td><td></td></tr><tr><td>idx3_check</td><td></td></tr><tr><td>idx4_check</td><td></td></tr><tr><td>idx5_check</td><td></td></tr><tr><td>idx6_check</td><td></td></tr><tr><td>idx7_check</td><td></td></tr><tr><td>idx8_check</td><td></td></tr><tr><td>idx9_check</td><td></td></tr><tr><td>mean_cosine_sim</td><td></td></tr><tr><td>mean_cosine_sim_0</td><td></td></tr><tr><td>mean_cosine_sim_1</td><td></td></tr><tr><td>mean_cosine_sim_10</td><td></td></tr><tr><td>mean_cosine_sim_11</td><td></td></tr><tr><td>mean_cosine_sim_12</td><td></td></tr><tr><td>mean_cosine_sim_13</td><td></td></tr><tr><td>mean_cosine_sim_14</td><td></td></tr><tr><td>mean_cosine_sim_2</td><td></td></tr><tr><td>mean_cosine_sim_3</td><td></td></tr><tr><td>mean_cosine_sim_4</td><td></td></tr><tr><td>mean_cosine_sim_5</td><td></td></tr><tr><td>mean_cosine_sim_6</td><td></td></tr><tr><td>mean_cosine_sim_7</td><td></td></tr><tr><td>mean_cosine_sim_8</td><td></td></tr><tr><td>mean_cosine_sim_9</td><td></td></tr><tr><td>model_repeat_frac</td><td></td></tr><tr><td>test_acc</td><td></td></tr><tr><td>train_acc</td><td></td></tr><tr><td>train_loss</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>att_prog_measure</td><td>0.79276</td></tr><tr><td>data_repeat_frac</td><td>0.06458</td></tr><tr><td>idx0_check</td><td>1.0</td></tr><tr><td>idx10_check</td><td>1.0</td></tr><tr><td>idx11_check</td><td>1.0</td></tr><tr><td>idx12_check</td><td>1.0</td></tr><tr><td>idx13_check</td><td>1.0</td></tr><tr><td>idx14_check</td><td>1.0</td></tr><tr><td>idx15_check</td><td>1.0</td></tr><tr><td>idx1_check</td><td>1.0</td></tr><tr><td>idx2_check</td><td>1.0</td></tr><tr><td>idx3_check</td><td>1.0</td></tr><tr><td>idx4_check</td><td>1.0</td></tr><tr><td>idx5_check</td><td>1.0</td></tr><tr><td>idx6_check</td><td>1.0</td></tr><tr><td>idx7_check</td><td>1.0</td></tr><tr><td>idx8_check</td><td>1.0</td></tr><tr><td>idx9_check</td><td>1.0</td></tr><tr><td>mean_cosine_sim</td><td>0.00324</td></tr><tr><td>mean_cosine_sim_0</td><td>0.00114</td></tr><tr><td>mean_cosine_sim_1</td><td>0.00367</td></tr><tr><td>mean_cosine_sim_10</td><td>0.01559</td></tr><tr><td>mean_cosine_sim_11</td><td>0.00929</td></tr><tr><td>mean_cosine_sim_12</td><td>0.00556</td></tr><tr><td>mean_cosine_sim_13</td><td>0.00217</td></tr><tr><td>mean_cosine_sim_14</td><td>-0.00983</td></tr><tr><td>mean_cosine_sim_2</td><td>0.00342</td></tr><tr><td>mean_cosine_sim_3</td><td>0.00686</td></tr><tr><td>mean_cosine_sim_4</td><td>-0.00262</td></tr><tr><td>mean_cosine_sim_5</td><td>0.00648</td></tr><tr><td>mean_cosine_sim_6</td><td>0.00598</td></tr><tr><td>mean_cosine_sim_7</td><td>0.00148</td></tr><tr><td>mean_cosine_sim_8</td><td>-0.0085</td></tr><tr><td>mean_cosine_sim_9</td><td>-0.00109</td></tr><tr><td>model_repeat_frac</td><td>0.06458</td></tr><tr><td>test_acc</td><td>1.0</td></tr><tr><td>train_acc</td><td>1.0</td></tr><tr><td>train_loss</td><td>1.30758</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">mws</strong> at: <a href='https://wandb.ai/jetyue04-uc-san-diego/tf-loss-plateau/runs/cd58k54q' target=\"_blank\">https://wandb.ai/jetyue04-uc-san-diego/tf-loss-plateau/runs/cd58k54q</a><br/>Synced 6 W&B file(s), 400 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251006_145338-cd58k54q/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train\n",
    "\n",
    "def train_step(\n",
    "    model,\n",
    "    optim,\n",
    "    data_sampler,\n",
    "    step,\n",
    "    config,\n",
    "):\n",
    "    n_train, n_test, num_tokens = (\n",
    "        config.data.n_train,\n",
    "        config.data.n_test,\n",
    "        config.data.num_tokens,\n",
    "    )\n",
    "\n",
    "    data = data_sampler.sample(\n",
    "        num_samples=n_train + n_test,\n",
    "        num_tokens=num_tokens,\n",
    "    )\n",
    "\n",
    "    train_data = data[:n_train, :]\n",
    "    test_data = data[n_train:, :]\n",
    "\n",
    "    prompt_len = num_tokens + 1\n",
    "    gen_len = num_tokens\n",
    "    acc_start = num_tokens + 1\n",
    "\n",
    "    model.train()\n",
    "    optim.zero_grad(set_to_none=True)\n",
    "\n",
    "    _, _, _, loss = model(\n",
    "        train_data[:, :-1], targets=train_data[:, 1:]\n",
    "    )\n",
    "    loss.backward()\n",
    "\n",
    "    if config.train.grad_clip > 0:\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), config.train.grad_clip)\n",
    "\n",
    "    optim.step()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Log train loss, train / test acc, repetition frequency\n",
    "        attn_map, pre_lm_h, _, train_loss = model(train_data[:, :-1], targets=train_data[:, 1:])\n",
    "\n",
    "        train_pred = model.generate(\n",
    "            idx=train_data[:, :prompt_len],\n",
    "            max_new_tokens=gen_len,\n",
    "        )\n",
    "        test_pred = model.generate(\n",
    "            idx=test_data[:, :prompt_len],\n",
    "            max_new_tokens=gen_len,\n",
    "        )\n",
    "\n",
    "        train_acc = torch.mean(\n",
    "            (train_pred[:, acc_start:] == train_data[:, acc_start:]).to(float)\n",
    "        ).item()\n",
    "        test_acc = torch.mean(\n",
    "            (test_pred[:, acc_start:] == test_data[:, acc_start:]).to(float)\n",
    "        ).item()\n",
    "\n",
    "        data_repeat_frac = torch.mean((test_data[:, acc_start:-1] == test_data[:, acc_start+1:]).to(float))\n",
    "        model_repeat_frac = torch.mean((test_pred[:, acc_start:-1] == test_pred[:, acc_start+1:]).to(float))\n",
    "\n",
    "        # Log attention progress measure\n",
    "        attn_map_output_seq = attn_map[:, :, acc_start-1:]\n",
    "        att_mask = torch.zeros_like(attn_map_output_seq).to(device)\n",
    "\n",
    "        att_mask[:, :, 0, 0] = 1\n",
    "        for i in range(num_tokens - 1):\n",
    "            att_mask[:, :, i + 1, i : i + 2] = 1\n",
    "\n",
    "        att_prog_measure = torch.mean(\n",
    "            torch.sum(torch.abs(attn_map_output_seq) * att_mask, dim=(-3, -2, -1)) /\n",
    "            torch.sum(torch.abs(attn_map_output_seq), dim=(-3, -2, -1)),\n",
    "            dim=0\n",
    "        )\n",
    "\n",
    "        # Log pair-wise cosine similarity between hidden states\n",
    "        embed_start = acc_start - 1\n",
    "        embed_len = gen_len\n",
    "\n",
    "        logit_cs = torch.zeros((embed_len, embed_len))\n",
    "\n",
    "        for i_1 in range(embed_start, embed_start + embed_len):\n",
    "            for i_2 in range(embed_start, i_1):\n",
    "                logit_cs[i_1 - embed_start, i_2 - embed_start] = torch.mean(\n",
    "                    (\n",
    "                        cosine_similarity(\n",
    "                            pre_lm_h[:, i_1, :], pre_lm_h[:, i_2, :], dim=-1\n",
    "                        )\n",
    "                    ), dim=0\n",
    "                )\n",
    "\n",
    "        # Log plots for cosine similarity, attention map\n",
    "        logit_fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(30, 15))\n",
    "\n",
    "        im1 = ax[0].imshow(logit_cs)\n",
    "        ax[0].set_title(\"avg pre_lm_h cosine sim\")\n",
    "        cb1 = logit_fig.colorbar(im1, location=\"right\", shrink=0.99, pad=0.02, ax=ax[0])\n",
    "\n",
    "        avg_attn_map = torch.mean(attn_map, dim=0).squeeze().detach().cpu().numpy()\n",
    "\n",
    "        im2 = ax[1].imshow(avg_attn_map)\n",
    "        ax[1].set_title(\"att map\")\n",
    "        cb4 = logit_fig.colorbar(im2, location=\"right\", shrink=0.99, pad=0.02, ax=ax[1])\n",
    "        ax[1].set_xticks(range(avg_attn_map.shape[-1]))\n",
    "        ax[1].set_yticks(range(avg_attn_map.shape[-2]))\n",
    "\n",
    "        for i1 in range(embed_len):\n",
    "            for i2 in range(embed_len):\n",
    "                text1 = ax[0].text(\n",
    "                    i2,\n",
    "                    i1,\n",
    "                    round(logit_cs[i1, i2].item(), 2),\n",
    "                    ha=\"center\",\n",
    "                    va=\"center\",\n",
    "                    color=\"w\",\n",
    "                )\n",
    "\n",
    "\n",
    "        print(\n",
    "            f\"Step {step} -- Train loss: {train_loss}, Train Acc: {train_acc} Test Acc: {test_acc}\"\n",
    "        )\n",
    "        # print(f\"input: {test_data[0]} \\n predicted:{test_pred[0]}\")\n",
    "\n",
    "        if config.train.wandb:\n",
    "\n",
    "            log_data = {\n",
    "                \"train_loss\": train_loss,\n",
    "                \"train_acc\": train_acc,\n",
    "                \"test_acc\": test_acc,\n",
    "                \"data_repeat_frac\": data_repeat_frac,\n",
    "                \"model_repeat_frac\": model_repeat_frac,\n",
    "                \"att_prog_measure\": att_prog_measure,\n",
    "                \"pre_lm_h_cosine_sim\": logit_fig,\n",
    "                \"mean_cosine_sim\": torch.sum(logit_cs[:, 1:]) / (0.5 * (gen_len-1) * (gen_len-2))\n",
    "            }\n",
    "\n",
    "            for output_pos in range(gen_len):\n",
    "                log_data.update(\n",
    "                    {\n",
    "                        f\"idx{output_pos}_check\": torch.mean(\n",
    "                            (train_pred[:, acc_start + output_pos] == train_data[:, acc_start + output_pos]).to(float)\n",
    "                        ).item()\n",
    "                    }\n",
    "                )\n",
    "\n",
    "                if output_pos < gen_len-1:\n",
    "                    log_data.update(\n",
    "                        {\n",
    "                            f\"mean_cosine_sim_{output_pos}\": torch.sum(logit_cs[:, output_pos]) / (gen_len-1-output_pos)\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "            wandb.log(log_data)\n",
    "\n",
    "        plt.close()\n",
    "        del (\n",
    "            logit_fig,\n",
    "            ax,\n",
    "            logit_cs,\n",
    "        )\n",
    "\n",
    "        if config.train.save_ckpt:\n",
    "            if (step == 0) or ((step + 1) % config.train.ckpt_freq == 0):\n",
    "                model.train()\n",
    "                torch.save(\n",
    "                    {\n",
    "                        \"epoch\": step,\n",
    "                        \"model\": model.state_dict(),\n",
    "                        \"optim\": optim.state_dict(),\n",
    "                        \"train_loss\": train_loss,\n",
    "                        \"test_acc\": test_acc,\n",
    "                    },\n",
    "                    \"./mws_k2_l1_h1_a16_n16.tar\",\n",
    "                )\n",
    "                print(f\"saved state at epoch {step} to {f'./mws_k2_l1_h1_a16_n16.tar'}\")\n",
    "\n",
    "                if config.train.wandb:\n",
    "                    model_wandb = wandb.Artifact(\n",
    "                        f\"model_step{step}\", type=\"model\"\n",
    "                    )\n",
    "                    model_wandb.add_file(f\"./mws_k2_l1_h1_a16_n16.tar\")\n",
    "                    wandb.log_artifact(model_wandb)\n",
    "                    print(\"model uploaded to wandb\")\n",
    "\n",
    "config = DotMap(config)\n",
    "\n",
    "config.model.vocab_size = max(config.data.p, config.data.max_num) + 1\n",
    "config.model.block_size = 2 * config.data.num_tokens + 1\n",
    "\n",
    "data_sampler = MovingWindowSum(\n",
    "    min_num=config.data.min_num,\n",
    "    max_num=config.data.max_num,\n",
    "    k=config.data.k,\n",
    "    p=config.data.p,\n",
    ")\n",
    "\n",
    "model = GPTLinear(config.model, return_att=True).to(device)\n",
    "optim = Adam(model.parameters(), lr=config.train.lr)\n",
    "\n",
    "if config.train.wandb:\n",
    "    wandb_run_name = 'mws'\n",
    "    wandb.login(key=\"\")\n",
    "    wandb.init(project=\"tf-loss-plateau\", name=wandb_run_name, config=config)\n",
    "    wandb.watch(model)\n",
    "\n",
    "for step in range(config.train.num_steps):\n",
    "    train_step(\n",
    "        model=model,\n",
    "        optim=optim,\n",
    "        data_sampler=data_sampler,\n",
    "        step=step,\n",
    "        config=config,\n",
    "    )\n",
    "\n",
    "if config.train.wandb:\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FglJOzSAKJaa"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
