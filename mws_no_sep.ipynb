{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "YiF5Vq1LGhEw"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import yaml\n",
    "import argparse\n",
    "from dotmap import DotMap\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.optim import Adam\n",
    "from torch.nn.functional import cosine_similarity\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"./src\")  # make sure Python can find src/\n",
    "from model_linear import GPTLinear\n",
    "from model_softmax import GPTSoftmax\n",
    "from data import MovingWindowSum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defined MWS without a mod function to it to see effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "\n",
    "class MovingWindowSumNoSep:\n",
    "    def __init__(self, min_num=1, max_num=16, k=2, p=17, sep=17, device=\"cuda\"):\n",
    "        self.min_num = min_num\n",
    "        self.max_num = max_num\n",
    "        self.k = k\n",
    "        self.p = p\n",
    "        self.sep = sep\n",
    "        self.device = device\n",
    "        assert self.p > self.max_num\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def sample(\n",
    "        self,\n",
    "        num_samples,\n",
    "        num_tokens,\n",
    "    ):\n",
    "        random_ints = torch.randint(\n",
    "            low=self.min_num, high=self.max_num + 1, size=(num_samples, num_tokens)\n",
    "        ).to(self.device)\n",
    "\n",
    "        random_ints_np = random_ints.detach().cpu().numpy()\n",
    "        convolution = torch.stack(\n",
    "            [\n",
    "                torch.from_numpy(\n",
    "                    np.convolve(\n",
    "                        random_ints_np[i],\n",
    "                        np.ones(self.k),\n",
    "                        mode=\"valid\",\n",
    "                    )\n",
    "                )\n",
    "                for i in range(random_ints.shape[0])\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        moving_sum = random_ints.clone().detach()\n",
    "        moving_sum[:, self.k - 1 :] = convolution\n",
    "\n",
    "        # for i in range(num_samples):\n",
    "        #     for j in range(0, self.k - 1):\n",
    "        #         if moving_sum[i, j] != random_ints[i, j]:\n",
    "        #             print(f\"ERROR! {i} {j}\")\n",
    "        #     for j in range(self.k - 1, num_tokens):\n",
    "        #         if moving_sum[i, j] != torch.sum(random_ints[i, j-self.k+1:j+1]):\n",
    "        #             print(f\"ERROR! {i} {j}\")\n",
    "\n",
    "        # exit()\n",
    "        samples = (\n",
    "            torch.cat(\n",
    "                [\n",
    "                    random_ints,\n",
    "                    # self.sep * torch.ones(size=(num_samples, 1)).to(self.device),\n",
    "                    torch.remainder(input=moving_sum, other=self.p),\n",
    "                ],\n",
    "                axis=-1,\n",
    "            )\n",
    "            .to(int)\n",
    "            .detach()\n",
    "        )\n",
    "\n",
    "        return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = MovingWindowSumNoSep()\n",
    "test = data.sample(num_samples=1, num_tokens = 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([13, 10, 13,  9,  5,  4,  2,  7,  5, 14, 10, 14, 12,  4, 13,  4],\n",
      "       device='cuda:0')\n",
      "tensor([13,  6,  6,  5, 14,  9,  6,  9, 12,  2,  7,  7,  9, 16,  0,  0],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(test[0, :16])\n",
    "print(test[0, 16:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "def train_step(\n",
    "    model,\n",
    "    optim,\n",
    "    data_sampler,\n",
    "    step,\n",
    "    config,\n",
    "):\n",
    "    n_train, n_test, num_tokens = (\n",
    "        config.data.n_train,\n",
    "        config.data.n_test,\n",
    "        config.data.num_tokens,\n",
    "    )\n",
    "\n",
    "    data = data_sampler.sample(\n",
    "        num_samples=n_train + n_test,\n",
    "        num_tokens=num_tokens,\n",
    "    )\n",
    "\n",
    "    train_data = data[:n_train, :]\n",
    "    test_data = data[n_train:, :]\n",
    "\n",
    "    prompt_len = num_tokens# + 1\n",
    "    gen_len = num_tokens\n",
    "    acc_start = num_tokens# + 1\n",
    "\n",
    "    model.train()\n",
    "    optim.zero_grad(set_to_none=True)\n",
    "\n",
    "    _, _, _, loss = model(\n",
    "        train_data[:, :-1], targets=train_data[:, 1:]\n",
    "    )\n",
    "    loss.backward()\n",
    "\n",
    "    if config.train.grad_clip > 0:\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), config.train.grad_clip)\n",
    "\n",
    "    optim.step()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Log train loss, train / test acc, repetition frequency\n",
    "        attn_map, pre_lm_h, _, train_loss = model(train_data[:, :-1], targets=train_data[:, 1:])\n",
    "\n",
    "        train_pred = model.generate(\n",
    "            idx=train_data[:, :prompt_len],\n",
    "            max_new_tokens=gen_len,\n",
    "        )\n",
    "        test_pred = model.generate(\n",
    "            idx=test_data[:, :prompt_len],\n",
    "            max_new_tokens=gen_len,\n",
    "        )\n",
    "\n",
    "        train_acc = torch.mean(\n",
    "            (train_pred[:, acc_start:] == train_data[:, acc_start:]).to(float)\n",
    "        ).item()\n",
    "        test_acc = torch.mean(\n",
    "            (test_pred[:, acc_start:] == test_data[:, acc_start:]).to(float)\n",
    "        ).item()\n",
    "\n",
    "        data_repeat_frac = torch.mean((test_data[:, acc_start:-1] == test_data[:, acc_start+1:]).to(float))\n",
    "        model_repeat_frac = torch.mean((test_pred[:, acc_start:-1] == test_pred[:, acc_start+1:]).to(float))\n",
    "\n",
    "        # Log attention progress measure\n",
    "        attn_map_output_seq = attn_map[:, :, acc_start-1:]\n",
    "        att_mask = torch.zeros_like(attn_map_output_seq).to(device)\n",
    "\n",
    "        att_mask[:, :, 0, 0] = 1\n",
    "        for i in range(num_tokens - 1):\n",
    "            att_mask[:, :, i + 1, i : i + 2] = 1\n",
    "\n",
    "        att_prog_measure = torch.mean(\n",
    "            torch.sum(torch.abs(attn_map_output_seq) * att_mask, dim=(-3, -2, -1)) /\n",
    "            torch.sum(torch.abs(attn_map_output_seq), dim=(-3, -2, -1)),\n",
    "            dim=0\n",
    "        )\n",
    "\n",
    "        # Log pair-wise cosine similarity between hidden states\n",
    "        embed_start = acc_start - 1\n",
    "        embed_len = gen_len\n",
    "\n",
    "        logit_cs = torch.zeros((embed_len, embed_len))\n",
    "\n",
    "        for i_1 in range(embed_start, embed_start + embed_len):\n",
    "            for i_2 in range(embed_start, i_1):\n",
    "                logit_cs[i_1 - embed_start, i_2 - embed_start] = torch.mean(\n",
    "                    (\n",
    "                        cosine_similarity(\n",
    "                            pre_lm_h[:, i_1, :], pre_lm_h[:, i_2, :], dim=-1\n",
    "                        )\n",
    "                    ), dim=0\n",
    "                )\n",
    "\n",
    "        # Log plots for cosine similarity, attention map\n",
    "        logit_fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(30, 15))\n",
    "\n",
    "        im1 = ax[0].imshow(logit_cs)\n",
    "        ax[0].set_title(\"avg pre_lm_h cosine sim\")\n",
    "        cb1 = logit_fig.colorbar(im1, location=\"right\", shrink=0.99, pad=0.02, ax=ax[0])\n",
    "\n",
    "        avg_attn_map = torch.mean(attn_map, dim=0).squeeze().detach().cpu().numpy()\n",
    "\n",
    "        im2 = ax[1].imshow(avg_attn_map)\n",
    "        ax[1].set_title(\"att map\")\n",
    "        cb4 = logit_fig.colorbar(im2, location=\"right\", shrink=0.99, pad=0.02, ax=ax[1])\n",
    "        ax[1].set_xticks(range(avg_attn_map.shape[-1]))\n",
    "        ax[1].set_yticks(range(avg_attn_map.shape[-2]))\n",
    "\n",
    "        for i1 in range(embed_len):\n",
    "            for i2 in range(embed_len):\n",
    "                text1 = ax[0].text(\n",
    "                    i2,\n",
    "                    i1,\n",
    "                    round(logit_cs[i1, i2].item(), 2),\n",
    "                    ha=\"center\",\n",
    "                    va=\"center\",\n",
    "                    color=\"w\",\n",
    "                )\n",
    "\n",
    "\n",
    "        print(\n",
    "            f\"Step {step} -- Train loss: {train_loss}, Train Acc: {train_acc} Test Acc: {test_acc}\"\n",
    "        )\n",
    "        # print(f\"input: {test_data[0]} \\n predicted:{test_pred[0]}\")\n",
    "\n",
    "        if config.train.wandb:\n",
    "\n",
    "            log_data = {\n",
    "                \"train_loss\": train_loss,\n",
    "                \"train_acc\": train_acc,\n",
    "                \"test_acc\": test_acc,\n",
    "                \"data_repeat_frac\": data_repeat_frac,\n",
    "                \"model_repeat_frac\": model_repeat_frac,\n",
    "                \"att_prog_measure\": att_prog_measure,\n",
    "                \"pre_lm_h_cosine_sim\": logit_fig,\n",
    "                \"mean_cosine_sim\": torch.sum(logit_cs[:, 1:]) / (0.5 * (gen_len-1) * (gen_len-2))\n",
    "            }\n",
    "\n",
    "            for output_pos in range(gen_len):\n",
    "                log_data.update(\n",
    "                    {\n",
    "                        f\"idx{output_pos}_check\": torch.mean(\n",
    "                            (train_pred[:, acc_start + output_pos] == train_data[:, acc_start + output_pos]).to(float)\n",
    "                        ).item()\n",
    "                    }\n",
    "                )\n",
    "\n",
    "                if output_pos < gen_len-1:\n",
    "                    log_data.update(\n",
    "                        {\n",
    "                            f\"mean_cosine_sim_{output_pos}\": torch.sum(logit_cs[:, output_pos]) / (gen_len-1-output_pos)\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "            wandb.log(log_data)\n",
    "\n",
    "        plt.close()\n",
    "        del (\n",
    "            logit_fig,\n",
    "            ax,\n",
    "            logit_cs,\n",
    "        )\n",
    "\n",
    "        if config.train.save_ckpt:\n",
    "            if (step == 0) or ((step + 1) % config.train.ckpt_freq == 0):\n",
    "                model.train()\n",
    "                torch.save(\n",
    "                    {\n",
    "                        \"epoch\": step,\n",
    "                        \"model\": model.state_dict(),\n",
    "                        \"optim\": optim.state_dict(),\n",
    "                        \"train_loss\": train_loss,\n",
    "                        \"test_acc\": test_acc,\n",
    "                    },\n",
    "                    \"./mws_k2_l1_h1_a16_n16.tar\",\n",
    "                )\n",
    "                print(f\"saved state at epoch {step} to {f'./mws_k2_l1_h1_a16_n16.tar'}\")\n",
    "\n",
    "                if config.train.wandb:\n",
    "                    model_wandb = wandb.Artifact(\n",
    "                        f\"model_step{step}\", type=\"model\"\n",
    "                    )\n",
    "                    model_wandb.add_file(f\"./mws_k2_l1_h1_a16_n16.tar\")\n",
    "                    wandb.log_artifact(model_wandb)\n",
    "                    print(\"model uploaded to wandb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# device = \"cpu\"\n",
    "\n",
    "config = {\n",
    "'model':\n",
    "  {\n",
    "    'n_layer': 1,\n",
    "    'n_head': 1,\n",
    "    'n_embd': 256,\n",
    "    'linear': True,\n",
    "  },\n",
    "\n",
    "'data':\n",
    "  {\n",
    "    'name': 'window',\n",
    "    'min_num': 1,\n",
    "    'max_num': 16,\n",
    "    'k': 2,\n",
    "    'p': 17,\n",
    "    'sep': 17,\n",
    "    'cot': False,\n",
    "    'num_tokens': 16,\n",
    "    'n_train': 256,\n",
    "    'n_test': 64,\n",
    "    'fixed_len': True,\n",
    "  },\n",
    "\n",
    "'train':\n",
    "  {\n",
    "    'lr': 0.0001,\n",
    "    'grad_clip': -1,\n",
    "    'num_steps': 500,\n",
    "    'norm_type': \"none_rank\",\n",
    "    'wandb': True,\n",
    "    'save_ckpt': False,\n",
    "    'ckpt_freq': 20,\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'data'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m data_sampler \u001b[38;5;241m=\u001b[39m MovingWindowSumNoSep(\n\u001b[0;32m----> 2\u001b[0m     min_num\u001b[38;5;241m=\u001b[39m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241m.\u001b[39mmin_num,\n\u001b[1;32m      3\u001b[0m     max_num\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mmax_num,\n\u001b[1;32m      4\u001b[0m     k\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mk,\n\u001b[1;32m      5\u001b[0m     p\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mp,\n\u001b[1;32m      6\u001b[0m     device\u001b[38;5;241m=\u001b[39mdevice\n\u001b[1;32m      7\u001b[0m )\n\u001b[1;32m      8\u001b[0m data_sampler\u001b[38;5;241m.\u001b[39msample(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m16\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'data'"
     ]
    }
   ],
   "source": [
    "data_sampler = MovingWindowSumNoSep(\n",
    "    min_num=config.data.min_num,\n",
    "    max_num=config.data.max_num,\n",
    "    k=config.data.k,\n",
    "    p=config.data.p,\n",
    "    device=device\n",
    ")\n",
    "data_sampler.sample(1, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:ukncigwc) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>att_prog_measure</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▅███████████████████████</td></tr><tr><td>data_repeat_frac</td><td>▄▅▇▅▃▅▄▄▅▅▅▄▆▃▅▂▃▅▂▃▅▃▁▃▇▅▄▃▆▆▆▄▆▃▆▅█▆▃▃</td></tr><tr><td>idx0_check</td><td>▃▄▄▃▅▄▃▂▄▅█▅▆▄▄▅▆▄▄▃▂▄▄▄▅▁▅▆▅▆▅▆▃▅▃▅▃▅▃▄</td></tr><tr><td>idx10_check</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁████████████████████████</td></tr><tr><td>idx11_check</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▇███████████████████████</td></tr><tr><td>idx12_check</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁████████████████████████</td></tr><tr><td>idx13_check</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▅███████████████████████</td></tr><tr><td>idx14_check</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▆███████████████████████</td></tr><tr><td>idx15_check</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▄███████████████████████</td></tr><tr><td>idx1_check</td><td>▅▄▇▃▄▅▄▃▅▃▆█▅▆▅▆▇▅▄▃▂▅▅▅▆▁▆▇▅▇▆▇▄▆▃▅▃▆▄▄</td></tr><tr><td>idx2_check</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▇███████████████████████</td></tr><tr><td>idx3_check</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁████████████████████████</td></tr><tr><td>idx4_check</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁████████████████████████</td></tr><tr><td>idx5_check</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁████████████████████████</td></tr><tr><td>idx6_check</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▇███████████████████████</td></tr><tr><td>idx7_check</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂████████████████████████</td></tr><tr><td>idx8_check</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁████████████████████████</td></tr><tr><td>idx9_check</td><td>▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▂▄███████████████████████</td></tr><tr><td>mean_cosine_sim</td><td>▇█████████▇▇▇▇▇▇▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>mean_cosine_sim_0</td><td>▇█▇▇▇▇▇▆▆▅▄▄▄▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>mean_cosine_sim_1</td><td>▇█████████▄▃▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>mean_cosine_sim_10</td><td>▆▇█████████████▇▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>mean_cosine_sim_11</td><td>▆▇█████████████▇▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>mean_cosine_sim_12</td><td>▇▇█████████████▇▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>mean_cosine_sim_13</td><td>▆▇█████████████▇▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>mean_cosine_sim_14</td><td>▆▇█████████████▇▃▁▁▁▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁</td></tr><tr><td>mean_cosine_sim_2</td><td>▇██████████████▇▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>mean_cosine_sim_3</td><td>▇██████████████▇▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>mean_cosine_sim_4</td><td>▇██████████████▇▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>mean_cosine_sim_5</td><td>▇██████████████▇▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>mean_cosine_sim_6</td><td>▆██████████████▇▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>mean_cosine_sim_7</td><td>▆▇█████████████▇▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>mean_cosine_sim_8</td><td>▇██████████████▇▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>mean_cosine_sim_9</td><td>▇██████████████▇▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>model_repeat_frac</td><td>▁▅▄▂▂▃▅▄▂█▂▂▂▆█▄▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>test_acc</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▇███████████████████████</td></tr><tr><td>train_acc</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▇███████████████████████</td></tr><tr><td>train_loss</td><td>████████████████▄▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>att_prog_measure</td><td>0.87435</td></tr><tr><td>data_repeat_frac</td><td>0.05417</td></tr><tr><td>idx0_check</td><td>0.06641</td></tr><tr><td>idx10_check</td><td>1.0</td></tr><tr><td>idx11_check</td><td>1.0</td></tr><tr><td>idx12_check</td><td>1.0</td></tr><tr><td>idx13_check</td><td>1.0</td></tr><tr><td>idx14_check</td><td>1.0</td></tr><tr><td>idx15_check</td><td>1.0</td></tr><tr><td>idx1_check</td><td>0.06641</td></tr><tr><td>idx2_check</td><td>1.0</td></tr><tr><td>idx3_check</td><td>1.0</td></tr><tr><td>idx4_check</td><td>1.0</td></tr><tr><td>idx5_check</td><td>1.0</td></tr><tr><td>idx6_check</td><td>1.0</td></tr><tr><td>idx7_check</td><td>1.0</td></tr><tr><td>idx8_check</td><td>1.0</td></tr><tr><td>idx9_check</td><td>1.0</td></tr><tr><td>mean_cosine_sim</td><td>0.00535</td></tr><tr><td>mean_cosine_sim_0</td><td>0.04021</td></tr><tr><td>mean_cosine_sim_1</td><td>0.00869</td></tr><tr><td>mean_cosine_sim_10</td><td>0.00965</td></tr><tr><td>mean_cosine_sim_11</td><td>-0.00945</td></tr><tr><td>mean_cosine_sim_12</td><td>0.01508</td></tr><tr><td>mean_cosine_sim_13</td><td>0.00816</td></tr><tr><td>mean_cosine_sim_14</td><td>0.02504</td></tr><tr><td>mean_cosine_sim_2</td><td>0.00463</td></tr><tr><td>mean_cosine_sim_3</td><td>0.01114</td></tr><tr><td>mean_cosine_sim_4</td><td>-0.00176</td></tr><tr><td>mean_cosine_sim_5</td><td>0.01223</td></tr><tr><td>mean_cosine_sim_6</td><td>-0.00454</td></tr><tr><td>mean_cosine_sim_7</td><td>-0.00269</td></tr><tr><td>mean_cosine_sim_8</td><td>0.00103</td></tr><tr><td>mean_cosine_sim_9</td><td>0.01681</td></tr><tr><td>model_repeat_frac</td><td>0.05417</td></tr><tr><td>test_acc</td><td>0.88477</td></tr><tr><td>train_acc</td><td>0.8833</td></tr><tr><td>train_loss</td><td>1.43405</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">mws_softmax_attention_mwsNoSep_1000steps</strong> at: <a href='https://wandb.ai/wth_ucsd/loss_plateau_tf/runs/ukncigwc' target=\"_blank\">https://wandb.ai/wth_ucsd/loss_plateau_tf/runs/ukncigwc</a><br/>Synced 6 W&B file(s), 833 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251013_143421-ukncigwc/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:ukncigwc). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.22.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jyue/private/tf-loss-plateau/wandb/run-20251013_144809-ywpesvzo</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/wth_ucsd/loss_plateau_tf/runs/ywpesvzo' target=\"_blank\">mws_linear_attention_mwsNoSep_500steps</a></strong> to <a href='https://wandb.ai/wth_ucsd/loss_plateau_tf' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/wth_ucsd/loss_plateau_tf' target=\"_blank\">https://wandb.ai/wth_ucsd/loss_plateau_tf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/wth_ucsd/loss_plateau_tf/runs/ywpesvzo' target=\"_blank\">https://wandb.ai/wth_ucsd/loss_plateau_tf/runs/ywpesvzo</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0 -- Train loss: 2.8803505897521973, Train Acc: 0.06591796875 Test Acc: 0.0537109375\n",
      "Step 1 -- Train loss: 2.8661346435546875, Train Acc: 0.0673828125 Test Acc: 0.044921875\n",
      "Step 2 -- Train loss: 2.8638405799865723, Train Acc: 0.066650390625 Test Acc: 0.0625\n",
      "Step 3 -- Train loss: 2.852747917175293, Train Acc: 0.063720703125 Test Acc: 0.0703125\n",
      "Step 4 -- Train loss: 2.8412275314331055, Train Acc: 0.06103515625 Test Acc: 0.060546875\n",
      "Step 5 -- Train loss: 2.8388898372650146, Train Acc: 0.063720703125 Test Acc: 0.068359375\n",
      "Step 6 -- Train loss: 2.8427233695983887, Train Acc: 0.058349609375 Test Acc: 0.04296875\n",
      "Step 7 -- Train loss: 2.8331716060638428, Train Acc: 0.056396484375 Test Acc: 0.064453125\n",
      "Step 8 -- Train loss: 2.8313705921173096, Train Acc: 0.06103515625 Test Acc: 0.0615234375\n",
      "Step 9 -- Train loss: 2.8249669075012207, Train Acc: 0.066650390625 Test Acc: 0.0625\n",
      "Step 10 -- Train loss: 2.8227436542510986, Train Acc: 0.06396484375 Test Acc: 0.0625\n",
      "Step 11 -- Train loss: 2.8242197036743164, Train Acc: 0.063232421875 Test Acc: 0.0556640625\n",
      "Step 12 -- Train loss: 2.820688486099243, Train Acc: 0.063232421875 Test Acc: 0.0556640625\n",
      "Step 13 -- Train loss: 2.8190488815307617, Train Acc: 0.063232421875 Test Acc: 0.0556640625\n",
      "Step 14 -- Train loss: 2.819413661956787, Train Acc: 0.06201171875 Test Acc: 0.0625\n",
      "Step 15 -- Train loss: 2.817112922668457, Train Acc: 0.059814453125 Test Acc: 0.06640625\n",
      "Step 16 -- Train loss: 2.817568302154541, Train Acc: 0.06005859375 Test Acc: 0.044921875\n",
      "Step 17 -- Train loss: 2.817589521408081, Train Acc: 0.0615234375 Test Acc: 0.0693359375\n",
      "Step 18 -- Train loss: 2.8141276836395264, Train Acc: 0.06005859375 Test Acc: 0.064453125\n",
      "Step 19 -- Train loss: 2.816502809524536, Train Acc: 0.060546875 Test Acc: 0.056640625\n",
      "Step 20 -- Train loss: 2.812044858932495, Train Acc: 0.064208984375 Test Acc: 0.0537109375\n",
      "Step 21 -- Train loss: 2.8127613067626953, Train Acc: 0.06396484375 Test Acc: 0.060546875\n",
      "Step 22 -- Train loss: 2.81278395652771, Train Acc: 0.06201171875 Test Acc: 0.052734375\n",
      "Step 23 -- Train loss: 2.81097149848938, Train Acc: 0.068359375 Test Acc: 0.05859375\n",
      "Step 24 -- Train loss: 2.8113090991973877, Train Acc: 0.057373046875 Test Acc: 0.072265625\n",
      "Step 25 -- Train loss: 2.8106307983398438, Train Acc: 0.05615234375 Test Acc: 0.0712890625\n",
      "Step 26 -- Train loss: 2.810032606124878, Train Acc: 0.070556640625 Test Acc: 0.07421875\n",
      "Step 27 -- Train loss: 2.808096408843994, Train Acc: 0.065673828125 Test Acc: 0.068359375\n",
      "Step 28 -- Train loss: 2.809835910797119, Train Acc: 0.066650390625 Test Acc: 0.0546875\n",
      "Step 29 -- Train loss: 2.8084585666656494, Train Acc: 0.061767578125 Test Acc: 0.0556640625\n",
      "Step 30 -- Train loss: 2.8070573806762695, Train Acc: 0.065673828125 Test Acc: 0.060546875\n",
      "Step 31 -- Train loss: 2.8077666759490967, Train Acc: 0.06884765625 Test Acc: 0.05859375\n",
      "Step 32 -- Train loss: 2.8068418502807617, Train Acc: 0.059326171875 Test Acc: 0.05859375\n",
      "Step 33 -- Train loss: 2.8051695823669434, Train Acc: 0.064453125 Test Acc: 0.052734375\n",
      "Step 34 -- Train loss: 2.806515693664551, Train Acc: 0.0654296875 Test Acc: 0.0654296875\n",
      "Step 35 -- Train loss: 2.8055341243743896, Train Acc: 0.067138671875 Test Acc: 0.0595703125\n",
      "Step 36 -- Train loss: 2.8062920570373535, Train Acc: 0.065673828125 Test Acc: 0.072265625\n",
      "Step 37 -- Train loss: 2.806061029434204, Train Acc: 0.072998046875 Test Acc: 0.0830078125\n",
      "Step 38 -- Train loss: 2.804006338119507, Train Acc: 0.0771484375 Test Acc: 0.0693359375\n",
      "Step 39 -- Train loss: 2.803295850753784, Train Acc: 0.073486328125 Test Acc: 0.0810546875\n",
      "Step 40 -- Train loss: 2.8057234287261963, Train Acc: 0.075439453125 Test Acc: 0.07421875\n",
      "Step 41 -- Train loss: 2.802466869354248, Train Acc: 0.073486328125 Test Acc: 0.07421875\n",
      "Step 42 -- Train loss: 2.8015120029449463, Train Acc: 0.080322265625 Test Acc: 0.076171875\n",
      "Step 43 -- Train loss: 2.804985284805298, Train Acc: 0.0712890625 Test Acc: 0.0625\n",
      "Step 44 -- Train loss: 2.802027463912964, Train Acc: 0.08984375 Test Acc: 0.068359375\n",
      "Step 45 -- Train loss: 2.7998759746551514, Train Acc: 0.092041015625 Test Acc: 0.0908203125\n",
      "Step 46 -- Train loss: 2.8000454902648926, Train Acc: 0.083251953125 Test Acc: 0.0859375\n",
      "Step 47 -- Train loss: 2.7999274730682373, Train Acc: 0.0869140625 Test Acc: 0.0966796875\n",
      "Step 48 -- Train loss: 2.7967801094055176, Train Acc: 0.10009765625 Test Acc: 0.0986328125\n",
      "Step 49 -- Train loss: 2.79622745513916, Train Acc: 0.09228515625 Test Acc: 0.0869140625\n",
      "Step 50 -- Train loss: 2.7941436767578125, Train Acc: 0.096923828125 Test Acc: 0.1171875\n",
      "Step 51 -- Train loss: 2.7919907569885254, Train Acc: 0.105712890625 Test Acc: 0.119140625\n",
      "Step 52 -- Train loss: 2.7924797534942627, Train Acc: 0.1005859375 Test Acc: 0.1103515625\n",
      "Step 53 -- Train loss: 2.7867212295532227, Train Acc: 0.1083984375 Test Acc: 0.109375\n",
      "Step 54 -- Train loss: 2.7893099784851074, Train Acc: 0.103515625 Test Acc: 0.10546875\n",
      "Step 55 -- Train loss: 2.784562587738037, Train Acc: 0.108642578125 Test Acc: 0.103515625\n",
      "Step 56 -- Train loss: 2.7825374603271484, Train Acc: 0.10986328125 Test Acc: 0.1171875\n",
      "Step 57 -- Train loss: 2.7785696983337402, Train Acc: 0.113525390625 Test Acc: 0.1083984375\n",
      "Step 58 -- Train loss: 2.779306650161743, Train Acc: 0.10546875 Test Acc: 0.1044921875\n",
      "Step 59 -- Train loss: 2.773956537246704, Train Acc: 0.114501953125 Test Acc: 0.126953125\n",
      "Step 60 -- Train loss: 2.770463228225708, Train Acc: 0.109375 Test Acc: 0.115234375\n",
      "Step 61 -- Train loss: 2.7687649726867676, Train Acc: 0.11328125 Test Acc: 0.115234375\n",
      "Step 62 -- Train loss: 2.7637393474578857, Train Acc: 0.114501953125 Test Acc: 0.1123046875\n",
      "Step 63 -- Train loss: 2.7647223472595215, Train Acc: 0.11376953125 Test Acc: 0.107421875\n",
      "Step 64 -- Train loss: 2.762415647506714, Train Acc: 0.1083984375 Test Acc: 0.125\n",
      "Step 65 -- Train loss: 2.75849986076355, Train Acc: 0.113525390625 Test Acc: 0.109375\n",
      "Step 66 -- Train loss: 2.7547781467437744, Train Acc: 0.117431640625 Test Acc: 0.1064453125\n",
      "Step 67 -- Train loss: 2.7539472579956055, Train Acc: 0.115966796875 Test Acc: 0.1025390625\n",
      "Step 68 -- Train loss: 2.7504947185516357, Train Acc: 0.114013671875 Test Acc: 0.1328125\n",
      "Step 69 -- Train loss: 2.7471938133239746, Train Acc: 0.119873046875 Test Acc: 0.11328125\n",
      "Step 70 -- Train loss: 2.744492292404175, Train Acc: 0.114501953125 Test Acc: 0.1201171875\n",
      "Step 71 -- Train loss: 2.7459869384765625, Train Acc: 0.12060546875 Test Acc: 0.1064453125\n",
      "Step 72 -- Train loss: 2.746527910232544, Train Acc: 0.11279296875 Test Acc: 0.1162109375\n",
      "Step 73 -- Train loss: 2.7427501678466797, Train Acc: 0.115966796875 Test Acc: 0.11328125\n",
      "Step 74 -- Train loss: 2.7412047386169434, Train Acc: 0.11865234375 Test Acc: 0.119140625\n",
      "Step 75 -- Train loss: 2.7407517433166504, Train Acc: 0.106689453125 Test Acc: 0.1142578125\n",
      "Step 76 -- Train loss: 2.734313488006592, Train Acc: 0.116455078125 Test Acc: 0.1181640625\n",
      "Step 77 -- Train loss: 2.7350943088531494, Train Acc: 0.11962890625 Test Acc: 0.1162109375\n",
      "Step 78 -- Train loss: 2.7326674461364746, Train Acc: 0.11474609375 Test Acc: 0.125\n",
      "Step 79 -- Train loss: 2.734466791152954, Train Acc: 0.1083984375 Test Acc: 0.109375\n",
      "Step 80 -- Train loss: 2.7340738773345947, Train Acc: 0.116943359375 Test Acc: 0.115234375\n",
      "Step 81 -- Train loss: 2.731672525405884, Train Acc: 0.11865234375 Test Acc: 0.115234375\n",
      "Step 82 -- Train loss: 2.733070135116577, Train Acc: 0.111328125 Test Acc: 0.1142578125\n",
      "Step 83 -- Train loss: 2.724142551422119, Train Acc: 0.118896484375 Test Acc: 0.11328125\n",
      "Step 84 -- Train loss: 2.727222204208374, Train Acc: 0.115478515625 Test Acc: 0.12109375\n",
      "Step 85 -- Train loss: 2.729013442993164, Train Acc: 0.11474609375 Test Acc: 0.1279296875\n",
      "Step 86 -- Train loss: 2.7290167808532715, Train Acc: 0.1142578125 Test Acc: 0.1357421875\n",
      "Step 87 -- Train loss: 2.7273669242858887, Train Acc: 0.119384765625 Test Acc: 0.1181640625\n",
      "Step 88 -- Train loss: 2.723773241043091, Train Acc: 0.114501953125 Test Acc: 0.1220703125\n",
      "Step 89 -- Train loss: 2.725903272628784, Train Acc: 0.1201171875 Test Acc: 0.1259765625\n",
      "Step 90 -- Train loss: 2.7241299152374268, Train Acc: 0.11279296875 Test Acc: 0.12890625\n",
      "Step 91 -- Train loss: 2.7226054668426514, Train Acc: 0.118408203125 Test Acc: 0.11328125\n",
      "Step 92 -- Train loss: 2.723088026046753, Train Acc: 0.109375 Test Acc: 0.123046875\n",
      "Step 93 -- Train loss: 2.7237091064453125, Train Acc: 0.110107421875 Test Acc: 0.12890625\n",
      "Step 94 -- Train loss: 2.721634864807129, Train Acc: 0.11767578125 Test Acc: 0.111328125\n",
      "Step 95 -- Train loss: 2.720795154571533, Train Acc: 0.11376953125 Test Acc: 0.1005859375\n",
      "Step 96 -- Train loss: 2.7198328971862793, Train Acc: 0.117919921875 Test Acc: 0.115234375\n",
      "Step 97 -- Train loss: 2.721146583557129, Train Acc: 0.114013671875 Test Acc: 0.126953125\n",
      "Step 98 -- Train loss: 2.7197959423065186, Train Acc: 0.11328125 Test Acc: 0.1171875\n",
      "Step 99 -- Train loss: 2.7210114002227783, Train Acc: 0.120849609375 Test Acc: 0.115234375\n",
      "Step 100 -- Train loss: 2.716747760772705, Train Acc: 0.118896484375 Test Acc: 0.119140625\n",
      "Step 101 -- Train loss: 2.7166495323181152, Train Acc: 0.1259765625 Test Acc: 0.111328125\n",
      "Step 102 -- Train loss: 2.7170064449310303, Train Acc: 0.1181640625 Test Acc: 0.1220703125\n",
      "Step 103 -- Train loss: 2.717467784881592, Train Acc: 0.119873046875 Test Acc: 0.12109375\n",
      "Step 104 -- Train loss: 2.715343713760376, Train Acc: 0.12060546875 Test Acc: 0.125\n",
      "Step 105 -- Train loss: 2.7167439460754395, Train Acc: 0.118896484375 Test Acc: 0.119140625\n",
      "Step 106 -- Train loss: 2.7177281379699707, Train Acc: 0.114501953125 Test Acc: 0.1162109375\n",
      "Step 107 -- Train loss: 2.7148020267486572, Train Acc: 0.11572265625 Test Acc: 0.1162109375\n",
      "Step 108 -- Train loss: 2.713318347930908, Train Acc: 0.119873046875 Test Acc: 0.111328125\n",
      "Step 109 -- Train loss: 2.713338851928711, Train Acc: 0.120361328125 Test Acc: 0.1103515625\n",
      "Step 110 -- Train loss: 2.71208119392395, Train Acc: 0.1220703125 Test Acc: 0.125\n",
      "Step 111 -- Train loss: 2.7128567695617676, Train Acc: 0.12353515625 Test Acc: 0.125\n",
      "Step 112 -- Train loss: 2.712589979171753, Train Acc: 0.121826171875 Test Acc: 0.1376953125\n",
      "Step 113 -- Train loss: 2.713437557220459, Train Acc: 0.12109375 Test Acc: 0.1240234375\n",
      "Step 114 -- Train loss: 2.71264910697937, Train Acc: 0.1181640625 Test Acc: 0.1337890625\n",
      "Step 115 -- Train loss: 2.7111330032348633, Train Acc: 0.12451171875 Test Acc: 0.1220703125\n",
      "Step 116 -- Train loss: 2.7114064693450928, Train Acc: 0.12255859375 Test Acc: 0.1376953125\n",
      "Step 117 -- Train loss: 2.708374500274658, Train Acc: 0.127197265625 Test Acc: 0.126953125\n",
      "Step 118 -- Train loss: 2.7076919078826904, Train Acc: 0.124267578125 Test Acc: 0.1201171875\n",
      "Step 119 -- Train loss: 2.7081034183502197, Train Acc: 0.12548828125 Test Acc: 0.123046875\n",
      "Step 120 -- Train loss: 2.706064224243164, Train Acc: 0.119140625 Test Acc: 0.1259765625\n",
      "Step 121 -- Train loss: 2.704292058944702, Train Acc: 0.12890625 Test Acc: 0.138671875\n",
      "Step 122 -- Train loss: 2.7066800594329834, Train Acc: 0.122802734375 Test Acc: 0.1259765625\n",
      "Step 123 -- Train loss: 2.704657793045044, Train Acc: 0.135009765625 Test Acc: 0.1240234375\n",
      "Step 124 -- Train loss: 2.70414662361145, Train Acc: 0.12646484375 Test Acc: 0.1396484375\n",
      "Step 125 -- Train loss: 2.704805612564087, Train Acc: 0.128662109375 Test Acc: 0.12890625\n",
      "Step 126 -- Train loss: 2.702256679534912, Train Acc: 0.1328125 Test Acc: 0.1220703125\n",
      "Step 127 -- Train loss: 2.7033674716949463, Train Acc: 0.129150390625 Test Acc: 0.126953125\n",
      "Step 128 -- Train loss: 2.7006194591522217, Train Acc: 0.130126953125 Test Acc: 0.1279296875\n",
      "Step 129 -- Train loss: 2.6983883380889893, Train Acc: 0.12939453125 Test Acc: 0.1240234375\n",
      "Step 130 -- Train loss: 2.6933786869049072, Train Acc: 0.13427734375 Test Acc: 0.1298828125\n",
      "Step 131 -- Train loss: 2.695543050765991, Train Acc: 0.130615234375 Test Acc: 0.1513671875\n",
      "Step 132 -- Train loss: 2.6924242973327637, Train Acc: 0.13916015625 Test Acc: 0.1337890625\n",
      "Step 133 -- Train loss: 2.690734386444092, Train Acc: 0.12841796875 Test Acc: 0.1357421875\n",
      "Step 134 -- Train loss: 2.6909568309783936, Train Acc: 0.142822265625 Test Acc: 0.1337890625\n",
      "Step 135 -- Train loss: 2.6868014335632324, Train Acc: 0.142822265625 Test Acc: 0.142578125\n",
      "Step 136 -- Train loss: 2.6807405948638916, Train Acc: 0.14697265625 Test Acc: 0.1513671875\n",
      "Step 137 -- Train loss: 2.679252862930298, Train Acc: 0.1484375 Test Acc: 0.1240234375\n",
      "Step 138 -- Train loss: 2.6817092895507812, Train Acc: 0.140625 Test Acc: 0.146484375\n",
      "Step 139 -- Train loss: 2.679736852645874, Train Acc: 0.148193359375 Test Acc: 0.138671875\n",
      "Step 140 -- Train loss: 2.667100191116333, Train Acc: 0.154296875 Test Acc: 0.1572265625\n",
      "Step 141 -- Train loss: 2.6666719913482666, Train Acc: 0.160888671875 Test Acc: 0.1416015625\n",
      "Step 142 -- Train loss: 2.662003755569458, Train Acc: 0.16552734375 Test Acc: 0.16015625\n",
      "Step 143 -- Train loss: 2.661165237426758, Train Acc: 0.156982421875 Test Acc: 0.15234375\n",
      "Step 144 -- Train loss: 2.65348482131958, Train Acc: 0.166259765625 Test Acc: 0.173828125\n",
      "Step 145 -- Train loss: 2.6508243083953857, Train Acc: 0.166748046875 Test Acc: 0.181640625\n",
      "Step 146 -- Train loss: 2.6404733657836914, Train Acc: 0.17724609375 Test Acc: 0.189453125\n",
      "Step 147 -- Train loss: 2.634084701538086, Train Acc: 0.193115234375 Test Acc: 0.1796875\n",
      "Step 148 -- Train loss: 2.623006820678711, Train Acc: 0.208251953125 Test Acc: 0.1904296875\n",
      "Step 149 -- Train loss: 2.610567569732666, Train Acc: 0.211669921875 Test Acc: 0.21484375\n",
      "Step 150 -- Train loss: 2.597712278366089, Train Acc: 0.228271484375 Test Acc: 0.2177734375\n",
      "Step 151 -- Train loss: 2.587843656539917, Train Acc: 0.244140625 Test Acc: 0.2373046875\n",
      "Step 152 -- Train loss: 2.572202205657959, Train Acc: 0.27294921875 Test Acc: 0.2548828125\n",
      "Step 153 -- Train loss: 2.5532848834991455, Train Acc: 0.3203125 Test Acc: 0.3076171875\n",
      "Step 154 -- Train loss: 2.533601760864258, Train Acc: 0.35400390625 Test Acc: 0.333984375\n",
      "Step 155 -- Train loss: 2.5180747509002686, Train Acc: 0.384521484375 Test Acc: 0.37109375\n",
      "Step 156 -- Train loss: 2.4914348125457764, Train Acc: 0.441162109375 Test Acc: 0.41796875\n",
      "Step 157 -- Train loss: 2.462583541870117, Train Acc: 0.49951171875 Test Acc: 0.48046875\n",
      "Step 158 -- Train loss: 2.440713405609131, Train Acc: 0.545654296875 Test Acc: 0.51953125\n",
      "Step 159 -- Train loss: 2.4018967151641846, Train Acc: 0.622314453125 Test Acc: 0.60546875\n",
      "Step 160 -- Train loss: 2.366182804107666, Train Acc: 0.688720703125 Test Acc: 0.6640625\n",
      "Step 161 -- Train loss: 2.3307552337646484, Train Acc: 0.73583984375 Test Acc: 0.7451171875\n",
      "Step 162 -- Train loss: 2.282182216644287, Train Acc: 0.78369140625 Test Acc: 0.775390625\n",
      "Step 163 -- Train loss: 2.2337210178375244, Train Acc: 0.844970703125 Test Acc: 0.83984375\n",
      "Step 164 -- Train loss: 2.180091142654419, Train Acc: 0.876708984375 Test Acc: 0.8720703125\n",
      "Step 165 -- Train loss: 2.1217281818389893, Train Acc: 0.9033203125 Test Acc: 0.9013671875\n",
      "Step 166 -- Train loss: 2.0600085258483887, Train Acc: 0.92578125 Test Acc: 0.9072265625\n",
      "Step 167 -- Train loss: 2.0026485919952393, Train Acc: 0.942138671875 Test Acc: 0.9345703125\n",
      "Step 168 -- Train loss: 1.9452240467071533, Train Acc: 0.955810546875 Test Acc: 0.95703125\n",
      "Step 169 -- Train loss: 1.8886650800704956, Train Acc: 0.966552734375 Test Acc: 0.9560546875\n",
      "Step 170 -- Train loss: 1.8419004678726196, Train Acc: 0.972900390625 Test Acc: 0.9736328125\n",
      "Step 171 -- Train loss: 1.7884734869003296, Train Acc: 0.98583984375 Test Acc: 0.9814453125\n",
      "Step 172 -- Train loss: 1.756394386291504, Train Acc: 0.991455078125 Test Acc: 0.9921875\n",
      "Step 173 -- Train loss: 1.7218043804168701, Train Acc: 0.997314453125 Test Acc: 0.99609375\n",
      "Step 174 -- Train loss: 1.688180685043335, Train Acc: 0.9990234375 Test Acc: 0.9990234375\n",
      "Step 175 -- Train loss: 1.6647714376449585, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 176 -- Train loss: 1.631121277809143, Train Acc: 0.999755859375 Test Acc: 1.0\n",
      "Step 177 -- Train loss: 1.6082050800323486, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 178 -- Train loss: 1.5849885940551758, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 179 -- Train loss: 1.5691393613815308, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 180 -- Train loss: 1.549795389175415, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 181 -- Train loss: 1.532484769821167, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 182 -- Train loss: 1.5168936252593994, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 183 -- Train loss: 1.507269024848938, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 184 -- Train loss: 1.498276710510254, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 185 -- Train loss: 1.485605001449585, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 186 -- Train loss: 1.4748989343643188, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 187 -- Train loss: 1.4702361822128296, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 188 -- Train loss: 1.4583444595336914, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 189 -- Train loss: 1.455246925354004, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 190 -- Train loss: 1.450846552848816, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 191 -- Train loss: 1.4440584182739258, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 192 -- Train loss: 1.4375759363174438, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 193 -- Train loss: 1.4321519136428833, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 194 -- Train loss: 1.4278652667999268, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 195 -- Train loss: 1.4276841878890991, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 196 -- Train loss: 1.4201358556747437, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 197 -- Train loss: 1.419974684715271, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 198 -- Train loss: 1.4161862134933472, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 199 -- Train loss: 1.4091540575027466, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 200 -- Train loss: 1.4087601900100708, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 201 -- Train loss: 1.406956672668457, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 202 -- Train loss: 1.4021154642105103, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 203 -- Train loss: 1.402451753616333, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 204 -- Train loss: 1.4012929201126099, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 205 -- Train loss: 1.3987934589385986, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 206 -- Train loss: 1.3976340293884277, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 207 -- Train loss: 1.396911382675171, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 208 -- Train loss: 1.3952683210372925, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 209 -- Train loss: 1.3920245170593262, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 210 -- Train loss: 1.390973448753357, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 211 -- Train loss: 1.389816403388977, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 212 -- Train loss: 1.3901394605636597, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 213 -- Train loss: 1.3866333961486816, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 214 -- Train loss: 1.3856313228607178, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 215 -- Train loss: 1.385227918624878, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 216 -- Train loss: 1.3837957382202148, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 217 -- Train loss: 1.3829060792922974, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 218 -- Train loss: 1.3817110061645508, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 219 -- Train loss: 1.381732702255249, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 220 -- Train loss: 1.3800060749053955, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 221 -- Train loss: 1.3800561428070068, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 222 -- Train loss: 1.379204511642456, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 223 -- Train loss: 1.3782662153244019, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 224 -- Train loss: 1.3774954080581665, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 225 -- Train loss: 1.378246545791626, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 226 -- Train loss: 1.3749816417694092, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 227 -- Train loss: 1.3762779235839844, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 228 -- Train loss: 1.3753092288970947, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 229 -- Train loss: 1.3739888668060303, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 230 -- Train loss: 1.3724637031555176, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 231 -- Train loss: 1.3730063438415527, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 232 -- Train loss: 1.3740566968917847, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 233 -- Train loss: 1.3736506700515747, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 234 -- Train loss: 1.3709502220153809, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 235 -- Train loss: 1.3709465265274048, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 236 -- Train loss: 1.370816707611084, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 237 -- Train loss: 1.3691747188568115, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 238 -- Train loss: 1.3690539598464966, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 239 -- Train loss: 1.3700858354568481, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 240 -- Train loss: 1.3701062202453613, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 241 -- Train loss: 1.3691855669021606, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 242 -- Train loss: 1.3689305782318115, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 243 -- Train loss: 1.3677618503570557, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 244 -- Train loss: 1.367161512374878, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 245 -- Train loss: 1.3671365976333618, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 246 -- Train loss: 1.3683080673217773, Train Acc: 1.0 Test Acc: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x7f66b43b1880>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jyue/.conda/envs/emerge/lib/python3.8/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 247 -- Train loss: 1.3680305480957031, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 248 -- Train loss: 1.3680940866470337, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 249 -- Train loss: 1.3660619258880615, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 250 -- Train loss: 1.3671163320541382, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 251 -- Train loss: 1.3662883043289185, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 252 -- Train loss: 1.3643603324890137, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 253 -- Train loss: 1.3644561767578125, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 254 -- Train loss: 1.365095853805542, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 255 -- Train loss: 1.3660961389541626, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 256 -- Train loss: 1.366362452507019, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 257 -- Train loss: 1.3643907308578491, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 258 -- Train loss: 1.3646924495697021, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 259 -- Train loss: 1.3649851083755493, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 260 -- Train loss: 1.3646376132965088, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 261 -- Train loss: 1.3635696172714233, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 262 -- Train loss: 1.3630529642105103, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 263 -- Train loss: 1.3633252382278442, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 264 -- Train loss: 1.3620169162750244, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 265 -- Train loss: 1.3625260591506958, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 266 -- Train loss: 1.3626829385757446, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 267 -- Train loss: 1.3628449440002441, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 268 -- Train loss: 1.3618477582931519, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 269 -- Train loss: 1.3623487949371338, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 270 -- Train loss: 1.36197030544281, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 271 -- Train loss: 1.3603990077972412, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 272 -- Train loss: 1.3613594770431519, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 273 -- Train loss: 1.3614566326141357, Train Acc: 1.0 Test Acc: 1.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 24\u001b[0m\n\u001b[1;32m     21\u001b[0m     wandb\u001b[38;5;241m.\u001b[39mwatch(model)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(config\u001b[38;5;241m.\u001b[39mtrain\u001b[38;5;241m.\u001b[39mnum_steps):\n\u001b[0;32m---> 24\u001b[0m     \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata_sampler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_sampler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39mtrain\u001b[38;5;241m.\u001b[39mwandb:\n\u001b[1;32m     33\u001b[0m     wandb\u001b[38;5;241m.\u001b[39mfinish()\n",
      "Cell \u001b[0;32mIn[13], line 54\u001b[0m, in \u001b[0;36mtrain_step\u001b[0;34m(model, optim, data_sampler, step, config)\u001b[0m\n\u001b[1;32m     45\u001b[0m train_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[1;32m     46\u001b[0m     idx\u001b[38;5;241m=\u001b[39mtrain_data[:, :prompt_len],\n\u001b[1;32m     47\u001b[0m     max_new_tokens\u001b[38;5;241m=\u001b[39mgen_len,\n\u001b[1;32m     48\u001b[0m )\n\u001b[1;32m     49\u001b[0m test_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[1;32m     50\u001b[0m     idx\u001b[38;5;241m=\u001b[39mtest_data[:, :prompt_len],\n\u001b[1;32m     51\u001b[0m     max_new_tokens\u001b[38;5;241m=\u001b[39mgen_len,\n\u001b[1;32m     52\u001b[0m )\n\u001b[0;32m---> 54\u001b[0m train_acc \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_pred\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43macc_start\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43macc_start\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m test_acc \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean(\n\u001b[1;32m     58\u001b[0m     (test_pred[:, acc_start:] \u001b[38;5;241m==\u001b[39m test_data[:, acc_start:])\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mfloat\u001b[39m)\n\u001b[1;32m     59\u001b[0m )\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     61\u001b[0m data_repeat_frac \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean((test_data[:, acc_start:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m test_data[:, acc_start\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m:])\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mfloat\u001b[39m))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "config = DotMap(config)\n",
    "\n",
    "config.model.vocab_size = max(config.data.p, config.data.max_num) + 1\n",
    "config.model.block_size = 2 * config.data.num_tokens\n",
    "\n",
    "data_sampler = MovingWindowSumNoSep(\n",
    "    min_num=config.data.min_num,\n",
    "    max_num=config.data.max_num,\n",
    "    k=config.data.k,\n",
    "    p=config.data.p,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "model = GPTLinear(config.model, return_att=True).to(device)\n",
    "optim = Adam(model.parameters(), lr=config.train.lr)\n",
    "\n",
    "if config.train.wandb:\n",
    "    wandb_run_name = 'mws_linear_attention_mwsNoSep_500steps'\n",
    "    wandb.login(key=\"\")\n",
    "    wandb.init(project=\"loss_plateau_tf\", name=wandb_run_name, config=config)\n",
    "    wandb.watch(model)\n",
    "\n",
    "for step in range(config.train.num_steps):\n",
    "    train_step(\n",
    "        model=model,\n",
    "        optim=optim,\n",
    "        data_sampler=data_sampler,\n",
    "        step=step,\n",
    "        config=config,\n",
    "    )\n",
    "\n",
    "if config.train.wandb:\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.22.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jyue/private/tf-loss-plateau/wandb/run-20251013_150418-k1ol4kkp</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/wth_ucsd/loss_plateau_tf/runs/k1ol4kkp' target=\"_blank\">mws_softmax_attention_mws_1000steps</a></strong> to <a href='https://wandb.ai/wth_ucsd/loss_plateau_tf' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/wth_ucsd/loss_plateau_tf' target=\"_blank\">https://wandb.ai/wth_ucsd/loss_plateau_tf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/wth_ucsd/loss_plateau_tf/runs/k1ol4kkp' target=\"_blank\">https://wandb.ai/wth_ucsd/loss_plateau_tf/runs/k1ol4kkp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (16) must match the size of tensor b (17) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 25\u001b[0m\n\u001b[1;32m     22\u001b[0m     wandb\u001b[38;5;241m.\u001b[39mwatch(model)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(config\u001b[38;5;241m.\u001b[39mtrain\u001b[38;5;241m.\u001b[39mnum_steps):\n\u001b[0;32m---> 25\u001b[0m     \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata_sampler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_sampler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39mtrain\u001b[38;5;241m.\u001b[39mwandb:\n\u001b[1;32m     34\u001b[0m     wandb\u001b[38;5;241m.\u001b[39mfinish()\n",
      "Cell \u001b[0;32mIn[4], line 55\u001b[0m, in \u001b[0;36mtrain_step\u001b[0;34m(model, optim, data_sampler, step, config)\u001b[0m\n\u001b[1;32m     45\u001b[0m train_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[1;32m     46\u001b[0m     idx\u001b[38;5;241m=\u001b[39mtrain_data[:, :prompt_len],\n\u001b[1;32m     47\u001b[0m     max_new_tokens\u001b[38;5;241m=\u001b[39mgen_len,\n\u001b[1;32m     48\u001b[0m )\n\u001b[1;32m     49\u001b[0m test_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[1;32m     50\u001b[0m     idx\u001b[38;5;241m=\u001b[39mtest_data[:, :prompt_len],\n\u001b[1;32m     51\u001b[0m     max_new_tokens\u001b[38;5;241m=\u001b[39mgen_len,\n\u001b[1;32m     52\u001b[0m )\n\u001b[1;32m     54\u001b[0m train_acc \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean(\n\u001b[0;32m---> 55\u001b[0m     (\u001b[43mtrain_pred\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43macc_start\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43macc_start\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mfloat\u001b[39m)\n\u001b[1;32m     56\u001b[0m )\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     57\u001b[0m test_acc \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean(\n\u001b[1;32m     58\u001b[0m     (test_pred[:, acc_start:] \u001b[38;5;241m==\u001b[39m test_data[:, acc_start:])\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mfloat\u001b[39m)\n\u001b[1;32m     59\u001b[0m )\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     61\u001b[0m data_repeat_frac \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean((test_data[:, acc_start:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m test_data[:, acc_start\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m:])\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mfloat\u001b[39m))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (16) must match the size of tensor b (17) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "config = DotMap(config)\n",
    "\n",
    "config.model.vocab_size = 2 * (max(config.data.p, config.data.max_num) + 1)\n",
    "config.model.block_size = 2 * config.data.num_tokens + 1\n",
    "\n",
    "data_sampler = MovingWindowSumNoMod(\n",
    "    min_num=config.data.min_num,\n",
    "    max_num=config.data.max_num,\n",
    "    k=config.data.k,\n",
    "    p=config.data.p,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "model = GPTSoftmax(config.model, return_att=True).to(device)\n",
    "optim = Adam(model.parameters(), lr=config.train.lr)\n",
    "\n",
    "if config.train.wandb:\n",
    "    wandb_run_name = 'mws_softmax_attention_mwsNoMod_1000steps'\n",
    "    wandb.login(key=\"\")\n",
    "    wandb.init(project=\"loss_plateau_tf\", name=wandb_run_name, config=config)\n",
    "    wandb.watch(model)\n",
    "\n",
    "for step in range(config.train.num_steps):\n",
    "    train_step(\n",
    "        model=model,\n",
    "        optim=optim,\n",
    "        data_sampler=data_sampler,\n",
    "        step=step,\n",
    "        config=config,\n",
    "    )\n",
    "\n",
    "if config.train.wandb:\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>att_prog_measure</td><td>▁▁▁▁▁▁▁▁▁▂▂▂▂▃▄▄▅▆▆▆▆▇▇▇▇▇██████████████</td></tr><tr><td>data_repeat_frac</td><td>▅█▁▅▂▁▁▄▄▃▅▁▂▂▂▄▃▃▅▇▅█▃▅▅▁▅▄▄▄▆▆▃▄▅▅▃▇▅▅</td></tr><tr><td>idx0_check</td><td>▁▁▁▁▂▂▂▂▃▂▃▂▂▂▂▂▁▁▂▁▁▁▂▂▃▃▃▅▇▇██████████</td></tr><tr><td>idx10_check</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂▃▄▄▄▅▅▆▇███████████████</td></tr><tr><td>idx11_check</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂▂▃▄▄▅▅▆▇███████████████</td></tr><tr><td>idx12_check</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂▂▃▃▄▄▅▆▇███████████████</td></tr><tr><td>idx13_check</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂▂▃▃▃▄▅▅▆▇███████████████</td></tr><tr><td>idx14_check</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▃▃▃▄▄▅▆▇███████████████</td></tr><tr><td>idx15_check</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂▃▃▃▄▅▆▇███████████████</td></tr><tr><td>idx1_check</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▁▁▁▂▂▂▂▃▃▃▄▄▅▇▇██████████</td></tr><tr><td>idx2_check</td><td>▁▁▁▁▁▁▁▁▁▁▂▁▁▂▂▂▂▂▃▃▄▄▅▆▇███████████████</td></tr><tr><td>idx3_check</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂▂▂▃▃▄▄▅▆▇███████████████</td></tr><tr><td>idx4_check</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▂▁▂▂▂▃▃▃▄▄▅▆▇▇██████████████</td></tr><tr><td>idx5_check</td><td>▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂▃▂▃▃▄▄▄▅▆▇███████████████</td></tr><tr><td>idx6_check</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▃▃▃▃▄▄▅▆▇███████████████</td></tr><tr><td>idx7_check</td><td>▁▁▁▁▁▁▁▁▁▂▁▁▂▂▂▂▂▃▃▄▄▅▅▆▇███████████████</td></tr><tr><td>idx8_check</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂▃▃▄▅▄▅▆▇███████████████</td></tr><tr><td>idx9_check</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂▃▄▄▄▅▅▆▇███████████████</td></tr><tr><td>mean_cosine_sim</td><td>▅▇▇▇██████▇▇▆▅▅▄▄▄▃▃▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>mean_cosine_sim_0</td><td>▆█▆▄▄▄▄▄▄▃▄▄▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▁▂▂▂▂▂▂▂▂▂</td></tr><tr><td>mean_cosine_sim_1</td><td>▅▇▆▆████▇▇▇▆▅▅▄▄▄▃▃▃▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>mean_cosine_sim_10</td><td>▅▆▇███████▇▇▇▆▅▅▄▄▃▂▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>mean_cosine_sim_11</td><td>▅▆▇██████▇▇▇▇▆▅▅▄▄▃▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>mean_cosine_sim_12</td><td>▄▆▇████████▇▇▇▆▆▅▅▄▃▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>mean_cosine_sim_13</td><td>▅▆▇████████▇▇▇▆▆▅▅▅▄▄▃▃▂▂▁▂▁▁▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>mean_cosine_sim_14</td><td>▅▆▇██████████▇▇▇▆▆▅▅▄▄▃▃▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>mean_cosine_sim_2</td><td>▅▇▇▇██████▇▇▆▅▅▅▄▄▃▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>mean_cosine_sim_3</td><td>▅▇▇██████▇▇▇▆▅▅▄▄▄▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>mean_cosine_sim_4</td><td>▅▇▇██████▇▇▇▆▅▅▄▄▃▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>mean_cosine_sim_5</td><td>▅▇▇▇██████▇▇▆▅▅▄▄▄▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>mean_cosine_sim_6</td><td>▅▆▇██████▇▇▇▆▅▄▄▄▃▃▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>mean_cosine_sim_7</td><td>▅▇▇███████▇▇▆▅▄▄▄▄▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>mean_cosine_sim_8</td><td>▅▆▇███████▇▇▇▆▅▄▄▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>mean_cosine_sim_9</td><td>▅▆▇███████▇▇▇▆▅▅▄▃▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>model_repeat_frac</td><td>▄▅▇▃▇████▇▆▆▄▃▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>test_acc</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂▃▃▃▄▅▆▇▇██████████████</td></tr><tr><td>train_acc</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂▃▃▄▄▅▆▆▇▇█████████████</td></tr><tr><td>train_loss</td><td>██▇▇▇▇▇▆▆▆▆▆▆▆▅▅▅▅▅▄▄▄▄▃▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>att_prog_measure</td><td>0.84503</td></tr><tr><td>data_repeat_frac</td><td>0.06979</td></tr><tr><td>idx0_check</td><td>1.0</td></tr><tr><td>idx10_check</td><td>1.0</td></tr><tr><td>idx11_check</td><td>1.0</td></tr><tr><td>idx12_check</td><td>1.0</td></tr><tr><td>idx13_check</td><td>1.0</td></tr><tr><td>idx14_check</td><td>1.0</td></tr><tr><td>idx15_check</td><td>1.0</td></tr><tr><td>idx1_check</td><td>1.0</td></tr><tr><td>idx2_check</td><td>1.0</td></tr><tr><td>idx3_check</td><td>1.0</td></tr><tr><td>idx4_check</td><td>1.0</td></tr><tr><td>idx5_check</td><td>1.0</td></tr><tr><td>idx6_check</td><td>1.0</td></tr><tr><td>idx7_check</td><td>1.0</td></tr><tr><td>idx8_check</td><td>1.0</td></tr><tr><td>idx9_check</td><td>1.0</td></tr><tr><td>mean_cosine_sim</td><td>0.03101</td></tr><tr><td>mean_cosine_sim_0</td><td>0.00547</td></tr><tr><td>mean_cosine_sim_1</td><td>0.03047</td></tr><tr><td>mean_cosine_sim_10</td><td>0.05142</td></tr><tr><td>mean_cosine_sim_11</td><td>0.03127</td></tr><tr><td>mean_cosine_sim_12</td><td>0.04914</td></tr><tr><td>mean_cosine_sim_13</td><td>0.0584</td></tr><tr><td>mean_cosine_sim_14</td><td>0.09585</td></tr><tr><td>mean_cosine_sim_2</td><td>0.02462</td></tr><tr><td>mean_cosine_sim_3</td><td>0.02766</td></tr><tr><td>mean_cosine_sim_4</td><td>0.03486</td></tr><tr><td>mean_cosine_sim_5</td><td>0.01875</td></tr><tr><td>mean_cosine_sim_6</td><td>0.02934</td></tr><tr><td>mean_cosine_sim_7</td><td>0.025</td></tr><tr><td>mean_cosine_sim_8</td><td>0.03259</td></tr><tr><td>mean_cosine_sim_9</td><td>0.0286</td></tr><tr><td>model_repeat_frac</td><td>0.06979</td></tr><tr><td>test_acc</td><td>1.0</td></tr><tr><td>train_acc</td><td>1.0</td></tr><tr><td>train_loss</td><td>1.32667</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">mws_softmax_attention_mwsNoMod_1000steps</strong> at: <a href='https://wandb.ai/wth_ucsd/loss_plateau_tf/runs/byu5hsmr' target=\"_blank\">https://wandb.ai/wth_ucsd/loss_plateau_tf/runs/byu5hsmr</a><br/>Synced 6 W&B file(s), 303 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251013_002216-byu5hsmr/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
