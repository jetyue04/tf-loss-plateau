{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "343d31a9-529b-444e-a8ae-02753a3804f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import yaml\n",
    "import argparse\n",
    "from dotmap import DotMap\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.optim import Adam\n",
    "from torch.nn.functional import cosine_similarity\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c24b0e8-ad65-4a41-8e7f-558a9778b101",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"./src\")  # make sure Python can find src/\n",
    "# from model_softmax import GPTSoftmax\n",
    "from data import MovingWindowSum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "60774732",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Andrej Karpathy's minGPT implementation (https://github.com/karpathy/minGPT/blob/master/mingpt/model.py)\n",
    "\n",
    "Full definition of a GPT Language Model, all of it in this single file.\n",
    "\n",
    "References:\n",
    "1) the official GPT-2 TensorFlow implementation released by OpenAI:\n",
    "https://github.com/openai/gpt-2/blob/master/src/model.py\n",
    "2) huggingface/transformers PyTorch implementation:\n",
    "https://github.com/huggingface/transformers/blob/main/src/transformers/models/gpt2/modeling_gpt2.py\n",
    "\"\"\"\n",
    "\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# from mingpt.utils import CfgNode as CN\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "class NewGELU(nn.Module):\n",
    "    \"\"\"\n",
    "    Implementation of the GELU activation function currently in Google BERT repo (identical to OpenAI GPT).\n",
    "    Reference: Gaussian Error Linear Units (GELU) paper: https://arxiv.org/abs/1606.08415\n",
    "    \"\"\"\n",
    "\n",
    "    def forward(self, x):\n",
    "        return (\n",
    "            0.5\n",
    "            * x\n",
    "            * (\n",
    "                1.0\n",
    "                + torch.tanh(\n",
    "                    math.sqrt(2.0 / math.pi) * (x + 0.044715 * torch.pow(x, 3.0))\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    A vanilla multi-head masked self-attention layer with a projection at the end.\n",
    "    It is possible to use torch.nn.MultiheadAttention here but I am including an\n",
    "    explicit implementation here to show that there is nothing too scary here.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config, return_att=False):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "\n",
    "        # key, query, value projections for all heads\n",
    "        self.k = nn.Linear(config.n_embd, config.n_embd)\n",
    "        self.q = nn.Linear(config.n_embd, config.n_embd)\n",
    "        self.v = nn.Linear(config.n_embd, config.n_embd)\n",
    "        \n",
    "        # output projection\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
    "\n",
    "        # causal mask to ensure that attention is only applied to the left in the input sequence\n",
    "        self.register_buffer(\n",
    "            \"bias\",\n",
    "            torch.tril(torch.ones(config.block_size, config.block_size)).view(\n",
    "                1, 1, config.block_size, config.block_size\n",
    "            ),\n",
    "        )\n",
    "        \n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        self.return_att = return_att\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = (\n",
    "            x.size()\n",
    "        )  # batch size, sequence length, embedding dimensionality (n_embd)\n",
    "\n",
    "        # calculate query, key, values for all heads\n",
    "        q = self.q(x)\n",
    "        k = self.k(x)\n",
    "        v = self.v(x)\n",
    "\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(\n",
    "            1, 2\n",
    "        )  # (B, nh, T, hs)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(\n",
    "            1, 2\n",
    "        )  # (B, nh, T, hs)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(\n",
    "            1, 2\n",
    "        )  # (B, nh, T, hs)\n",
    "\n",
    "        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
    "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "        att = att.masked_fill(self.bias[:, :, :T, :T] == 0, float('-inf'))\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        \n",
    "        att_copy = att.clone().detach()\n",
    "\n",
    "        y = att @ v  # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "        y = (\n",
    "            y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        )  # re-assemble all head outputs side by side\n",
    "\n",
    "        # output projection\n",
    "        y = self.c_proj(y)\n",
    "\n",
    "        if self.return_att:\n",
    "            return y, att_copy\n",
    "\n",
    "        return y\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\"an unassuming Transformer block\"\"\"\n",
    "\n",
    "    def __init__(self, config, return_att=False):\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
    "        self.attn = CausalSelfAttention(config, return_att=return_att)\n",
    "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
    "        self.mlp = nn.ModuleDict(\n",
    "            dict(\n",
    "                c_fc=nn.Linear(config.n_embd, 4 * config.n_embd),\n",
    "                c_proj=nn.Linear(4 * config.n_embd, config.n_embd),\n",
    "                act=NewGELU(),\n",
    "                # dropout=nn.Dropout(config.resid_pdrop),\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        m = self.mlp\n",
    "        self.mlpf = lambda x: m.c_proj(m.act(m.c_fc(x)))  # MLP forward\n",
    "        self.return_att = return_att\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.return_att:\n",
    "            x_prev, att = self.attn(self.ln_1(x))\n",
    "            x = x + x_prev\n",
    "            \n",
    "            x = x + self.mlpf(self.ln_2(x))\n",
    "            \n",
    "            return x, att\n",
    "\n",
    "        else:\n",
    "            x = x + self.attn(self.ln_1(x))\n",
    "            x = x + self.mlpf(self.ln_2(x))\n",
    "            return x\n",
    "\n",
    "\n",
    "class GPTSoftmax(nn.Module):\n",
    "    \"\"\"GPT Language Model\"\"\"\n",
    "\n",
    "    def __init__(self, config, return_att=False):\n",
    "        super().__init__()\n",
    "        assert config.vocab_size is not None\n",
    "        assert config.block_size is not None\n",
    "        self.block_size = config.block_size\n",
    "        self.return_att = return_att\n",
    "\n",
    "        self.transformer = nn.ModuleDict(\n",
    "            dict(\n",
    "                wte=nn.Embedding(config.vocab_size, config.n_embd),\n",
    "                wpe=nn.Embedding(config.block_size, config.n_embd),\n",
    "                h=nn.ModuleList(\n",
    "                    [\n",
    "                        Block(config, return_att=self.return_att)\n",
    "                        for _ in range(config.n_layer)\n",
    "                    ]\n",
    "                ),\n",
    "                ln_f=nn.LayerNorm(config.n_embd),\n",
    "            )\n",
    "        )\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "\n",
    "        # init all weights, and apply a special scaled init to the residual projections, per GPT-2 paper\n",
    "        self.apply(self._init_weights)\n",
    "        for pn, p in self.named_parameters():\n",
    "            if pn.endswith(\"c_proj.weight\"):\n",
    "                torch.nn.init.normal_(\n",
    "                    p, mean=0.0, std=0.02 / math.sqrt(2 * config.n_layer)\n",
    "                )\n",
    "\n",
    "        # report number of parameters (note we don't count the decoder parameters in lm_head)\n",
    "        n_params = sum(p.numel() for p in self.transformer.parameters())\n",
    "        # print(\"number of parameters: %.2fM\" % (n_params / 1e6,))\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            torch.nn.init.zeros_(module.bias)\n",
    "            torch.nn.init.ones_(module.weight)\n",
    "\n",
    "\n",
    "    # Only used for weight decay experiments -------------------------------------------\n",
    "    def configure_optimizers(self, train_config):\n",
    "        \"\"\"\n",
    "        This long function is unfortunately doing something very simple and is being very defensive:\n",
    "        We are separating out all parameters of the model into two buckets: those that will experience\n",
    "        weight decay for regularization and those that won't (biases, and layernorm/embedding weights).\n",
    "        We are then returning the PyTorch optimizer object.\n",
    "        \"\"\"\n",
    "\n",
    "        # separate out all parameters to those that will and won't experience regularizing weight decay\n",
    "        decay = set()\n",
    "        no_decay = set()\n",
    "        whitelist_weight_modules = (torch.nn.Linear, )\n",
    "        blacklist_weight_modules = (torch.nn.LayerNorm, torch.nn.Embedding)\n",
    "        for mn, m in self.named_modules():\n",
    "            for pn, p in m.named_parameters():\n",
    "                fpn = '%s.%s' % (mn, pn) if mn else pn # full param name\n",
    "                # random note: because named_modules and named_parameters are recursive\n",
    "                # we will see the same tensors p many many times. but doing it this way\n",
    "                # allows us to know which parent module any tensor p belongs to...\n",
    "                if pn.endswith('bias'):\n",
    "                    # all biases will not be decayed\n",
    "                    no_decay.add(fpn)\n",
    "                elif pn.endswith('weight') and isinstance(m, whitelist_weight_modules):\n",
    "                    # weights of whitelist modules will be weight decayed\n",
    "                    decay.add(fpn)\n",
    "                elif pn.endswith('weight') and isinstance(m, blacklist_weight_modules):\n",
    "                    # weights of blacklist modules will NOT be weight decayed\n",
    "                    no_decay.add(fpn)\n",
    "\n",
    "        # validate that we considered every parameter\n",
    "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
    "        inter_params = decay & no_decay\n",
    "        union_params = decay | no_decay\n",
    "        assert len(inter_params) == 0, \"parameters %s made it into both decay/no_decay sets!\" % (str(inter_params), )\n",
    "        assert len(param_dict.keys() - union_params) == 0, \"parameters %s were not separated into either decay/no_decay set!\" \\\n",
    "                                                    % (str(param_dict.keys() - union_params), )\n",
    "\n",
    "        # create the pytorch optimizer object\n",
    "        optim_groups = [\n",
    "            {\"params\": [param_dict[pn] for pn in sorted(list(decay))], \"weight_decay\": train_config.wd},\n",
    "            {\"params\": [param_dict[pn] for pn in sorted(list(no_decay))], \"weight_decay\": 0.0},\n",
    "        ]\n",
    "        optimizer = torch.optim.AdamW(optim_groups, lr=train_config.lr)\n",
    "        return optimizer\n",
    "    # -------------------------------------------\n",
    "\n",
    "\n",
    "    def forward(self, idx, prompt_len, targets=None):\n",
    "        device = idx.device\n",
    "        b, t = idx.size()\n",
    "        assert (\n",
    "            t <= self.block_size\n",
    "        ), f\"Cannot forward sequence of length {t}, block size is only {self.block_size}\"\n",
    "        pos = torch.arange(0, t, dtype=torch.long, device=device).unsqueeze(\n",
    "            0\n",
    "        )  # shape (1, t)\n",
    "\n",
    "        # forward the GPT model itself\n",
    "        tok_emb = self.transformer.wte(idx)  # token embeddings of shape (b, t, n_embd)\n",
    "        pos_emb = self.transformer.wpe(pos)  # position embeddings of shape (1, t, n_embd)\n",
    "        x = tok_emb + pos_emb\n",
    "        \n",
    "        for block in self.transformer.h:\n",
    "            if self.return_att:\n",
    "                x, attn_map = block(x)\n",
    "            else:\n",
    "                x = block(x)\n",
    "\n",
    "        \n",
    "        x = self.transformer.ln_f(x)\n",
    "        \n",
    "        # Track residual state before LM head for representation collapse\n",
    "        pre_lm_h = x.clone().detach()\n",
    "        \n",
    "        # Final logits\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        # if we are given some desired targets also calculate the loss\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            # Masking input tokens\n",
    "            targets_masked = targets.clone()\n",
    "            targets_masked[:, :prompt_len-1] = -1\n",
    "            # targets_masked[:, prompt_len+1:] = -1\n",
    "            loss = F.cross_entropy(\n",
    "                logits.reshape(-1, logits.size(-1)),\n",
    "                targets_masked.reshape(-1),\n",
    "                # targets.reshape(-1),\n",
    "                ignore_index=-1,\n",
    "            )\n",
    "\n",
    "        if self.return_att:\n",
    "            return attn_map, pre_lm_h, logits, loss\n",
    "        \n",
    "        return pre_lm_h, logits, loss\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def generate(\n",
    "        self, idx, max_new_tokens, prompt_len, temperature=1.0, do_sample=False, top_k=None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Take a conditioning sequence of indices idx (LongTensor of shape (b,t)) and complete\n",
    "        the sequence max_new_tokens times, feeding the predictions back into the model each time.\n",
    "        Most likely you'll want to make sure to be in model.eval() mode of operation for this.\n",
    "        \"\"\"\n",
    "        \n",
    "        for _ in range(max_new_tokens):\n",
    "            # if the sequence context is growing too long we must crop it at block_size\n",
    "            idx_cond = (\n",
    "                idx if idx.size(1) <= self.block_size else idx[:, -self.block_size :]\n",
    "            )\n",
    "            # forward the model to get the logits for the index in the sequence\n",
    "            _, _, logits, _ = self(idx_cond, prompt_len=prompt_len)\n",
    "            # pluck the logits at the final step and scale by desired temperature\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            # optionally crop the logits to only the top k options\n",
    "            if top_k is not None:\n",
    "                v, _ = torch.topk(logits, top_k)\n",
    "                logits[logits < v[:, [-1]]] = -float(\"Inf\")\n",
    "            # apply softmax to convert logits to (normalized) probabilities\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            # either sample from the distribution or take the most likely element\n",
    "            if do_sample:\n",
    "                idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            else:\n",
    "                _, idx_next = torch.topk(probs, k=1, dim=-1)\n",
    "            # append sampled index to the running sequence and continue\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "\n",
    "        return idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3bb0acd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "\n",
    "class MovingWindowSum:\n",
    "    def __init__(self, min_num=1, max_num=16, k=2, p=17, sep=17, device=\"cuda\"):\n",
    "        self.min_num = min_num\n",
    "        self.max_num = max_num\n",
    "        self.k = k\n",
    "        self.p = p\n",
    "        self.sep = sep\n",
    "        self.device = device\n",
    "        assert self.p > self.max_num\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def sample(\n",
    "        self,\n",
    "        num_samples,\n",
    "        num_tokens,\n",
    "    ):\n",
    "        random_ints = torch.randint(\n",
    "            low=self.min_num, high=self.max_num + 1, size=(num_samples, num_tokens)\n",
    "        ).to(self.device)\n",
    "\n",
    "        random_ints_np = random_ints.detach().cpu().numpy()\n",
    "        convolution = torch.stack(\n",
    "            [\n",
    "                torch.from_numpy(\n",
    "                    np.convolve(\n",
    "                        random_ints_np[i],\n",
    "                        np.ones(self.k),\n",
    "                        mode=\"valid\",\n",
    "                    )\n",
    "                )\n",
    "                for i in range(random_ints.shape[0])\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        moving_sum = random_ints.clone().detach()\n",
    "        moving_sum[:, self.k - 1 :] = convolution\n",
    "\n",
    "        # for i in range(num_samples):\n",
    "        #     for j in range(0, self.k - 1):\n",
    "        #         if moving_sum[i, j] != random_ints[i, j]:\n",
    "        #             print(f\"ERROR! {i} {j}\")\n",
    "        #     for j in range(self.k - 1, num_tokens):\n",
    "        #         if moving_sum[i, j] != torch.sum(random_ints[i, j-self.k+1:j+1]):\n",
    "        #             print(f\"ERROR! {i} {j}\")\n",
    "\n",
    "        # exit()\n",
    "        samples = (\n",
    "            torch.cat(\n",
    "                [\n",
    "                    random_ints,\n",
    "                    self.sep * torch.ones(size=(num_samples, 1)).to(self.device),\n",
    "                    torch.remainder(input=moving_sum, other=self.p),\n",
    "                ],\n",
    "                axis=-1,\n",
    "            )\n",
    "            .to(int)\n",
    "            .detach()\n",
    "        )\n",
    "\n",
    "        return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5fd65fd6-1b99-46c0-bcf1-8fbe17e2da44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "def train_step(\n",
    "    model,\n",
    "    optim,\n",
    "    data_sampler,\n",
    "    step,\n",
    "    config,\n",
    "):\n",
    "    n_train, n_test, num_tokens = (\n",
    "        config.data.n_train,\n",
    "        config.data.n_test,\n",
    "        config.data.num_tokens,\n",
    "    )\n",
    "\n",
    "    data = data_sampler.sample(\n",
    "        num_samples=n_train + n_test,\n",
    "        num_tokens=num_tokens,\n",
    "    )\n",
    "\n",
    "    train_data = data[:n_train, :]\n",
    "    test_data = data[n_train:, :]\n",
    "\n",
    "    prompt_len = num_tokens + 1\n",
    "    gen_len = num_tokens\n",
    "    acc_start = num_tokens + 1\n",
    "\n",
    "    model.train()\n",
    "    optim.zero_grad(set_to_none=True)\n",
    "\n",
    "    _, _, _, loss = model(\n",
    "        train_data[:, :-1], targets=train_data[:, 1:], prompt_len =prompt_len,\n",
    "    )\n",
    "    loss.backward()\n",
    "\n",
    "    if config.train.grad_clip > 0:\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), config.train.grad_clip)\n",
    "\n",
    "    optim.step()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Log train loss, train / test acc, repetition frequency\n",
    "        attn_map, pre_lm_h, _, train_loss = model(train_data[:, :-1], targets=train_data[:, 1:], prompt_len =prompt_len,)\n",
    "\n",
    "        train_pred = model.generate(\n",
    "            idx=train_data[:, :prompt_len],\n",
    "            max_new_tokens=gen_len,\n",
    "            prompt_len =prompt_len,\n",
    "        )\n",
    "        test_pred = model.generate(\n",
    "            idx=test_data[:, :prompt_len],\n",
    "            max_new_tokens=gen_len,\n",
    "            prompt_len =prompt_len,\n",
    "        )\n",
    "\n",
    "        train_acc = torch.mean(\n",
    "            (train_pred[:, acc_start:] == train_data[:, acc_start:]).to(float)\n",
    "        ).item()\n",
    "        test_acc = torch.mean(\n",
    "            (test_pred[:, acc_start:] == test_data[:, acc_start:]).to(float)\n",
    "        ).item()\n",
    "\n",
    "        data_repeat_frac = torch.mean((test_data[:, acc_start:-1] == test_data[:, acc_start+1:]).to(float))\n",
    "        model_repeat_frac = torch.mean((test_pred[:, acc_start:-1] == test_pred[:, acc_start+1:]).to(float))\n",
    "\n",
    "        # Log attention progress measure\n",
    "        attn_map_output_seq = attn_map[:, :, acc_start-1:]\n",
    "        att_mask = torch.zeros_like(attn_map_output_seq).to(device)\n",
    "\n",
    "        att_mask[:, :, 0, 0] = 1\n",
    "        for i in range(num_tokens - 1):\n",
    "            att_mask[:, :, i + 1, i : i + 2] = 1\n",
    "\n",
    "        att_prog_measure = torch.mean(\n",
    "            torch.sum(torch.abs(attn_map_output_seq) * att_mask, dim=(-3, -2, -1)) /\n",
    "            torch.sum(torch.abs(attn_map_output_seq), dim=(-3, -2, -1)),\n",
    "            dim=0\n",
    "        )\n",
    "\n",
    "        # Log pair-wise cosine similarity between hidden states\n",
    "        embed_start = acc_start - 1\n",
    "        embed_len = gen_len\n",
    "\n",
    "        logit_cs = torch.zeros((embed_len, embed_len))\n",
    "\n",
    "        for i_1 in range(embed_start, embed_start + embed_len):\n",
    "            for i_2 in range(embed_start, i_1):\n",
    "                logit_cs[i_1 - embed_start, i_2 - embed_start] = torch.mean(\n",
    "                    (\n",
    "                        cosine_similarity(\n",
    "                            pre_lm_h[:, i_1, :], pre_lm_h[:, i_2, :], dim=-1\n",
    "                        )\n",
    "                    ), dim=0\n",
    "                )\n",
    "\n",
    "        # Log plots for cosine similarity, attention map\n",
    "        logit_fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(30, 15))\n",
    "\n",
    "        im1 = ax[0].imshow(logit_cs)\n",
    "        ax[0].set_title(\"avg pre_lm_h cosine sim\")\n",
    "        cb1 = logit_fig.colorbar(im1, location=\"right\", shrink=0.99, pad=0.02, ax=ax[0])\n",
    "\n",
    "        avg_attn_map = torch.mean(attn_map, dim=0).squeeze().detach().cpu().numpy()\n",
    "\n",
    "        im2 = ax[1].imshow(avg_attn_map)\n",
    "        ax[1].set_title(\"att map\")\n",
    "        cb4 = logit_fig.colorbar(im2, location=\"right\", shrink=0.99, pad=0.02, ax=ax[1])\n",
    "        ax[1].set_xticks(range(avg_attn_map.shape[-1]))\n",
    "        ax[1].set_yticks(range(avg_attn_map.shape[-2]))\n",
    "\n",
    "        for i1 in range(embed_len):\n",
    "            for i2 in range(embed_len):\n",
    "                text1 = ax[0].text(\n",
    "                    i2,\n",
    "                    i1,\n",
    "                    round(logit_cs[i1, i2].item(), 2),\n",
    "                    ha=\"center\",\n",
    "                    va=\"center\",\n",
    "                    color=\"w\",\n",
    "                )\n",
    "\n",
    "\n",
    "        print(\n",
    "            f\"Step {step} -- Train loss: {train_loss}, Train Acc: {train_acc} Test Acc: {test_acc}\"\n",
    "        )\n",
    "        # print(f\"input: {test_data[0]} \\n predicted:{test_pred[0]}\")\n",
    "\n",
    "        if config.train.wandb:\n",
    "\n",
    "            log_data = {\n",
    "                \"train_loss\": train_loss,\n",
    "                \"train_acc\": train_acc,\n",
    "                \"test_acc\": test_acc,\n",
    "                \"data_repeat_frac\": data_repeat_frac,\n",
    "                \"model_repeat_frac\": model_repeat_frac,\n",
    "                \"att_prog_measure\": att_prog_measure,\n",
    "                \"pre_lm_h_cosine_sim\": logit_fig,\n",
    "                \"mean_cosine_sim\": torch.sum(logit_cs[:, 1:]) / (0.5 * (gen_len-1) * (gen_len-2))\n",
    "            }\n",
    "\n",
    "            for output_pos in range(gen_len):\n",
    "                log_data.update(\n",
    "                    {\n",
    "                        f\"idx{output_pos}_check\": torch.mean(\n",
    "                            (train_pred[:, acc_start + output_pos] == train_data[:, acc_start + output_pos]).to(float)\n",
    "                        ).item()\n",
    "                    }\n",
    "                )\n",
    "\n",
    "                if output_pos < gen_len-1:\n",
    "                    log_data.update(\n",
    "                        {\n",
    "                            f\"mean_cosine_sim_{output_pos}\": torch.sum(logit_cs[:, output_pos]) / (gen_len-1-output_pos)\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "            wandb.log(log_data)\n",
    "\n",
    "        plt.close()\n",
    "        del (\n",
    "            logit_fig,\n",
    "            ax,\n",
    "            logit_cs,\n",
    "        )\n",
    "\n",
    "        if config.train.save_ckpt:\n",
    "            if (step == 0) or ((step + 1) % config.train.ckpt_freq == 0):\n",
    "                model.train()\n",
    "                torch.save(\n",
    "                    {\n",
    "                        \"epoch\": step,\n",
    "                        \"model\": model.state_dict(),\n",
    "                        \"optim\": optim.state_dict(),\n",
    "                        \"train_loss\": train_loss,\n",
    "                        \"test_acc\": test_acc,\n",
    "                    },\n",
    "                    \"./mws_k2_l1_h1_a16_n16.tar\",\n",
    "                )\n",
    "                print(f\"saved state at epoch {step} to {f'./mws_k2_l1_h1_a16_n16.tar'}\")\n",
    "\n",
    "                if config.train.wandb:\n",
    "                    model_wandb = wandb.Artifact(\n",
    "                        f\"model_step{step}\", type=\"model\"\n",
    "                    )\n",
    "                    model_wandb.add_file(f\"./mws_k2_l1_h1_a16_n16.tar\")\n",
    "                    wandb.log_artifact(model_wandb)\n",
    "                    print(\"model uploaded to wandb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "697f626b-eb89-47e1-8c0f-3b8d247b8017",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "config = {\n",
    "'model':\n",
    "  {\n",
    "    'n_layer': 1,\n",
    "    'n_head': 1,\n",
    "    'n_embd': 256,\n",
    "    'linear': True,\n",
    "  },\n",
    "\n",
    "'data':\n",
    "  {\n",
    "    'name': 'window',\n",
    "    'min_num': 1,\n",
    "    'max_num': 16,\n",
    "    'k': 2,\n",
    "    'p': 17,\n",
    "    'sep': 17,\n",
    "    'cot': False,\n",
    "    'num_tokens': 16,\n",
    "    'n_train': 256,\n",
    "    'n_test': 64,\n",
    "    'fixed_len': True,\n",
    "  },\n",
    "\n",
    "'train':\n",
    "  {\n",
    "    'lr': 0.0005,\n",
    "    'grad_clip': -1,\n",
    "    'num_steps': 2000,\n",
    "    'norm_type': \"none_rank\",\n",
    "    'wandb': True,\n",
    "    'save_ckpt': False,\n",
    "    'ckpt_freq': 20,\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f45111d2-5e03-4980-8621-5a78b871c468",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:l6k1mm1v) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>att_prog_measure</td><td>▁▁▂▂▄▅▇████████████▇▇▇▇</td></tr><tr><td>data_repeat_frac</td><td>▄▆▅▅▆▇▅▃▁▄▄▄▅▃▄▄▅▅▅▅▄▄█</td></tr><tr><td>idx0_check</td><td>▁▁▁▂▅██████████████████</td></tr><tr><td>idx10_check</td><td>▃▃▅▁▆▇▅▃▅▅▇▁▃█▄▃▅▃▇▃▅▅▁</td></tr><tr><td>idx11_check</td><td>▅█▇█▃▃▅▅▃▅▄▃▅▂▅▆▄▁▂▃█▅▃</td></tr><tr><td>idx12_check</td><td>▁▅▄▅▆▆▅▆▆▆▄▃▅▁▅▃▃█▅▆▅▄▃</td></tr><tr><td>idx13_check</td><td>▆▅▅▇▃█▅▃▆▂▃▅▂▇▅▅▁█▅▅▂▄▇</td></tr><tr><td>idx14_check</td><td>▆▅▅▃▆▅▅▂▃▁▃█▂▂▄▂▁▁▅▂▃▄▃</td></tr><tr><td>idx15_check</td><td>▆▇▅▆▆██▃▇▁▅▇▄▇▅▅▇▇█▅▂▄▁</td></tr><tr><td>idx1_check</td><td>▅▅█▅▅▃▅▆▁▄▅▃▃▅▅▂▃▃▃▅▃▄▄</td></tr><tr><td>idx2_check</td><td>▆▃▇▅█▂▄▆▂▇▃█▂▁▄▇▆▆█▂▂▃▁</td></tr><tr><td>idx3_check</td><td>██▅▄▅▄▄▄▅▃▃▁▅▁▄▄▅▅▄▃▄▆▅</td></tr><tr><td>idx4_check</td><td>▅▄▃█▄▄▂▅▃▅▂▄▂▄▁▄▄▂▃▅▂▄▃</td></tr><tr><td>idx5_check</td><td>▇▆▄▇▆▅▅▄▇▃▆▅▇▅▂▄▄▅▁▂▅█▁</td></tr><tr><td>idx6_check</td><td>█▅▆▆▅▅▄▃▆▄▃▄▁▂▅▄▆▄▄▃▄▃▄</td></tr><tr><td>idx7_check</td><td>▇▄▆▇▄▂█▃▅▄▄▄▃▅▁▅▃▁▆▆▇▃▁</td></tr><tr><td>idx8_check</td><td>█▇▅▃▄▅▅▃▄▄▃▄▃▂▁▂▂▆▃▃▃▂▄</td></tr><tr><td>idx9_check</td><td>▆▄▃▇▄▃▇▇▃▄▃▃█▅▁▂▃██▇▄▅▃</td></tr><tr><td>mean_cosine_sim</td><td>▁▃▅▆▆▆▅▅▅▅▆▆▆▆▇▇▇██████</td></tr><tr><td>mean_cosine_sim_0</td><td>▆▇███▆▂▁▃▄▄▄▃▃▂▂▂▂▂▃▃▃▃</td></tr><tr><td>mean_cosine_sim_1</td><td>▁▃▅▆▆▆▅▅▅▅▆▆▆▆▇▇▇██████</td></tr><tr><td>mean_cosine_sim_10</td><td>▁▃▅▆▆▆▅▅▅▅▆▆▆▆▇▇▇▇█████</td></tr><tr><td>mean_cosine_sim_11</td><td>▁▃▅▅▆▅▅▅▅▅▆▆▆▆▇▇▇██████</td></tr><tr><td>mean_cosine_sim_12</td><td>▁▃▄▅▆▅▅▅▅▅▆▆▆▆▇▇▇██████</td></tr><tr><td>mean_cosine_sim_13</td><td>▁▃▄▅▆▅▅▅▅▅▅▅▆▆▇▇▇▇█████</td></tr><tr><td>mean_cosine_sim_14</td><td>▁▃▄▅▆▅▅▅▅▅▆▆▆▆▇▇▇██████</td></tr><tr><td>mean_cosine_sim_2</td><td>▁▃▅▆▆▆▅▅▅▅▆▆▆▆▇▇▇██████</td></tr><tr><td>mean_cosine_sim_3</td><td>▁▃▅▆▆▆▅▅▅▅▆▆▆▆▇▇▇▇█████</td></tr><tr><td>mean_cosine_sim_4</td><td>▁▃▅▆▆▅▅▅▅▅▅▆▆▆▇▇▇▇█████</td></tr><tr><td>mean_cosine_sim_5</td><td>▁▃▅▆▆▆▅▅▅▅▆▆▆▆▇▇▇▇█████</td></tr><tr><td>mean_cosine_sim_6</td><td>▁▃▅▆▆▆▅▅▅▅▆▆▆▆▇▇▇▇█████</td></tr><tr><td>mean_cosine_sim_7</td><td>▁▃▅▆▆▆▅▅▅▅▆▆▆▇▇▇▇██████</td></tr><tr><td>mean_cosine_sim_8</td><td>▁▃▅▆▆▆▅▅▅▅▆▆▆▇▇▇▇██████</td></tr><tr><td>mean_cosine_sim_9</td><td>▁▃▅▅▆▅▅▅▅▅▅▆▆▆▇▇▇▇█████</td></tr><tr><td>model_repeat_frac</td><td>▁▇▃▄▆▅▆▇▇▇▇▇█▇▇▆▆▇▆▆▆▆▆</td></tr><tr><td>test_acc</td><td>▂▁▁▃▅▇▇█▇▆▇█▇█▆▆▆▆▇▇▆█▆</td></tr><tr><td>train_acc</td><td>▁▁▁▃▅██▇▇▇▇▇▇▇▆▇▇██▇▇▇▆</td></tr><tr><td>train_loss</td><td>███▇▇▅▃▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>att_prog_measure</td><td>0.12915</td></tr><tr><td>data_repeat_frac</td><td>0.07292</td></tr><tr><td>idx0_check</td><td>1.0</td></tr><tr><td>idx10_check</td><td>0.03125</td></tr><tr><td>idx11_check</td><td>0.03516</td></tr><tr><td>idx12_check</td><td>0.03906</td></tr><tr><td>idx13_check</td><td>0.07031</td></tr><tr><td>idx14_check</td><td>0.04688</td></tr><tr><td>idx15_check</td><td>0.02734</td></tr><tr><td>idx1_check</td><td>0.05469</td></tr><tr><td>idx2_check</td><td>0.03125</td></tr><tr><td>idx3_check</td><td>0.0625</td></tr><tr><td>idx4_check</td><td>0.04688</td></tr><tr><td>idx5_check</td><td>0.03125</td></tr><tr><td>idx6_check</td><td>0.04688</td></tr><tr><td>idx7_check</td><td>0.02734</td></tr><tr><td>idx8_check</td><td>0.04688</td></tr><tr><td>idx9_check</td><td>0.03516</td></tr><tr><td>mean_cosine_sim</td><td>0.96995</td></tr><tr><td>mean_cosine_sim_0</td><td>0.16289</td></tr><tr><td>mean_cosine_sim_1</td><td>0.9713</td></tr><tr><td>mean_cosine_sim_10</td><td>0.95733</td></tr><tr><td>mean_cosine_sim_11</td><td>0.97881</td></tr><tr><td>mean_cosine_sim_12</td><td>0.97809</td></tr><tr><td>mean_cosine_sim_13</td><td>0.97453</td></tr><tr><td>mean_cosine_sim_14</td><td>0.97938</td></tr><tr><td>mean_cosine_sim_2</td><td>0.97456</td></tr><tr><td>mean_cosine_sim_3</td><td>0.97204</td></tr><tr><td>mean_cosine_sim_4</td><td>0.96982</td></tr><tr><td>mean_cosine_sim_5</td><td>0.96748</td></tr><tr><td>mean_cosine_sim_6</td><td>0.95268</td></tr><tr><td>mean_cosine_sim_7</td><td>0.97668</td></tr><tr><td>mean_cosine_sim_8</td><td>0.97512</td></tr><tr><td>mean_cosine_sim_9</td><td>0.96538</td></tr><tr><td>model_repeat_frac</td><td>0.50521</td></tr><tr><td>test_acc</td><td>0.10059</td></tr><tr><td>train_acc</td><td>0.10205</td></tr><tr><td>train_loss</td><td>1.46437</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">mws_softmax_attention_loss_noinput_lr0.0005_2kstep</strong> at: <a href='https://wandb.ai/wth_ucsd/loss_plateau_tf/runs/l6k1mm1v' target=\"_blank\">https://wandb.ai/wth_ucsd/loss_plateau_tf/runs/l6k1mm1v</a><br/>Synced 6 W&B file(s), 23 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251021_211742-l6k1mm1v/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:l6k1mm1v). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.22.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jyue/private/tf-loss-plateau/wandb/run-20251021_211828-9bw3w9au</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/wth_ucsd/loss_plateau_tf/runs/9bw3w9au' target=\"_blank\">mws_softmax_attention_loss_noinput_lr0.0005_2kstep</a></strong> to <a href='https://wandb.ai/wth_ucsd/loss_plateau_tf' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/wth_ucsd/loss_plateau_tf' target=\"_blank\">https://wandb.ai/wth_ucsd/loss_plateau_tf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/wth_ucsd/loss_plateau_tf/runs/9bw3w9au' target=\"_blank\">https://wandb.ai/wth_ucsd/loss_plateau_tf/runs/9bw3w9au</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0 -- Train loss: 2.9288675785064697, Train Acc: 0.0703125 Test Acc: 0.0546875\n",
      "Step 1 -- Train loss: 2.8536767959594727, Train Acc: 0.062744140625 Test Acc: 0.0546875\n",
      "Step 2 -- Train loss: 2.853163719177246, Train Acc: 0.0693359375 Test Acc: 0.056640625\n",
      "Step 3 -- Train loss: 2.862241506576538, Train Acc: 0.068603515625 Test Acc: 0.05859375\n",
      "Step 4 -- Train loss: 2.846468448638916, Train Acc: 0.0634765625 Test Acc: 0.0625\n",
      "Step 5 -- Train loss: 2.8394010066986084, Train Acc: 0.06689453125 Test Acc: 0.0693359375\n",
      "Step 6 -- Train loss: 2.8378608226776123, Train Acc: 0.059814453125 Test Acc: 0.056640625\n",
      "Step 7 -- Train loss: 2.8409571647644043, Train Acc: 0.06103515625 Test Acc: 0.05859375\n",
      "Step 8 -- Train loss: 2.834721565246582, Train Acc: 0.066650390625 Test Acc: 0.060546875\n",
      "Step 9 -- Train loss: 2.8422861099243164, Train Acc: 0.062744140625 Test Acc: 0.0654296875\n",
      "Step 10 -- Train loss: 2.8411641120910645, Train Acc: 0.052978515625 Test Acc: 0.0439453125\n",
      "Step 11 -- Train loss: 2.8357093334198, Train Acc: 0.062744140625 Test Acc: 0.056640625\n",
      "Step 12 -- Train loss: 2.8351759910583496, Train Acc: 0.05859375 Test Acc: 0.0595703125\n",
      "Step 13 -- Train loss: 2.8343923091888428, Train Acc: 0.06689453125 Test Acc: 0.0673828125\n",
      "Step 14 -- Train loss: 2.836519241333008, Train Acc: 0.064208984375 Test Acc: 0.0546875\n",
      "Step 15 -- Train loss: 2.8386573791503906, Train Acc: 0.062255859375 Test Acc: 0.052734375\n",
      "Step 16 -- Train loss: 2.836667060852051, Train Acc: 0.0625 Test Acc: 0.0498046875\n",
      "Step 17 -- Train loss: 2.8369956016540527, Train Acc: 0.064453125 Test Acc: 0.05078125\n",
      "Step 18 -- Train loss: 2.8360278606414795, Train Acc: 0.064453125 Test Acc: 0.0556640625\n",
      "Step 19 -- Train loss: 2.83384370803833, Train Acc: 0.06298828125 Test Acc: 0.060546875\n",
      "Step 20 -- Train loss: 2.8352901935577393, Train Acc: 0.065185546875 Test Acc: 0.0595703125\n",
      "Step 21 -- Train loss: 2.8318450450897217, Train Acc: 0.062255859375 Test Acc: 0.0537109375\n",
      "Step 22 -- Train loss: 2.834639072418213, Train Acc: 0.05810546875 Test Acc: 0.052734375\n",
      "Step 23 -- Train loss: 2.8323795795440674, Train Acc: 0.062744140625 Test Acc: 0.048828125\n",
      "Step 24 -- Train loss: 2.8378753662109375, Train Acc: 0.057373046875 Test Acc: 0.0537109375\n",
      "Step 25 -- Train loss: 2.8371903896331787, Train Acc: 0.0703125 Test Acc: 0.0546875\n",
      "Step 26 -- Train loss: 2.8351235389709473, Train Acc: 0.057861328125 Test Acc: 0.0712890625\n",
      "Step 27 -- Train loss: 2.8323800563812256, Train Acc: 0.056884765625 Test Acc: 0.0595703125\n",
      "Step 28 -- Train loss: 2.8334293365478516, Train Acc: 0.070556640625 Test Acc: 0.0634765625\n",
      "Step 29 -- Train loss: 2.8322091102600098, Train Acc: 0.059326171875 Test Acc: 0.0595703125\n",
      "Step 30 -- Train loss: 2.829936981201172, Train Acc: 0.0634765625 Test Acc: 0.0654296875\n",
      "Step 31 -- Train loss: 2.833026170730591, Train Acc: 0.05322265625 Test Acc: 0.0556640625\n",
      "Step 32 -- Train loss: 2.8319363594055176, Train Acc: 0.064697265625 Test Acc: 0.0654296875\n",
      "Step 33 -- Train loss: 2.829143524169922, Train Acc: 0.06494140625 Test Acc: 0.0673828125\n",
      "Step 34 -- Train loss: 2.83140230178833, Train Acc: 0.05859375 Test Acc: 0.0537109375\n",
      "Step 35 -- Train loss: 2.832171678543091, Train Acc: 0.062744140625 Test Acc: 0.046875\n",
      "Step 36 -- Train loss: 2.8278799057006836, Train Acc: 0.066650390625 Test Acc: 0.0546875\n",
      "Step 37 -- Train loss: 2.8323419094085693, Train Acc: 0.06103515625 Test Acc: 0.0615234375\n",
      "Step 38 -- Train loss: 2.8300204277038574, Train Acc: 0.06103515625 Test Acc: 0.0478515625\n",
      "Step 39 -- Train loss: 2.830540895462036, Train Acc: 0.06494140625 Test Acc: 0.0634765625\n",
      "Step 40 -- Train loss: 2.829022169113159, Train Acc: 0.062744140625 Test Acc: 0.0615234375\n",
      "Step 41 -- Train loss: 2.8303983211517334, Train Acc: 0.062255859375 Test Acc: 0.0546875\n",
      "Step 42 -- Train loss: 2.8301305770874023, Train Acc: 0.0595703125 Test Acc: 0.0673828125\n",
      "Step 43 -- Train loss: 2.8302485942840576, Train Acc: 0.064208984375 Test Acc: 0.0576171875\n",
      "Step 44 -- Train loss: 2.8305535316467285, Train Acc: 0.0625 Test Acc: 0.05859375\n",
      "Step 45 -- Train loss: 2.8307623863220215, Train Acc: 0.065673828125 Test Acc: 0.0498046875\n",
      "Step 46 -- Train loss: 2.8302974700927734, Train Acc: 0.063232421875 Test Acc: 0.076171875\n",
      "Step 47 -- Train loss: 2.829888343811035, Train Acc: 0.065185546875 Test Acc: 0.068359375\n",
      "Step 48 -- Train loss: 2.8279309272766113, Train Acc: 0.06298828125 Test Acc: 0.0546875\n",
      "Step 49 -- Train loss: 2.828364849090576, Train Acc: 0.061279296875 Test Acc: 0.0771484375\n",
      "Step 50 -- Train loss: 2.8280324935913086, Train Acc: 0.06982421875 Test Acc: 0.06640625\n",
      "Step 51 -- Train loss: 2.8284592628479004, Train Acc: 0.070068359375 Test Acc: 0.0537109375\n",
      "Step 52 -- Train loss: 2.827205181121826, Train Acc: 0.05908203125 Test Acc: 0.0654296875\n",
      "Step 53 -- Train loss: 2.8297882080078125, Train Acc: 0.061767578125 Test Acc: 0.0654296875\n",
      "Step 54 -- Train loss: 2.8274292945861816, Train Acc: 0.0654296875 Test Acc: 0.0712890625\n",
      "Step 55 -- Train loss: 2.829023599624634, Train Acc: 0.06494140625 Test Acc: 0.0595703125\n",
      "Step 56 -- Train loss: 2.8263309001922607, Train Acc: 0.070068359375 Test Acc: 0.0478515625\n",
      "Step 57 -- Train loss: 2.825514316558838, Train Acc: 0.070068359375 Test Acc: 0.0517578125\n",
      "Step 58 -- Train loss: 2.825357675552368, Train Acc: 0.07763671875 Test Acc: 0.0634765625\n",
      "Step 59 -- Train loss: 2.8268256187438965, Train Acc: 0.071044921875 Test Acc: 0.06640625\n",
      "Step 60 -- Train loss: 2.8246636390686035, Train Acc: 0.064453125 Test Acc: 0.064453125\n",
      "Step 61 -- Train loss: 2.823204755783081, Train Acc: 0.064697265625 Test Acc: 0.0517578125\n",
      "Step 62 -- Train loss: 2.823474168777466, Train Acc: 0.060546875 Test Acc: 0.0595703125\n",
      "Step 63 -- Train loss: 2.8220131397247314, Train Acc: 0.067138671875 Test Acc: 0.0712890625\n",
      "Step 64 -- Train loss: 2.817701816558838, Train Acc: 0.068603515625 Test Acc: 0.068359375\n",
      "Step 65 -- Train loss: 2.81787109375, Train Acc: 0.058837890625 Test Acc: 0.0595703125\n",
      "Step 66 -- Train loss: 2.8105950355529785, Train Acc: 0.07177734375 Test Acc: 0.0751953125\n",
      "Step 67 -- Train loss: 2.8095693588256836, Train Acc: 0.072509765625 Test Acc: 0.06640625\n",
      "Step 68 -- Train loss: 2.8044040203094482, Train Acc: 0.072265625 Test Acc: 0.0703125\n",
      "Step 69 -- Train loss: 2.801156997680664, Train Acc: 0.0703125 Test Acc: 0.0830078125\n",
      "Step 70 -- Train loss: 2.789379835128784, Train Acc: 0.070556640625 Test Acc: 0.06640625\n",
      "Step 71 -- Train loss: 2.7849934101104736, Train Acc: 0.075439453125 Test Acc: 0.072265625\n",
      "Step 72 -- Train loss: 2.7741496562957764, Train Acc: 0.077880859375 Test Acc: 0.068359375\n",
      "Step 73 -- Train loss: 2.7593448162078857, Train Acc: 0.085693359375 Test Acc: 0.0810546875\n",
      "Step 74 -- Train loss: 2.7374086380004883, Train Acc: 0.091552734375 Test Acc: 0.0986328125\n",
      "Step 75 -- Train loss: 2.705643892288208, Train Acc: 0.10791015625 Test Acc: 0.0869140625\n",
      "Step 76 -- Train loss: 2.7021360397338867, Train Acc: 0.10498046875 Test Acc: 0.0869140625\n",
      "Step 77 -- Train loss: 2.6486759185791016, Train Acc: 0.124267578125 Test Acc: 0.09765625\n",
      "Step 78 -- Train loss: 2.620743751525879, Train Acc: 0.14208984375 Test Acc: 0.109375\n",
      "Step 79 -- Train loss: 2.597062826156616, Train Acc: 0.1494140625 Test Acc: 0.158203125\n",
      "Step 80 -- Train loss: 2.5564327239990234, Train Acc: 0.165771484375 Test Acc: 0.1591796875\n",
      "Step 81 -- Train loss: 2.4711451530456543, Train Acc: 0.21533203125 Test Acc: 0.216796875\n",
      "Step 82 -- Train loss: 2.419358015060425, Train Acc: 0.247314453125 Test Acc: 0.2294921875\n",
      "Step 83 -- Train loss: 2.33834171295166, Train Acc: 0.3056640625 Test Acc: 0.2841796875\n",
      "Step 84 -- Train loss: 2.2283613681793213, Train Acc: 0.353515625 Test Acc: 0.3232421875\n",
      "Step 85 -- Train loss: 2.1292035579681396, Train Acc: 0.386474609375 Test Acc: 0.359375\n",
      "Step 86 -- Train loss: 2.0211503505706787, Train Acc: 0.447998046875 Test Acc: 0.4072265625\n",
      "Step 87 -- Train loss: 1.9256306886672974, Train Acc: 0.477294921875 Test Acc: 0.4853515625\n",
      "Step 88 -- Train loss: 1.7817453145980835, Train Acc: 0.531982421875 Test Acc: 0.525390625\n",
      "Step 89 -- Train loss: 1.6786211729049683, Train Acc: 0.560546875 Test Acc: 0.568359375\n",
      "Step 90 -- Train loss: 1.56355881690979, Train Acc: 0.595703125 Test Acc: 0.580078125\n",
      "Step 91 -- Train loss: 1.4205046892166138, Train Acc: 0.644287109375 Test Acc: 0.6318359375\n",
      "Step 92 -- Train loss: 1.2756623029708862, Train Acc: 0.66943359375 Test Acc: 0.6591796875\n",
      "Step 93 -- Train loss: 1.160691261291504, Train Acc: 0.689697265625 Test Acc: 0.6796875\n",
      "Step 94 -- Train loss: 1.032787799835205, Train Acc: 0.707275390625 Test Acc: 0.7021484375\n",
      "Step 95 -- Train loss: 0.9441909790039062, Train Acc: 0.762451171875 Test Acc: 0.7626953125\n",
      "Step 96 -- Train loss: 0.844618022441864, Train Acc: 0.85498046875 Test Acc: 0.83203125\n",
      "Step 97 -- Train loss: 0.7721575498580933, Train Acc: 0.906494140625 Test Acc: 0.896484375\n",
      "Step 98 -- Train loss: 0.688905656337738, Train Acc: 0.923583984375 Test Acc: 0.9267578125\n",
      "Step 99 -- Train loss: 0.6287301778793335, Train Acc: 0.939697265625 Test Acc: 0.9296875\n",
      "Step 100 -- Train loss: 0.5652381777763367, Train Acc: 0.94140625 Test Acc: 0.94140625\n",
      "Step 101 -- Train loss: 0.5185316205024719, Train Acc: 0.937744140625 Test Acc: 0.9404296875\n",
      "Step 102 -- Train loss: 0.47386592626571655, Train Acc: 0.94091796875 Test Acc: 0.939453125\n",
      "Step 103 -- Train loss: 0.429776132106781, Train Acc: 0.940673828125 Test Acc: 0.943359375\n",
      "Step 104 -- Train loss: 0.39465728402137756, Train Acc: 0.9423828125 Test Acc: 0.943359375\n",
      "Step 105 -- Train loss: 0.3678244650363922, Train Acc: 0.94189453125 Test Acc: 0.9404296875\n",
      "Step 106 -- Train loss: 0.33517077565193176, Train Acc: 0.94189453125 Test Acc: 0.9384765625\n",
      "Step 107 -- Train loss: 0.3147220313549042, Train Acc: 0.942626953125 Test Acc: 0.94140625\n",
      "Step 108 -- Train loss: 0.29646825790405273, Train Acc: 0.9423828125 Test Acc: 0.9404296875\n",
      "Step 109 -- Train loss: 0.2819623649120331, Train Acc: 0.941162109375 Test Acc: 0.94140625\n",
      "Step 110 -- Train loss: 0.2676924169063568, Train Acc: 0.9423828125 Test Acc: 0.9443359375\n",
      "Step 111 -- Train loss: 0.2570961117744446, Train Acc: 0.94140625 Test Acc: 0.94140625\n",
      "Step 112 -- Train loss: 0.251592755317688, Train Acc: 0.944091796875 Test Acc: 0.9423828125\n",
      "Step 113 -- Train loss: 0.24084703624248505, Train Acc: 0.940673828125 Test Acc: 0.939453125\n",
      "Step 114 -- Train loss: 0.23217061161994934, Train Acc: 0.942626953125 Test Acc: 0.939453125\n",
      "Step 115 -- Train loss: 0.22531090676784515, Train Acc: 0.94189453125 Test Acc: 0.943359375\n",
      "Step 116 -- Train loss: 0.2208232283592224, Train Acc: 0.94189453125 Test Acc: 0.943359375\n",
      "Step 117 -- Train loss: 0.21487589180469513, Train Acc: 0.94140625 Test Acc: 0.9443359375\n",
      "Step 118 -- Train loss: 0.21082760393619537, Train Acc: 0.942626953125 Test Acc: 0.939453125\n",
      "Step 119 -- Train loss: 0.20793697237968445, Train Acc: 0.941162109375 Test Acc: 0.9453125\n",
      "Step 120 -- Train loss: 0.20467375218868256, Train Acc: 0.9423828125 Test Acc: 0.9423828125\n",
      "Step 121 -- Train loss: 0.2031637281179428, Train Acc: 0.941162109375 Test Acc: 0.9453125\n",
      "Step 122 -- Train loss: 0.1992214322090149, Train Acc: 0.940185546875 Test Acc: 0.9462890625\n",
      "Step 123 -- Train loss: 0.19718557596206665, Train Acc: 0.94140625 Test Acc: 0.94140625\n",
      "Step 124 -- Train loss: 0.1970830112695694, Train Acc: 0.9423828125 Test Acc: 0.94140625\n",
      "Step 125 -- Train loss: 0.1944587677717209, Train Acc: 0.9423828125 Test Acc: 0.943359375\n",
      "Step 126 -- Train loss: 0.19294360280036926, Train Acc: 0.94140625 Test Acc: 0.94140625\n",
      "Step 127 -- Train loss: 0.19092604517936707, Train Acc: 0.942138671875 Test Acc: 0.94140625\n",
      "Step 128 -- Train loss: 0.19249658286571503, Train Acc: 0.941162109375 Test Acc: 0.94140625\n",
      "Step 129 -- Train loss: 0.18930263817310333, Train Acc: 0.942626953125 Test Acc: 0.94140625\n",
      "Step 130 -- Train loss: 0.18921591341495514, Train Acc: 0.940673828125 Test Acc: 0.94140625\n",
      "Step 131 -- Train loss: 0.18727990984916687, Train Acc: 0.941162109375 Test Acc: 0.943359375\n",
      "Step 132 -- Train loss: 0.18642254173755646, Train Acc: 0.94287109375 Test Acc: 0.94140625\n",
      "Step 133 -- Train loss: 0.18753989040851593, Train Acc: 0.94091796875 Test Acc: 0.9404296875\n",
      "Step 134 -- Train loss: 0.1876211315393448, Train Acc: 0.940673828125 Test Acc: 0.9404296875\n",
      "Step 135 -- Train loss: 0.18589487671852112, Train Acc: 0.940673828125 Test Acc: 0.9384765625\n",
      "Step 136 -- Train loss: 0.18417327105998993, Train Acc: 0.94189453125 Test Acc: 0.939453125\n",
      "Step 137 -- Train loss: 0.18252171576023102, Train Acc: 0.94287109375 Test Acc: 0.9404296875\n",
      "Step 138 -- Train loss: 0.18410281836986542, Train Acc: 0.942138671875 Test Acc: 0.9384765625\n",
      "Step 139 -- Train loss: 0.1833130270242691, Train Acc: 0.9404296875 Test Acc: 0.94140625\n",
      "Step 140 -- Train loss: 0.18281328678131104, Train Acc: 0.94287109375 Test Acc: 0.9423828125\n",
      "Step 141 -- Train loss: 0.1832161396741867, Train Acc: 0.941162109375 Test Acc: 0.9453125\n",
      "Step 142 -- Train loss: 0.1838252991437912, Train Acc: 0.940673828125 Test Acc: 0.939453125\n",
      "Step 143 -- Train loss: 0.1826184093952179, Train Acc: 0.94140625 Test Acc: 0.9462890625\n",
      "Step 144 -- Train loss: 0.18198862671852112, Train Acc: 0.943359375 Test Acc: 0.9384765625\n",
      "Step 145 -- Train loss: 0.18230940401554108, Train Acc: 0.941650390625 Test Acc: 0.9375\n",
      "Step 146 -- Train loss: 0.18255150318145752, Train Acc: 0.940673828125 Test Acc: 0.9423828125\n",
      "Step 147 -- Train loss: 0.1825960874557495, Train Acc: 0.941162109375 Test Acc: 0.939453125\n",
      "Step 148 -- Train loss: 0.1808081865310669, Train Acc: 0.94287109375 Test Acc: 0.9404296875\n",
      "Step 149 -- Train loss: 0.18198679387569427, Train Acc: 0.941162109375 Test Acc: 0.943359375\n",
      "Step 150 -- Train loss: 0.18128719925880432, Train Acc: 0.940185546875 Test Acc: 0.9375\n",
      "Step 151 -- Train loss: 0.18042033910751343, Train Acc: 0.941650390625 Test Acc: 0.9443359375\n",
      "Step 152 -- Train loss: 0.18042969703674316, Train Acc: 0.9423828125 Test Acc: 0.939453125\n",
      "Step 153 -- Train loss: 0.17937271296977997, Train Acc: 0.941650390625 Test Acc: 0.94140625\n",
      "Step 154 -- Train loss: 0.18047086894512177, Train Acc: 0.941162109375 Test Acc: 0.9423828125\n",
      "Step 155 -- Train loss: 0.1800149530172348, Train Acc: 0.942138671875 Test Acc: 0.9443359375\n",
      "Step 156 -- Train loss: 0.18077704310417175, Train Acc: 0.94140625 Test Acc: 0.9423828125\n",
      "Step 157 -- Train loss: 0.17945490777492523, Train Acc: 0.94091796875 Test Acc: 0.9404296875\n",
      "Step 158 -- Train loss: 0.18108515441417694, Train Acc: 0.94091796875 Test Acc: 0.9443359375\n",
      "Step 159 -- Train loss: 0.1810608059167862, Train Acc: 0.941162109375 Test Acc: 0.9423828125\n",
      "Step 160 -- Train loss: 0.18081288039684296, Train Acc: 0.942138671875 Test Acc: 0.94140625\n",
      "Step 161 -- Train loss: 0.17928600311279297, Train Acc: 0.942626953125 Test Acc: 0.94140625\n",
      "Step 162 -- Train loss: 0.17940373718738556, Train Acc: 0.942138671875 Test Acc: 0.9423828125\n",
      "Step 163 -- Train loss: 0.1805320531129837, Train Acc: 0.9404296875 Test Acc: 0.94140625\n",
      "Step 164 -- Train loss: 0.18065893650054932, Train Acc: 0.93994140625 Test Acc: 0.94140625\n",
      "Step 165 -- Train loss: 0.1787603199481964, Train Acc: 0.941650390625 Test Acc: 0.9375\n",
      "Step 166 -- Train loss: 0.17860063910484314, Train Acc: 0.9423828125 Test Acc: 0.9423828125\n",
      "Step 167 -- Train loss: 0.1792932152748108, Train Acc: 0.941650390625 Test Acc: 0.94140625\n",
      "Step 168 -- Train loss: 0.17918390035629272, Train Acc: 0.9404296875 Test Acc: 0.9453125\n",
      "Step 169 -- Train loss: 0.17832839488983154, Train Acc: 0.941650390625 Test Acc: 0.9404296875\n",
      "Step 170 -- Train loss: 0.178444504737854, Train Acc: 0.944091796875 Test Acc: 0.94140625\n",
      "Step 171 -- Train loss: 0.18013989925384521, Train Acc: 0.94140625 Test Acc: 0.94140625\n",
      "Step 172 -- Train loss: 0.17865869402885437, Train Acc: 0.940673828125 Test Acc: 0.94140625\n",
      "Step 173 -- Train loss: 0.17869336903095245, Train Acc: 0.9423828125 Test Acc: 0.94140625\n",
      "Step 174 -- Train loss: 0.17809079587459564, Train Acc: 0.9423828125 Test Acc: 0.9384765625\n",
      "Step 175 -- Train loss: 0.1784367561340332, Train Acc: 0.941162109375 Test Acc: 0.943359375\n",
      "Step 176 -- Train loss: 0.17866285145282745, Train Acc: 0.942138671875 Test Acc: 0.9453125\n",
      "Step 177 -- Train loss: 0.1790381222963333, Train Acc: 0.940673828125 Test Acc: 0.939453125\n",
      "Step 178 -- Train loss: 0.1779664158821106, Train Acc: 0.943359375 Test Acc: 0.943359375\n",
      "Step 179 -- Train loss: 0.17818406224250793, Train Acc: 0.94287109375 Test Acc: 0.939453125\n",
      "Step 180 -- Train loss: 0.17907455563545227, Train Acc: 0.940673828125 Test Acc: 0.943359375\n",
      "Step 181 -- Train loss: 0.178447887301445, Train Acc: 0.941162109375 Test Acc: 0.939453125\n",
      "Step 182 -- Train loss: 0.1786009520292282, Train Acc: 0.94140625 Test Acc: 0.939453125\n",
      "Step 183 -- Train loss: 0.1781250685453415, Train Acc: 0.94140625 Test Acc: 0.9404296875\n",
      "Step 184 -- Train loss: 0.17803701758384705, Train Acc: 0.941162109375 Test Acc: 0.94140625\n",
      "Step 185 -- Train loss: 0.1778569519519806, Train Acc: 0.94189453125 Test Acc: 0.939453125\n",
      "Step 186 -- Train loss: 0.17790696024894714, Train Acc: 0.94140625 Test Acc: 0.9423828125\n",
      "Step 187 -- Train loss: 0.17822477221488953, Train Acc: 0.94189453125 Test Acc: 0.94140625\n",
      "Step 188 -- Train loss: 0.17684495449066162, Train Acc: 0.94091796875 Test Acc: 0.9453125\n",
      "Step 189 -- Train loss: 0.17764639854431152, Train Acc: 0.94140625 Test Acc: 0.9423828125\n",
      "Step 190 -- Train loss: 0.17902608215808868, Train Acc: 0.94140625 Test Acc: 0.943359375\n",
      "Step 191 -- Train loss: 0.17924244701862335, Train Acc: 0.940673828125 Test Acc: 0.9423828125\n",
      "Step 192 -- Train loss: 0.17819955945014954, Train Acc: 0.943603515625 Test Acc: 0.9384765625\n",
      "Step 193 -- Train loss: 0.1782284826040268, Train Acc: 0.94140625 Test Acc: 0.9404296875\n",
      "Step 194 -- Train loss: 0.17851684987545013, Train Acc: 0.94189453125 Test Acc: 0.9443359375\n",
      "Step 195 -- Train loss: 0.1780674159526825, Train Acc: 0.942138671875 Test Acc: 0.9443359375\n",
      "Step 196 -- Train loss: 0.17865054309368134, Train Acc: 0.94140625 Test Acc: 0.9404296875\n",
      "Step 197 -- Train loss: 0.17693904042243958, Train Acc: 0.941162109375 Test Acc: 0.9423828125\n",
      "Step 198 -- Train loss: 0.17659752070903778, Train Acc: 0.942138671875 Test Acc: 0.9423828125\n",
      "Step 199 -- Train loss: 0.17764098942279816, Train Acc: 0.940673828125 Test Acc: 0.9443359375\n",
      "Step 200 -- Train loss: 0.17784623801708221, Train Acc: 0.941650390625 Test Acc: 0.94140625\n",
      "Step 201 -- Train loss: 0.17756153643131256, Train Acc: 0.94140625 Test Acc: 0.9453125\n",
      "Step 202 -- Train loss: 0.17735601961612701, Train Acc: 0.943603515625 Test Acc: 0.9384765625\n",
      "Step 203 -- Train loss: 0.1779417097568512, Train Acc: 0.942138671875 Test Acc: 0.9423828125\n",
      "Step 204 -- Train loss: 0.1777193695306778, Train Acc: 0.941162109375 Test Acc: 0.9423828125\n",
      "Step 205 -- Train loss: 0.1772250235080719, Train Acc: 0.942138671875 Test Acc: 0.943359375\n",
      "Step 206 -- Train loss: 0.17827019095420837, Train Acc: 0.939208984375 Test Acc: 0.94140625\n",
      "Step 207 -- Train loss: 0.17730149626731873, Train Acc: 0.941162109375 Test Acc: 0.9423828125\n",
      "Step 208 -- Train loss: 0.1771402209997177, Train Acc: 0.940185546875 Test Acc: 0.9404296875\n",
      "Step 209 -- Train loss: 0.17850720882415771, Train Acc: 0.94140625 Test Acc: 0.94140625\n",
      "Step 210 -- Train loss: 0.17738421261310577, Train Acc: 0.941162109375 Test Acc: 0.94140625\n",
      "Step 211 -- Train loss: 0.1769140213727951, Train Acc: 0.9423828125 Test Acc: 0.9404296875\n",
      "Step 212 -- Train loss: 0.17689542472362518, Train Acc: 0.942138671875 Test Acc: 0.9443359375\n",
      "Step 213 -- Train loss: 0.17676705121994019, Train Acc: 0.942626953125 Test Acc: 0.9404296875\n",
      "Step 214 -- Train loss: 0.17672957479953766, Train Acc: 0.942138671875 Test Acc: 0.9404296875\n",
      "Step 215 -- Train loss: 0.17626729607582092, Train Acc: 0.94189453125 Test Acc: 0.94140625\n",
      "Step 216 -- Train loss: 0.17809858918190002, Train Acc: 0.941650390625 Test Acc: 0.943359375\n",
      "Step 217 -- Train loss: 0.17879635095596313, Train Acc: 0.940673828125 Test Acc: 0.939453125\n",
      "Step 218 -- Train loss: 0.17716175317764282, Train Acc: 0.9423828125 Test Acc: 0.94140625\n",
      "Step 219 -- Train loss: 0.17723774909973145, Train Acc: 0.940673828125 Test Acc: 0.94140625\n",
      "Step 220 -- Train loss: 0.17701475322246552, Train Acc: 0.941650390625 Test Acc: 0.943359375\n",
      "Step 221 -- Train loss: 0.17695565521717072, Train Acc: 0.941650390625 Test Acc: 0.939453125\n",
      "Step 222 -- Train loss: 0.1782066971063614, Train Acc: 0.940185546875 Test Acc: 0.9404296875\n",
      "Step 223 -- Train loss: 0.17653480172157288, Train Acc: 0.940673828125 Test Acc: 0.9453125\n",
      "Step 224 -- Train loss: 0.1771318018436432, Train Acc: 0.942138671875 Test Acc: 0.939453125\n",
      "Step 225 -- Train loss: 0.17667506635189056, Train Acc: 0.942138671875 Test Acc: 0.9443359375\n",
      "Step 226 -- Train loss: 0.1766124963760376, Train Acc: 0.943603515625 Test Acc: 0.94140625\n",
      "Step 227 -- Train loss: 0.17654038965702057, Train Acc: 0.943115234375 Test Acc: 0.94140625\n",
      "Step 228 -- Train loss: 0.1763097643852234, Train Acc: 0.9404296875 Test Acc: 0.9404296875\n",
      "Step 229 -- Train loss: 0.17654679715633392, Train Acc: 0.941162109375 Test Acc: 0.9443359375\n",
      "Step 230 -- Train loss: 0.17638595402240753, Train Acc: 0.941162109375 Test Acc: 0.939453125\n",
      "Step 231 -- Train loss: 0.1764645129442215, Train Acc: 0.942626953125 Test Acc: 0.9404296875\n",
      "Step 232 -- Train loss: 0.1786811649799347, Train Acc: 0.9404296875 Test Acc: 0.939453125\n",
      "Step 233 -- Train loss: 0.1763109564781189, Train Acc: 0.94140625 Test Acc: 0.943359375\n",
      "Step 234 -- Train loss: 0.17566978931427002, Train Acc: 0.943603515625 Test Acc: 0.94140625\n",
      "Step 235 -- Train loss: 0.17707666754722595, Train Acc: 0.9404296875 Test Acc: 0.939453125\n",
      "Step 236 -- Train loss: 0.17729951441287994, Train Acc: 0.939697265625 Test Acc: 0.9404296875\n",
      "Step 237 -- Train loss: 0.17758548259735107, Train Acc: 0.942138671875 Test Acc: 0.939453125\n",
      "Step 238 -- Train loss: 0.1770963817834854, Train Acc: 0.94091796875 Test Acc: 0.94140625\n",
      "Step 239 -- Train loss: 0.1756732165813446, Train Acc: 0.941650390625 Test Acc: 0.9462890625\n",
      "Step 240 -- Train loss: 0.17590825259685516, Train Acc: 0.943603515625 Test Acc: 0.943359375\n",
      "Step 241 -- Train loss: 0.17556945979595184, Train Acc: 0.942626953125 Test Acc: 0.9443359375\n",
      "Step 242 -- Train loss: 0.1762017011642456, Train Acc: 0.94287109375 Test Acc: 0.94140625\n",
      "Step 243 -- Train loss: 0.1764260232448578, Train Acc: 0.93994140625 Test Acc: 0.94140625\n",
      "Step 244 -- Train loss: 0.17630328238010406, Train Acc: 0.941650390625 Test Acc: 0.94140625\n",
      "Step 245 -- Train loss: 0.17555439472198486, Train Acc: 0.943359375 Test Acc: 0.943359375\n",
      "Step 246 -- Train loss: 0.17571395635604858, Train Acc: 0.94091796875 Test Acc: 0.94140625\n",
      "Step 247 -- Train loss: 0.17614737153053284, Train Acc: 0.94287109375 Test Acc: 0.9404296875\n",
      "Step 248 -- Train loss: 0.1763526350259781, Train Acc: 0.94091796875 Test Acc: 0.943359375\n",
      "Step 249 -- Train loss: 0.17613032460212708, Train Acc: 0.941162109375 Test Acc: 0.9462890625\n",
      "Step 250 -- Train loss: 0.17713811993598938, Train Acc: 0.94140625 Test Acc: 0.94140625\n",
      "Step 251 -- Train loss: 0.17596915364265442, Train Acc: 0.941650390625 Test Acc: 0.94140625\n",
      "Step 252 -- Train loss: 0.17623768746852875, Train Acc: 0.9423828125 Test Acc: 0.943359375\n",
      "Step 253 -- Train loss: 0.17666204273700714, Train Acc: 0.94140625 Test Acc: 0.9404296875\n",
      "Step 254 -- Train loss: 0.1767122745513916, Train Acc: 0.94189453125 Test Acc: 0.94140625\n",
      "Step 255 -- Train loss: 0.17608805000782013, Train Acc: 0.94091796875 Test Acc: 0.9404296875\n",
      "Step 256 -- Train loss: 0.17560088634490967, Train Acc: 0.942626953125 Test Acc: 0.94140625\n",
      "Step 257 -- Train loss: 0.1767197698354721, Train Acc: 0.9423828125 Test Acc: 0.94140625\n",
      "Step 258 -- Train loss: 0.17652319371700287, Train Acc: 0.941162109375 Test Acc: 0.94140625\n",
      "Step 259 -- Train loss: 0.1764592081308365, Train Acc: 0.941650390625 Test Acc: 0.94140625\n",
      "Step 260 -- Train loss: 0.17607621848583221, Train Acc: 0.941162109375 Test Acc: 0.939453125\n",
      "Step 261 -- Train loss: 0.17600972950458527, Train Acc: 0.943359375 Test Acc: 0.9404296875\n",
      "Step 262 -- Train loss: 0.1765279322862625, Train Acc: 0.9423828125 Test Acc: 0.9423828125\n",
      "Step 263 -- Train loss: 0.1762494295835495, Train Acc: 0.941650390625 Test Acc: 0.9404296875\n",
      "Step 264 -- Train loss: 0.17747725546360016, Train Acc: 0.939453125 Test Acc: 0.943359375\n",
      "Step 265 -- Train loss: 0.17550192773342133, Train Acc: 0.94140625 Test Acc: 0.939453125\n",
      "Step 266 -- Train loss: 0.17558778822422028, Train Acc: 0.94287109375 Test Acc: 0.94140625\n",
      "Step 267 -- Train loss: 0.1755170375108719, Train Acc: 0.940673828125 Test Acc: 0.9423828125\n",
      "Step 268 -- Train loss: 0.17633892595767975, Train Acc: 0.941650390625 Test Acc: 0.9423828125\n",
      "Step 269 -- Train loss: 0.17552687227725983, Train Acc: 0.941650390625 Test Acc: 0.94140625\n",
      "Step 270 -- Train loss: 0.1757902354001999, Train Acc: 0.940185546875 Test Acc: 0.939453125\n",
      "Step 271 -- Train loss: 0.17641952633857727, Train Acc: 0.941162109375 Test Acc: 0.9423828125\n",
      "Step 272 -- Train loss: 0.17550159990787506, Train Acc: 0.942138671875 Test Acc: 0.9404296875\n",
      "Step 273 -- Train loss: 0.17627081274986267, Train Acc: 0.940673828125 Test Acc: 0.9404296875\n",
      "Step 274 -- Train loss: 0.17518599331378937, Train Acc: 0.943359375 Test Acc: 0.943359375\n",
      "Step 275 -- Train loss: 0.17583774030208588, Train Acc: 0.942626953125 Test Acc: 0.9384765625\n",
      "Step 276 -- Train loss: 0.17607684433460236, Train Acc: 0.941650390625 Test Acc: 0.943359375\n",
      "Step 277 -- Train loss: 0.1760587841272354, Train Acc: 0.943115234375 Test Acc: 0.9404296875\n",
      "Step 278 -- Train loss: 0.17528840899467468, Train Acc: 0.9423828125 Test Acc: 0.9404296875\n",
      "Step 279 -- Train loss: 0.17585226893424988, Train Acc: 0.94140625 Test Acc: 0.94140625\n",
      "Step 280 -- Train loss: 0.17547954618930817, Train Acc: 0.9423828125 Test Acc: 0.94140625\n",
      "Step 281 -- Train loss: 0.17587175965309143, Train Acc: 0.941650390625 Test Acc: 0.939453125\n",
      "Step 282 -- Train loss: 0.17647790908813477, Train Acc: 0.94140625 Test Acc: 0.943359375\n",
      "Step 283 -- Train loss: 0.17582249641418457, Train Acc: 0.940673828125 Test Acc: 0.9423828125\n",
      "Step 284 -- Train loss: 0.17546798288822174, Train Acc: 0.94140625 Test Acc: 0.9404296875\n",
      "Step 285 -- Train loss: 0.17559199035167694, Train Acc: 0.94140625 Test Acc: 0.9404296875\n",
      "Step 286 -- Train loss: 0.17579132318496704, Train Acc: 0.9423828125 Test Acc: 0.9423828125\n",
      "Step 287 -- Train loss: 0.17515188455581665, Train Acc: 0.942626953125 Test Acc: 0.94140625\n",
      "Step 288 -- Train loss: 0.1751907765865326, Train Acc: 0.943603515625 Test Acc: 0.943359375\n",
      "Step 289 -- Train loss: 0.17653824388980865, Train Acc: 0.94091796875 Test Acc: 0.9462890625\n",
      "Step 290 -- Train loss: 0.17578604817390442, Train Acc: 0.94140625 Test Acc: 0.939453125\n",
      "Step 291 -- Train loss: 0.17575788497924805, Train Acc: 0.94140625 Test Acc: 0.9423828125\n",
      "Step 292 -- Train loss: 0.17577125132083893, Train Acc: 0.941650390625 Test Acc: 0.943359375\n",
      "Step 293 -- Train loss: 0.17550721764564514, Train Acc: 0.941162109375 Test Acc: 0.9384765625\n",
      "Step 294 -- Train loss: 0.17577940225601196, Train Acc: 0.94189453125 Test Acc: 0.9384765625\n",
      "Step 295 -- Train loss: 0.17617173492908478, Train Acc: 0.93994140625 Test Acc: 0.9384765625\n",
      "Step 296 -- Train loss: 0.17556512355804443, Train Acc: 0.93994140625 Test Acc: 0.939453125\n",
      "Step 297 -- Train loss: 0.1747705340385437, Train Acc: 0.9423828125 Test Acc: 0.94140625\n",
      "Step 298 -- Train loss: 0.17526735365390778, Train Acc: 0.94189453125 Test Acc: 0.9404296875\n",
      "Step 299 -- Train loss: 0.17556560039520264, Train Acc: 0.94140625 Test Acc: 0.94140625\n",
      "Step 300 -- Train loss: 0.17520467936992645, Train Acc: 0.94189453125 Test Acc: 0.943359375\n",
      "Step 301 -- Train loss: 0.17674745619297028, Train Acc: 0.94091796875 Test Acc: 0.94140625\n",
      "Step 302 -- Train loss: 0.17516905069351196, Train Acc: 0.942626953125 Test Acc: 0.943359375\n",
      "Step 303 -- Train loss: 0.17580373585224152, Train Acc: 0.94091796875 Test Acc: 0.9423828125\n",
      "Step 304 -- Train loss: 0.17518098652362823, Train Acc: 0.942626953125 Test Acc: 0.9404296875\n",
      "Step 305 -- Train loss: 0.17569921910762787, Train Acc: 0.94091796875 Test Acc: 0.9423828125\n",
      "Step 306 -- Train loss: 0.1763022243976593, Train Acc: 0.942138671875 Test Acc: 0.939453125\n",
      "Step 307 -- Train loss: 0.17535363137722015, Train Acc: 0.94189453125 Test Acc: 0.94140625\n",
      "Step 308 -- Train loss: 0.1747300773859024, Train Acc: 0.942626953125 Test Acc: 0.9404296875\n",
      "Step 309 -- Train loss: 0.1756652444601059, Train Acc: 0.940673828125 Test Acc: 0.9423828125\n",
      "Step 310 -- Train loss: 0.1745029091835022, Train Acc: 0.943359375 Test Acc: 0.9443359375\n",
      "Step 311 -- Train loss: 0.17522117495536804, Train Acc: 0.942138671875 Test Acc: 0.9423828125\n",
      "Step 312 -- Train loss: 0.17505036294460297, Train Acc: 0.942138671875 Test Acc: 0.9384765625\n",
      "Step 313 -- Train loss: 0.17538197338581085, Train Acc: 0.94140625 Test Acc: 0.9453125\n",
      "Step 314 -- Train loss: 0.17520219087600708, Train Acc: 0.941162109375 Test Acc: 0.94140625\n",
      "Step 315 -- Train loss: 0.17645412683486938, Train Acc: 0.941162109375 Test Acc: 0.94140625\n",
      "Step 316 -- Train loss: 0.17539264261722565, Train Acc: 0.94091796875 Test Acc: 0.9453125\n",
      "Step 317 -- Train loss: 0.17501981556415558, Train Acc: 0.942626953125 Test Acc: 0.943359375\n",
      "Step 318 -- Train loss: 0.17477606236934662, Train Acc: 0.94287109375 Test Acc: 0.9423828125\n",
      "Step 319 -- Train loss: 0.17601865530014038, Train Acc: 0.941162109375 Test Acc: 0.943359375\n",
      "Step 320 -- Train loss: 0.17551091313362122, Train Acc: 0.9423828125 Test Acc: 0.9443359375\n",
      "Step 321 -- Train loss: 0.17544561624526978, Train Acc: 0.94140625 Test Acc: 0.94140625\n",
      "Step 322 -- Train loss: 0.17585232853889465, Train Acc: 0.941162109375 Test Acc: 0.9462890625\n",
      "Step 323 -- Train loss: 0.17600905895233154, Train Acc: 0.94091796875 Test Acc: 0.9404296875\n",
      "Step 324 -- Train loss: 0.17503522336483002, Train Acc: 0.94287109375 Test Acc: 0.9404296875\n",
      "Step 325 -- Train loss: 0.1752757430076599, Train Acc: 0.9404296875 Test Acc: 0.9423828125\n",
      "Step 326 -- Train loss: 0.17466698586940765, Train Acc: 0.9423828125 Test Acc: 0.9404296875\n",
      "Step 327 -- Train loss: 0.17561481893062592, Train Acc: 0.942626953125 Test Acc: 0.9423828125\n",
      "Step 328 -- Train loss: 0.17499001324176788, Train Acc: 0.944580078125 Test Acc: 0.939453125\n",
      "Step 329 -- Train loss: 0.17524318397045135, Train Acc: 0.942138671875 Test Acc: 0.94140625\n",
      "Step 330 -- Train loss: 0.17519643902778625, Train Acc: 0.942626953125 Test Acc: 0.94140625\n",
      "Step 331 -- Train loss: 0.17564652860164642, Train Acc: 0.94140625 Test Acc: 0.9462890625\n",
      "Step 332 -- Train loss: 0.1753651648759842, Train Acc: 0.94189453125 Test Acc: 0.943359375\n",
      "Step 333 -- Train loss: 0.17509004473686218, Train Acc: 0.9423828125 Test Acc: 0.9375\n",
      "Step 334 -- Train loss: 0.17527589201927185, Train Acc: 0.941162109375 Test Acc: 0.9404296875\n",
      "Step 335 -- Train loss: 0.17578904330730438, Train Acc: 0.9423828125 Test Acc: 0.9462890625\n",
      "Step 336 -- Train loss: 0.17625480890274048, Train Acc: 0.941650390625 Test Acc: 0.939453125\n",
      "Step 337 -- Train loss: 0.17578426003456116, Train Acc: 0.93994140625 Test Acc: 0.9404296875\n",
      "Step 338 -- Train loss: 0.17549502849578857, Train Acc: 0.94140625 Test Acc: 0.9404296875\n",
      "Step 339 -- Train loss: 0.17511212825775146, Train Acc: 0.94189453125 Test Acc: 0.94140625\n",
      "Step 340 -- Train loss: 0.17555038630962372, Train Acc: 0.940185546875 Test Acc: 0.94140625\n",
      "Step 341 -- Train loss: 0.17539210617542267, Train Acc: 0.94287109375 Test Acc: 0.943359375\n",
      "Step 342 -- Train loss: 0.17562341690063477, Train Acc: 0.942626953125 Test Acc: 0.9453125\n",
      "Step 343 -- Train loss: 0.17509876191616058, Train Acc: 0.944580078125 Test Acc: 0.939453125\n",
      "Step 344 -- Train loss: 0.17593973875045776, Train Acc: 0.94140625 Test Acc: 0.9404296875\n",
      "Step 345 -- Train loss: 0.17541185021400452, Train Acc: 0.940185546875 Test Acc: 0.943359375\n",
      "Step 346 -- Train loss: 0.1760362982749939, Train Acc: 0.939697265625 Test Acc: 0.9423828125\n",
      "Step 347 -- Train loss: 0.1764795184135437, Train Acc: 0.940185546875 Test Acc: 0.94140625\n",
      "Step 348 -- Train loss: 0.1744074821472168, Train Acc: 0.942138671875 Test Acc: 0.939453125\n",
      "Step 349 -- Train loss: 0.17511038482189178, Train Acc: 0.942138671875 Test Acc: 0.94140625\n",
      "Step 350 -- Train loss: 0.17556294798851013, Train Acc: 0.942626953125 Test Acc: 0.939453125\n",
      "Step 351 -- Train loss: 0.17470866441726685, Train Acc: 0.94140625 Test Acc: 0.9423828125\n",
      "Step 352 -- Train loss: 0.17505478858947754, Train Acc: 0.941650390625 Test Acc: 0.9443359375\n",
      "Step 353 -- Train loss: 0.1754198968410492, Train Acc: 0.940673828125 Test Acc: 0.94140625\n",
      "Step 354 -- Train loss: 0.17465923726558685, Train Acc: 0.94091796875 Test Acc: 0.9404296875\n",
      "Step 355 -- Train loss: 0.1749984174966812, Train Acc: 0.94091796875 Test Acc: 0.939453125\n",
      "Step 356 -- Train loss: 0.17600801587104797, Train Acc: 0.94140625 Test Acc: 0.9384765625\n",
      "Step 357 -- Train loss: 0.17516233026981354, Train Acc: 0.940673828125 Test Acc: 0.939453125\n",
      "Step 358 -- Train loss: 0.1746254712343216, Train Acc: 0.940673828125 Test Acc: 0.9443359375\n",
      "Step 359 -- Train loss: 0.1750609129667282, Train Acc: 0.9423828125 Test Acc: 0.943359375\n",
      "Step 360 -- Train loss: 0.17493705451488495, Train Acc: 0.942626953125 Test Acc: 0.94140625\n",
      "Step 361 -- Train loss: 0.17449626326560974, Train Acc: 0.94140625 Test Acc: 0.9404296875\n",
      "Step 362 -- Train loss: 0.17481864988803864, Train Acc: 0.94140625 Test Acc: 0.939453125\n",
      "Step 363 -- Train loss: 0.17537955939769745, Train Acc: 0.94189453125 Test Acc: 0.939453125\n",
      "Step 364 -- Train loss: 0.1750721037387848, Train Acc: 0.942626953125 Test Acc: 0.94140625\n",
      "Step 365 -- Train loss: 0.17512628436088562, Train Acc: 0.942626953125 Test Acc: 0.9423828125\n",
      "Step 366 -- Train loss: 0.1754077970981598, Train Acc: 0.941650390625 Test Acc: 0.939453125\n",
      "Step 367 -- Train loss: 0.17467451095581055, Train Acc: 0.94140625 Test Acc: 0.9404296875\n",
      "Step 368 -- Train loss: 0.17411915957927704, Train Acc: 0.94287109375 Test Acc: 0.943359375\n",
      "Step 369 -- Train loss: 0.17499855160713196, Train Acc: 0.941162109375 Test Acc: 0.94140625\n",
      "Step 370 -- Train loss: 0.175472229719162, Train Acc: 0.94091796875 Test Acc: 0.94140625\n",
      "Step 371 -- Train loss: 0.175209641456604, Train Acc: 0.94091796875 Test Acc: 0.939453125\n",
      "Step 372 -- Train loss: 0.1746322512626648, Train Acc: 0.939697265625 Test Acc: 0.94140625\n",
      "Step 373 -- Train loss: 0.17452827095985413, Train Acc: 0.94091796875 Test Acc: 0.9423828125\n",
      "Step 374 -- Train loss: 0.1747603416442871, Train Acc: 0.941162109375 Test Acc: 0.943359375\n",
      "Step 375 -- Train loss: 0.17486153542995453, Train Acc: 0.942626953125 Test Acc: 0.9423828125\n",
      "Step 376 -- Train loss: 0.17515690624713898, Train Acc: 0.93994140625 Test Acc: 0.9423828125\n",
      "Step 377 -- Train loss: 0.1748652309179306, Train Acc: 0.94189453125 Test Acc: 0.9423828125\n",
      "Step 378 -- Train loss: 0.1744946837425232, Train Acc: 0.941162109375 Test Acc: 0.939453125\n",
      "Step 379 -- Train loss: 0.17541326582431793, Train Acc: 0.94091796875 Test Acc: 0.9384765625\n",
      "Step 380 -- Train loss: 0.17419539391994476, Train Acc: 0.943603515625 Test Acc: 0.94140625\n",
      "Step 381 -- Train loss: 0.1755266934633255, Train Acc: 0.94287109375 Test Acc: 0.94140625\n",
      "Step 382 -- Train loss: 0.17468473315238953, Train Acc: 0.94287109375 Test Acc: 0.9384765625\n",
      "Step 383 -- Train loss: 0.17468537390232086, Train Acc: 0.94091796875 Test Acc: 0.939453125\n",
      "Step 384 -- Train loss: 0.17517490684986115, Train Acc: 0.940673828125 Test Acc: 0.9404296875\n",
      "Step 385 -- Train loss: 0.17514248192310333, Train Acc: 0.94140625 Test Acc: 0.9423828125\n",
      "Step 386 -- Train loss: 0.1746227890253067, Train Acc: 0.942138671875 Test Acc: 0.9443359375\n",
      "Step 387 -- Train loss: 0.17501164972782135, Train Acc: 0.941650390625 Test Acc: 0.94140625\n",
      "Step 388 -- Train loss: 0.17503581941127777, Train Acc: 0.9404296875 Test Acc: 0.939453125\n",
      "Step 389 -- Train loss: 0.17443065345287323, Train Acc: 0.941162109375 Test Acc: 0.9423828125\n",
      "Step 390 -- Train loss: 0.1748911738395691, Train Acc: 0.94091796875 Test Acc: 0.9443359375\n",
      "Step 391 -- Train loss: 0.17413273453712463, Train Acc: 0.943115234375 Test Acc: 0.94140625\n",
      "Step 392 -- Train loss: 0.1755211502313614, Train Acc: 0.94189453125 Test Acc: 0.94140625\n",
      "Step 393 -- Train loss: 0.17663131654262543, Train Acc: 0.9404296875 Test Acc: 0.94140625\n",
      "Step 394 -- Train loss: 0.17458690702915192, Train Acc: 0.942138671875 Test Acc: 0.94140625\n",
      "Step 395 -- Train loss: 0.17595921456813812, Train Acc: 0.94140625 Test Acc: 0.943359375\n",
      "Step 396 -- Train loss: 0.1742834895849228, Train Acc: 0.942138671875 Test Acc: 0.9423828125\n",
      "Step 397 -- Train loss: 0.17498710751533508, Train Acc: 0.943115234375 Test Acc: 0.9453125\n",
      "Step 398 -- Train loss: 0.17486660182476044, Train Acc: 0.9423828125 Test Acc: 0.939453125\n",
      "Step 399 -- Train loss: 0.1753067672252655, Train Acc: 0.941650390625 Test Acc: 0.9404296875\n",
      "Step 400 -- Train loss: 0.17513418197631836, Train Acc: 0.940185546875 Test Acc: 0.9423828125\n",
      "Step 401 -- Train loss: 0.17603890597820282, Train Acc: 0.941162109375 Test Acc: 0.9404296875\n",
      "Step 402 -- Train loss: 0.17492704093456268, Train Acc: 0.941162109375 Test Acc: 0.94140625\n",
      "Step 403 -- Train loss: 0.17458876967430115, Train Acc: 0.942138671875 Test Acc: 0.9404296875\n",
      "Step 404 -- Train loss: 0.17670582234859467, Train Acc: 0.939697265625 Test Acc: 0.94140625\n",
      "Step 405 -- Train loss: 0.17486321926116943, Train Acc: 0.9404296875 Test Acc: 0.94140625\n",
      "Step 406 -- Train loss: 0.17517264187335968, Train Acc: 0.9423828125 Test Acc: 0.9423828125\n",
      "Step 407 -- Train loss: 0.174760639667511, Train Acc: 0.94140625 Test Acc: 0.939453125\n",
      "Step 408 -- Train loss: 0.17555946111679077, Train Acc: 0.940185546875 Test Acc: 0.94140625\n",
      "Step 409 -- Train loss: 0.17520268261432648, Train Acc: 0.941162109375 Test Acc: 0.9404296875\n",
      "Step 410 -- Train loss: 0.17440153658390045, Train Acc: 0.943359375 Test Acc: 0.9384765625\n",
      "Step 411 -- Train loss: 0.1748022884130478, Train Acc: 0.94189453125 Test Acc: 0.9404296875\n",
      "Step 412 -- Train loss: 0.1752665936946869, Train Acc: 0.940185546875 Test Acc: 0.9443359375\n",
      "Step 413 -- Train loss: 0.17374250292778015, Train Acc: 0.94287109375 Test Acc: 0.939453125\n",
      "Step 414 -- Train loss: 0.17541298270225525, Train Acc: 0.94091796875 Test Acc: 0.9423828125\n",
      "Step 415 -- Train loss: 0.1757216453552246, Train Acc: 0.940673828125 Test Acc: 0.9423828125\n",
      "Step 416 -- Train loss: 0.1753162443637848, Train Acc: 0.940673828125 Test Acc: 0.939453125\n",
      "Step 417 -- Train loss: 0.17512454092502594, Train Acc: 0.940185546875 Test Acc: 0.94140625\n",
      "Step 418 -- Train loss: 0.1746813952922821, Train Acc: 0.94140625 Test Acc: 0.9443359375\n",
      "Step 419 -- Train loss: 0.17371167242527008, Train Acc: 0.943359375 Test Acc: 0.9423828125\n",
      "Step 420 -- Train loss: 0.17556196451187134, Train Acc: 0.941650390625 Test Acc: 0.9404296875\n",
      "Step 421 -- Train loss: 0.17509996891021729, Train Acc: 0.94140625 Test Acc: 0.94140625\n",
      "Step 422 -- Train loss: 0.17487733066082, Train Acc: 0.943603515625 Test Acc: 0.943359375\n",
      "Step 423 -- Train loss: 0.17416398227214813, Train Acc: 0.941650390625 Test Acc: 0.9404296875\n",
      "Step 424 -- Train loss: 0.1749277412891388, Train Acc: 0.940673828125 Test Acc: 0.94140625\n",
      "Step 425 -- Train loss: 0.17528153955936432, Train Acc: 0.94287109375 Test Acc: 0.9443359375\n",
      "Step 426 -- Train loss: 0.17516955733299255, Train Acc: 0.941162109375 Test Acc: 0.9404296875\n",
      "Step 427 -- Train loss: 0.1743655651807785, Train Acc: 0.94091796875 Test Acc: 0.9443359375\n",
      "Step 428 -- Train loss: 0.17502814531326294, Train Acc: 0.9423828125 Test Acc: 0.9443359375\n",
      "Step 429 -- Train loss: 0.17475129663944244, Train Acc: 0.94189453125 Test Acc: 0.9443359375\n",
      "Step 430 -- Train loss: 0.17360003292560577, Train Acc: 0.9423828125 Test Acc: 0.9404296875\n",
      "Step 431 -- Train loss: 0.17451554536819458, Train Acc: 0.94287109375 Test Acc: 0.943359375\n",
      "Step 432 -- Train loss: 0.1756899058818817, Train Acc: 0.941162109375 Test Acc: 0.9384765625\n",
      "Step 433 -- Train loss: 0.17624472081661224, Train Acc: 0.941162109375 Test Acc: 0.943359375\n",
      "Step 434 -- Train loss: 0.17428229749202728, Train Acc: 0.942138671875 Test Acc: 0.939453125\n",
      "Step 435 -- Train loss: 0.17508631944656372, Train Acc: 0.941650390625 Test Acc: 0.9443359375\n",
      "Step 436 -- Train loss: 0.17432785034179688, Train Acc: 0.942138671875 Test Acc: 0.9404296875\n",
      "Step 437 -- Train loss: 0.1755157858133316, Train Acc: 0.942626953125 Test Acc: 0.94140625\n",
      "Step 438 -- Train loss: 0.17547957599163055, Train Acc: 0.940673828125 Test Acc: 0.939453125\n",
      "Step 439 -- Train loss: 0.17487117648124695, Train Acc: 0.942138671875 Test Acc: 0.939453125\n",
      "Step 440 -- Train loss: 0.1751018464565277, Train Acc: 0.941650390625 Test Acc: 0.9423828125\n",
      "Step 441 -- Train loss: 0.1738928258419037, Train Acc: 0.943115234375 Test Acc: 0.9404296875\n",
      "Step 442 -- Train loss: 0.17273710668087006, Train Acc: 0.94189453125 Test Acc: 0.9404296875\n",
      "Step 443 -- Train loss: 0.17597121000289917, Train Acc: 0.940673828125 Test Acc: 0.939453125\n",
      "Step 444 -- Train loss: 0.17513452470302582, Train Acc: 0.941162109375 Test Acc: 0.939453125\n",
      "Step 445 -- Train loss: 0.17531505227088928, Train Acc: 0.9404296875 Test Acc: 0.943359375\n",
      "Step 446 -- Train loss: 0.17421333491802216, Train Acc: 0.9423828125 Test Acc: 0.9404296875\n",
      "Step 447 -- Train loss: 0.17460229992866516, Train Acc: 0.940185546875 Test Acc: 0.939453125\n",
      "Step 448 -- Train loss: 0.17437715828418732, Train Acc: 0.9404296875 Test Acc: 0.9443359375\n",
      "Step 449 -- Train loss: 0.17431864142417908, Train Acc: 0.94189453125 Test Acc: 0.9375\n",
      "Step 450 -- Train loss: 0.1755826324224472, Train Acc: 0.94091796875 Test Acc: 0.9404296875\n",
      "Step 451 -- Train loss: 0.17499291896820068, Train Acc: 0.941650390625 Test Acc: 0.943359375\n",
      "Step 452 -- Train loss: 0.17503879964351654, Train Acc: 0.941650390625 Test Acc: 0.9423828125\n",
      "Step 453 -- Train loss: 0.17459124326705933, Train Acc: 0.940673828125 Test Acc: 0.94140625\n",
      "Step 454 -- Train loss: 0.17420728504657745, Train Acc: 0.94384765625 Test Acc: 0.943359375\n",
      "Step 455 -- Train loss: 0.17462033033370972, Train Acc: 0.941650390625 Test Acc: 0.9404296875\n",
      "Step 456 -- Train loss: 0.1746601015329361, Train Acc: 0.94140625 Test Acc: 0.94140625\n",
      "Step 457 -- Train loss: 0.17406494915485382, Train Acc: 0.9404296875 Test Acc: 0.939453125\n",
      "Step 458 -- Train loss: 0.17442086338996887, Train Acc: 0.94384765625 Test Acc: 0.9423828125\n",
      "Step 459 -- Train loss: 0.17383265495300293, Train Acc: 0.94384765625 Test Acc: 0.9423828125\n",
      "Step 460 -- Train loss: 0.17556226253509521, Train Acc: 0.94140625 Test Acc: 0.9443359375\n",
      "Step 461 -- Train loss: 0.17501197755336761, Train Acc: 0.94091796875 Test Acc: 0.943359375\n",
      "Step 462 -- Train loss: 0.1743404120206833, Train Acc: 0.94189453125 Test Acc: 0.943359375\n",
      "Step 463 -- Train loss: 0.17413070797920227, Train Acc: 0.943359375 Test Acc: 0.9423828125\n",
      "Step 464 -- Train loss: 0.17387180030345917, Train Acc: 0.94287109375 Test Acc: 0.939453125\n",
      "Step 465 -- Train loss: 0.17574916779994965, Train Acc: 0.940185546875 Test Acc: 0.9423828125\n",
      "Step 466 -- Train loss: 0.17377404868602753, Train Acc: 0.94091796875 Test Acc: 0.9404296875\n",
      "Step 467 -- Train loss: 0.17526596784591675, Train Acc: 0.94140625 Test Acc: 0.943359375\n",
      "Step 468 -- Train loss: 0.17598943412303925, Train Acc: 0.94091796875 Test Acc: 0.9404296875\n",
      "Step 469 -- Train loss: 0.17581135034561157, Train Acc: 0.94140625 Test Acc: 0.9443359375\n",
      "Step 470 -- Train loss: 0.17493967711925507, Train Acc: 0.94140625 Test Acc: 0.94140625\n",
      "Step 471 -- Train loss: 0.17476968467235565, Train Acc: 0.941650390625 Test Acc: 0.94140625\n",
      "Step 472 -- Train loss: 0.17515386641025543, Train Acc: 0.940185546875 Test Acc: 0.94140625\n",
      "Step 473 -- Train loss: 0.17407014966011047, Train Acc: 0.942138671875 Test Acc: 0.94140625\n",
      "Step 474 -- Train loss: 0.1738305687904358, Train Acc: 0.9423828125 Test Acc: 0.94140625\n",
      "Step 475 -- Train loss: 0.17486430704593658, Train Acc: 0.941162109375 Test Acc: 0.9443359375\n",
      "Step 476 -- Train loss: 0.17423737049102783, Train Acc: 0.9423828125 Test Acc: 0.9404296875\n",
      "Step 477 -- Train loss: 0.17463168501853943, Train Acc: 0.941162109375 Test Acc: 0.9404296875\n",
      "Step 478 -- Train loss: 0.17400187253952026, Train Acc: 0.942626953125 Test Acc: 0.943359375\n",
      "Step 479 -- Train loss: 0.1761821210384369, Train Acc: 0.941650390625 Test Acc: 0.9423828125\n",
      "Step 480 -- Train loss: 0.17425550520420074, Train Acc: 0.942138671875 Test Acc: 0.9404296875\n",
      "Step 481 -- Train loss: 0.17529284954071045, Train Acc: 0.943115234375 Test Acc: 0.9423828125\n",
      "Step 482 -- Train loss: 0.17432883381843567, Train Acc: 0.942626953125 Test Acc: 0.9404296875\n",
      "Step 483 -- Train loss: 0.17374175786972046, Train Acc: 0.9404296875 Test Acc: 0.9404296875\n",
      "Step 484 -- Train loss: 0.1748390793800354, Train Acc: 0.94189453125 Test Acc: 0.9404296875\n",
      "Step 485 -- Train loss: 0.1736738383769989, Train Acc: 0.94287109375 Test Acc: 0.943359375\n",
      "Step 486 -- Train loss: 0.17457179725170135, Train Acc: 0.941162109375 Test Acc: 0.9423828125\n",
      "Step 487 -- Train loss: 0.17585745453834534, Train Acc: 0.941650390625 Test Acc: 0.939453125\n",
      "Step 488 -- Train loss: 0.17412202060222626, Train Acc: 0.942138671875 Test Acc: 0.9423828125\n",
      "Step 489 -- Train loss: 0.1746198832988739, Train Acc: 0.942138671875 Test Acc: 0.939453125\n",
      "Step 490 -- Train loss: 0.1750091165304184, Train Acc: 0.941162109375 Test Acc: 0.9404296875\n",
      "Step 491 -- Train loss: 0.17468996345996857, Train Acc: 0.9404296875 Test Acc: 0.939453125\n",
      "Step 492 -- Train loss: 0.17477265000343323, Train Acc: 0.94287109375 Test Acc: 0.94140625\n",
      "Step 493 -- Train loss: 0.1750536859035492, Train Acc: 0.941650390625 Test Acc: 0.94140625\n",
      "Step 494 -- Train loss: 0.17417961359024048, Train Acc: 0.94140625 Test Acc: 0.9404296875\n",
      "Step 495 -- Train loss: 0.17452318966388702, Train Acc: 0.94189453125 Test Acc: 0.9423828125\n",
      "Step 496 -- Train loss: 0.1742677390575409, Train Acc: 0.94091796875 Test Acc: 0.9423828125\n",
      "Step 497 -- Train loss: 0.17378559708595276, Train Acc: 0.9423828125 Test Acc: 0.9384765625\n",
      "Step 498 -- Train loss: 0.17481085658073425, Train Acc: 0.94140625 Test Acc: 0.9404296875\n",
      "Step 499 -- Train loss: 0.17368806898593903, Train Acc: 0.943359375 Test Acc: 0.9423828125\n",
      "Step 500 -- Train loss: 0.17589245736598969, Train Acc: 0.94091796875 Test Acc: 0.9404296875\n",
      "Step 501 -- Train loss: 0.17416483163833618, Train Acc: 0.941650390625 Test Acc: 0.9404296875\n",
      "Step 502 -- Train loss: 0.1743524968624115, Train Acc: 0.94091796875 Test Acc: 0.939453125\n",
      "Step 503 -- Train loss: 0.1749347597360611, Train Acc: 0.94189453125 Test Acc: 0.94140625\n",
      "Step 504 -- Train loss: 0.17474111914634705, Train Acc: 0.941162109375 Test Acc: 0.9404296875\n",
      "Step 505 -- Train loss: 0.1747492402791977, Train Acc: 0.94140625 Test Acc: 0.94140625\n",
      "Step 506 -- Train loss: 0.1741628348827362, Train Acc: 0.94287109375 Test Acc: 0.9404296875\n",
      "Step 507 -- Train loss: 0.17598596215248108, Train Acc: 0.9404296875 Test Acc: 0.9404296875\n",
      "Step 508 -- Train loss: 0.1745942234992981, Train Acc: 0.941650390625 Test Acc: 0.9423828125\n",
      "Step 509 -- Train loss: 0.174240380525589, Train Acc: 0.94189453125 Test Acc: 0.94140625\n",
      "Step 510 -- Train loss: 0.17397718131542206, Train Acc: 0.94189453125 Test Acc: 0.939453125\n",
      "Step 511 -- Train loss: 0.17521890997886658, Train Acc: 0.939697265625 Test Acc: 0.939453125\n",
      "Step 512 -- Train loss: 0.17366161942481995, Train Acc: 0.94189453125 Test Acc: 0.9404296875\n",
      "Step 513 -- Train loss: 0.17411872744560242, Train Acc: 0.94189453125 Test Acc: 0.9423828125\n",
      "Step 514 -- Train loss: 0.1746656447649002, Train Acc: 0.9423828125 Test Acc: 0.939453125\n",
      "Step 515 -- Train loss: 0.17532791197299957, Train Acc: 0.942626953125 Test Acc: 0.9423828125\n",
      "Step 516 -- Train loss: 0.1753195822238922, Train Acc: 0.942138671875 Test Acc: 0.9443359375\n",
      "Step 517 -- Train loss: 0.17337563633918762, Train Acc: 0.944091796875 Test Acc: 0.939453125\n",
      "Step 518 -- Train loss: 0.17426730692386627, Train Acc: 0.9404296875 Test Acc: 0.94140625\n",
      "Step 519 -- Train loss: 0.17494939267635345, Train Acc: 0.94189453125 Test Acc: 0.9404296875\n",
      "Step 520 -- Train loss: 0.1741441786289215, Train Acc: 0.94140625 Test Acc: 0.939453125\n",
      "Step 521 -- Train loss: 0.17468415200710297, Train Acc: 0.941650390625 Test Acc: 0.939453125\n",
      "Step 522 -- Train loss: 0.17491932213306427, Train Acc: 0.940673828125 Test Acc: 0.943359375\n",
      "Step 523 -- Train loss: 0.17408347129821777, Train Acc: 0.94287109375 Test Acc: 0.9423828125\n",
      "Step 524 -- Train loss: 0.17386898398399353, Train Acc: 0.941162109375 Test Acc: 0.9453125\n",
      "Step 525 -- Train loss: 0.17498235404491425, Train Acc: 0.94140625 Test Acc: 0.94140625\n",
      "Step 526 -- Train loss: 0.17480862140655518, Train Acc: 0.9423828125 Test Acc: 0.9443359375\n",
      "Step 527 -- Train loss: 0.17407935857772827, Train Acc: 0.94189453125 Test Acc: 0.94140625\n",
      "Step 528 -- Train loss: 0.17536398768424988, Train Acc: 0.941162109375 Test Acc: 0.9384765625\n",
      "Step 529 -- Train loss: 0.17481395602226257, Train Acc: 0.94189453125 Test Acc: 0.9404296875\n",
      "Step 530 -- Train loss: 0.17404061555862427, Train Acc: 0.94287109375 Test Acc: 0.9384765625\n",
      "Step 531 -- Train loss: 0.1746579259634018, Train Acc: 0.94140625 Test Acc: 0.939453125\n",
      "Step 532 -- Train loss: 0.17497996985912323, Train Acc: 0.94091796875 Test Acc: 0.9423828125\n",
      "Step 533 -- Train loss: 0.1745215505361557, Train Acc: 0.94140625 Test Acc: 0.939453125\n",
      "Step 534 -- Train loss: 0.17433686554431915, Train Acc: 0.942138671875 Test Acc: 0.94140625\n",
      "Step 535 -- Train loss: 0.17462363839149475, Train Acc: 0.94140625 Test Acc: 0.9423828125\n",
      "Step 536 -- Train loss: 0.17496512830257416, Train Acc: 0.942138671875 Test Acc: 0.9404296875\n",
      "Step 537 -- Train loss: 0.17475980520248413, Train Acc: 0.9404296875 Test Acc: 0.939453125\n",
      "Step 538 -- Train loss: 0.1744263470172882, Train Acc: 0.94140625 Test Acc: 0.9384765625\n",
      "Step 539 -- Train loss: 0.17413386702537537, Train Acc: 0.942138671875 Test Acc: 0.9375\n",
      "Step 540 -- Train loss: 0.17471864819526672, Train Acc: 0.94189453125 Test Acc: 0.9404296875\n",
      "Step 541 -- Train loss: 0.1739213466644287, Train Acc: 0.94091796875 Test Acc: 0.943359375\n",
      "Step 542 -- Train loss: 0.17586608231067657, Train Acc: 0.94091796875 Test Acc: 0.94140625\n",
      "Step 543 -- Train loss: 0.1749393194913864, Train Acc: 0.94189453125 Test Acc: 0.939453125\n",
      "Step 544 -- Train loss: 0.17458191514015198, Train Acc: 0.940673828125 Test Acc: 0.939453125\n",
      "Step 545 -- Train loss: 0.17425629496574402, Train Acc: 0.94287109375 Test Acc: 0.939453125\n",
      "Step 546 -- Train loss: 0.17376193404197693, Train Acc: 0.942138671875 Test Acc: 0.9404296875\n",
      "Step 547 -- Train loss: 0.1733456403017044, Train Acc: 0.941650390625 Test Acc: 0.9404296875\n",
      "Step 548 -- Train loss: 0.1747598648071289, Train Acc: 0.941650390625 Test Acc: 0.939453125\n",
      "Step 549 -- Train loss: 0.17557671666145325, Train Acc: 0.9404296875 Test Acc: 0.94140625\n",
      "Step 550 -- Train loss: 0.17427890002727509, Train Acc: 0.942626953125 Test Acc: 0.94140625\n",
      "Step 551 -- Train loss: 0.17382119596004486, Train Acc: 0.943359375 Test Acc: 0.9384765625\n",
      "Step 552 -- Train loss: 0.17556174099445343, Train Acc: 0.940185546875 Test Acc: 0.94140625\n",
      "Step 553 -- Train loss: 0.17429372668266296, Train Acc: 0.942626953125 Test Acc: 0.9404296875\n",
      "Step 554 -- Train loss: 0.17455056309700012, Train Acc: 0.941650390625 Test Acc: 0.94140625\n",
      "Step 555 -- Train loss: 0.174658864736557, Train Acc: 0.942626953125 Test Acc: 0.94140625\n",
      "Step 556 -- Train loss: 0.17563770711421967, Train Acc: 0.94140625 Test Acc: 0.9404296875\n",
      "Step 557 -- Train loss: 0.17375198006629944, Train Acc: 0.94140625 Test Acc: 0.94140625\n",
      "Step 558 -- Train loss: 0.1745755672454834, Train Acc: 0.942138671875 Test Acc: 0.9443359375\n",
      "Step 559 -- Train loss: 0.17489390075206757, Train Acc: 0.941650390625 Test Acc: 0.94140625\n",
      "Step 560 -- Train loss: 0.1743336319923401, Train Acc: 0.941650390625 Test Acc: 0.9423828125\n",
      "Step 561 -- Train loss: 0.17500469088554382, Train Acc: 0.941162109375 Test Acc: 0.9384765625\n",
      "Step 562 -- Train loss: 0.17417292296886444, Train Acc: 0.943115234375 Test Acc: 0.94140625\n",
      "Step 563 -- Train loss: 0.17520453035831451, Train Acc: 0.941162109375 Test Acc: 0.9404296875\n",
      "Step 564 -- Train loss: 0.17429140210151672, Train Acc: 0.941650390625 Test Acc: 0.943359375\n",
      "Step 565 -- Train loss: 0.17354103922843933, Train Acc: 0.942626953125 Test Acc: 0.9423828125\n",
      "Step 566 -- Train loss: 0.17380419373512268, Train Acc: 0.94091796875 Test Acc: 0.9443359375\n",
      "Step 567 -- Train loss: 0.17405633628368378, Train Acc: 0.94140625 Test Acc: 0.9404296875\n",
      "Step 568 -- Train loss: 0.17420360445976257, Train Acc: 0.94140625 Test Acc: 0.94140625\n",
      "Step 569 -- Train loss: 0.17448177933692932, Train Acc: 0.942626953125 Test Acc: 0.9443359375\n",
      "Step 570 -- Train loss: 0.17440776526927948, Train Acc: 0.941650390625 Test Acc: 0.94140625\n",
      "Step 571 -- Train loss: 0.17409008741378784, Train Acc: 0.94091796875 Test Acc: 0.943359375\n",
      "Step 572 -- Train loss: 0.17453402280807495, Train Acc: 0.943115234375 Test Acc: 0.9423828125\n",
      "Step 573 -- Train loss: 0.17390896379947662, Train Acc: 0.942138671875 Test Acc: 0.94140625\n",
      "Step 574 -- Train loss: 0.17540723085403442, Train Acc: 0.939453125 Test Acc: 0.94140625\n",
      "Step 575 -- Train loss: 0.17434105277061462, Train Acc: 0.941162109375 Test Acc: 0.9423828125\n",
      "Step 576 -- Train loss: 0.17397016286849976, Train Acc: 0.941650390625 Test Acc: 0.9443359375\n",
      "Step 577 -- Train loss: 0.1738927811384201, Train Acc: 0.94189453125 Test Acc: 0.9453125\n",
      "Step 578 -- Train loss: 0.17400366067886353, Train Acc: 0.941650390625 Test Acc: 0.9384765625\n",
      "Step 579 -- Train loss: 0.17485778033733368, Train Acc: 0.94091796875 Test Acc: 0.9443359375\n",
      "Step 580 -- Train loss: 0.17519070208072662, Train Acc: 0.9404296875 Test Acc: 0.939453125\n",
      "Step 581 -- Train loss: 0.1745406687259674, Train Acc: 0.94189453125 Test Acc: 0.9384765625\n",
      "Step 582 -- Train loss: 0.17356634140014648, Train Acc: 0.941650390625 Test Acc: 0.939453125\n",
      "Step 583 -- Train loss: 0.17417068779468536, Train Acc: 0.9443359375 Test Acc: 0.94140625\n",
      "Step 584 -- Train loss: 0.1743089258670807, Train Acc: 0.94091796875 Test Acc: 0.9462890625\n",
      "Step 585 -- Train loss: 0.17394545674324036, Train Acc: 0.941650390625 Test Acc: 0.943359375\n",
      "Step 586 -- Train loss: 0.17403282225131989, Train Acc: 0.941162109375 Test Acc: 0.939453125\n",
      "Step 587 -- Train loss: 0.1750280112028122, Train Acc: 0.94189453125 Test Acc: 0.939453125\n",
      "Step 588 -- Train loss: 0.17472770810127258, Train Acc: 0.94091796875 Test Acc: 0.94140625\n",
      "Step 589 -- Train loss: 0.17444710433483124, Train Acc: 0.94140625 Test Acc: 0.943359375\n",
      "Step 590 -- Train loss: 0.1744098663330078, Train Acc: 0.94091796875 Test Acc: 0.9404296875\n",
      "Step 591 -- Train loss: 0.1735779345035553, Train Acc: 0.94189453125 Test Acc: 0.9423828125\n",
      "Step 592 -- Train loss: 0.17444927990436554, Train Acc: 0.941650390625 Test Acc: 0.9404296875\n",
      "Step 593 -- Train loss: 0.17467884719371796, Train Acc: 0.941162109375 Test Acc: 0.9423828125\n",
      "Step 594 -- Train loss: 0.1735152155160904, Train Acc: 0.94287109375 Test Acc: 0.94140625\n",
      "Step 595 -- Train loss: 0.17368660867214203, Train Acc: 0.941650390625 Test Acc: 0.9404296875\n",
      "Step 596 -- Train loss: 0.17419520020484924, Train Acc: 0.94189453125 Test Acc: 0.94140625\n",
      "Step 597 -- Train loss: 0.1744115948677063, Train Acc: 0.94287109375 Test Acc: 0.939453125\n",
      "Step 598 -- Train loss: 0.17453059554100037, Train Acc: 0.941162109375 Test Acc: 0.94140625\n",
      "Step 599 -- Train loss: 0.17501240968704224, Train Acc: 0.942138671875 Test Acc: 0.94140625\n",
      "Step 600 -- Train loss: 0.17448922991752625, Train Acc: 0.9404296875 Test Acc: 0.9404296875\n",
      "Step 601 -- Train loss: 0.17436958849430084, Train Acc: 0.9423828125 Test Acc: 0.9443359375\n",
      "Step 602 -- Train loss: 0.1736125499010086, Train Acc: 0.941162109375 Test Acc: 0.9404296875\n",
      "Step 603 -- Train loss: 0.1738135814666748, Train Acc: 0.940673828125 Test Acc: 0.9443359375\n",
      "Step 604 -- Train loss: 0.17449556291103363, Train Acc: 0.940185546875 Test Acc: 0.94140625\n",
      "Step 605 -- Train loss: 0.17446310818195343, Train Acc: 0.94091796875 Test Acc: 0.9375\n",
      "Step 606 -- Train loss: 0.17448528110980988, Train Acc: 0.940673828125 Test Acc: 0.9384765625\n",
      "Step 607 -- Train loss: 0.17420385777950287, Train Acc: 0.94140625 Test Acc: 0.9423828125\n",
      "Step 608 -- Train loss: 0.17357870936393738, Train Acc: 0.94189453125 Test Acc: 0.943359375\n",
      "Step 609 -- Train loss: 0.1739354133605957, Train Acc: 0.94287109375 Test Acc: 0.9384765625\n",
      "Step 610 -- Train loss: 0.1730923354625702, Train Acc: 0.943359375 Test Acc: 0.94140625\n",
      "Step 611 -- Train loss: 0.17540434002876282, Train Acc: 0.939208984375 Test Acc: 0.939453125\n",
      "Step 612 -- Train loss: 0.1741524636745453, Train Acc: 0.941650390625 Test Acc: 0.9404296875\n",
      "Step 613 -- Train loss: 0.1747244894504547, Train Acc: 0.942626953125 Test Acc: 0.9404296875\n",
      "Step 614 -- Train loss: 0.17510713636875153, Train Acc: 0.94189453125 Test Acc: 0.9443359375\n",
      "Step 615 -- Train loss: 0.17418387532234192, Train Acc: 0.942138671875 Test Acc: 0.939453125\n",
      "Step 616 -- Train loss: 0.17413926124572754, Train Acc: 0.942138671875 Test Acc: 0.939453125\n",
      "Step 617 -- Train loss: 0.17409341037273407, Train Acc: 0.9423828125 Test Acc: 0.939453125\n",
      "Step 618 -- Train loss: 0.17383506894111633, Train Acc: 0.93994140625 Test Acc: 0.9462890625\n",
      "Step 619 -- Train loss: 0.17405171692371368, Train Acc: 0.941650390625 Test Acc: 0.9404296875\n",
      "Step 620 -- Train loss: 0.1739487200975418, Train Acc: 0.94189453125 Test Acc: 0.9423828125\n",
      "Step 621 -- Train loss: 0.1736365109682083, Train Acc: 0.943359375 Test Acc: 0.94140625\n",
      "Step 622 -- Train loss: 0.17429378628730774, Train Acc: 0.941162109375 Test Acc: 0.94140625\n",
      "Step 623 -- Train loss: 0.17416352033615112, Train Acc: 0.9423828125 Test Acc: 0.94140625\n",
      "Step 624 -- Train loss: 0.17445847392082214, Train Acc: 0.941650390625 Test Acc: 0.9404296875\n",
      "Step 625 -- Train loss: 0.1732666790485382, Train Acc: 0.942138671875 Test Acc: 0.947265625\n",
      "Step 626 -- Train loss: 0.17364078760147095, Train Acc: 0.942138671875 Test Acc: 0.9384765625\n",
      "Step 627 -- Train loss: 0.17501461505889893, Train Acc: 0.940673828125 Test Acc: 0.9404296875\n",
      "Step 628 -- Train loss: 0.17415228486061096, Train Acc: 0.943359375 Test Acc: 0.9404296875\n",
      "Step 629 -- Train loss: 0.17402544617652893, Train Acc: 0.94189453125 Test Acc: 0.943359375\n",
      "Step 630 -- Train loss: 0.17400161921977997, Train Acc: 0.9404296875 Test Acc: 0.9423828125\n",
      "Step 631 -- Train loss: 0.17384129762649536, Train Acc: 0.942138671875 Test Acc: 0.943359375\n",
      "Step 632 -- Train loss: 0.17430903017520905, Train Acc: 0.94189453125 Test Acc: 0.9423828125\n",
      "Step 633 -- Train loss: 0.17420142889022827, Train Acc: 0.94140625 Test Acc: 0.939453125\n",
      "Step 634 -- Train loss: 0.17408454418182373, Train Acc: 0.9423828125 Test Acc: 0.9404296875\n",
      "Step 635 -- Train loss: 0.17471040785312653, Train Acc: 0.93994140625 Test Acc: 0.9423828125\n",
      "Step 636 -- Train loss: 0.173863023519516, Train Acc: 0.941650390625 Test Acc: 0.9423828125\n",
      "Step 637 -- Train loss: 0.17402595281600952, Train Acc: 0.941650390625 Test Acc: 0.94140625\n",
      "Step 638 -- Train loss: 0.17435245215892792, Train Acc: 0.94287109375 Test Acc: 0.9404296875\n",
      "Step 639 -- Train loss: 0.17469438910484314, Train Acc: 0.941162109375 Test Acc: 0.9462890625\n",
      "Step 640 -- Train loss: 0.1741074025630951, Train Acc: 0.9423828125 Test Acc: 0.939453125\n",
      "Step 641 -- Train loss: 0.17487473785877228, Train Acc: 0.9404296875 Test Acc: 0.9404296875\n",
      "Step 642 -- Train loss: 0.173778235912323, Train Acc: 0.94140625 Test Acc: 0.94140625\n",
      "Step 643 -- Train loss: 0.17369526624679565, Train Acc: 0.9423828125 Test Acc: 0.94140625\n",
      "Step 644 -- Train loss: 0.1744564324617386, Train Acc: 0.941162109375 Test Acc: 0.9404296875\n",
      "Step 645 -- Train loss: 0.1738172173500061, Train Acc: 0.94091796875 Test Acc: 0.939453125\n",
      "Step 646 -- Train loss: 0.17398762702941895, Train Acc: 0.94140625 Test Acc: 0.943359375\n",
      "Step 647 -- Train loss: 0.17392238974571228, Train Acc: 0.941650390625 Test Acc: 0.94140625\n",
      "Step 648 -- Train loss: 0.1750417798757553, Train Acc: 0.94287109375 Test Acc: 0.939453125\n",
      "Step 649 -- Train loss: 0.17397361993789673, Train Acc: 0.943115234375 Test Acc: 0.943359375\n",
      "Step 650 -- Train loss: 0.1745912879705429, Train Acc: 0.94140625 Test Acc: 0.94140625\n",
      "Step 651 -- Train loss: 0.17363637685775757, Train Acc: 0.941650390625 Test Acc: 0.9384765625\n",
      "Step 652 -- Train loss: 0.17406392097473145, Train Acc: 0.942138671875 Test Acc: 0.9423828125\n",
      "Step 653 -- Train loss: 0.1741953194141388, Train Acc: 0.940673828125 Test Acc: 0.9423828125\n",
      "Step 654 -- Train loss: 0.17312227189540863, Train Acc: 0.943603515625 Test Acc: 0.9404296875\n",
      "Step 655 -- Train loss: 0.1746574193239212, Train Acc: 0.940673828125 Test Acc: 0.9384765625\n",
      "Step 656 -- Train loss: 0.17473497986793518, Train Acc: 0.9404296875 Test Acc: 0.9404296875\n",
      "Step 657 -- Train loss: 0.1741185039281845, Train Acc: 0.94140625 Test Acc: 0.9423828125\n",
      "Step 658 -- Train loss: 0.17418447136878967, Train Acc: 0.94091796875 Test Acc: 0.9423828125\n",
      "Step 659 -- Train loss: 0.17371107637882233, Train Acc: 0.9423828125 Test Acc: 0.939453125\n",
      "Step 660 -- Train loss: 0.17460651695728302, Train Acc: 0.93994140625 Test Acc: 0.9404296875\n",
      "Step 661 -- Train loss: 0.17431694269180298, Train Acc: 0.93994140625 Test Acc: 0.939453125\n",
      "Step 662 -- Train loss: 0.17409098148345947, Train Acc: 0.9423828125 Test Acc: 0.9443359375\n",
      "Step 663 -- Train loss: 0.1742253303527832, Train Acc: 0.942138671875 Test Acc: 0.9423828125\n",
      "Step 664 -- Train loss: 0.17415562272071838, Train Acc: 0.943359375 Test Acc: 0.94140625\n",
      "Step 665 -- Train loss: 0.17509441077709198, Train Acc: 0.9404296875 Test Acc: 0.9384765625\n",
      "Step 666 -- Train loss: 0.17417380213737488, Train Acc: 0.941650390625 Test Acc: 0.939453125\n",
      "Step 667 -- Train loss: 0.17445021867752075, Train Acc: 0.94189453125 Test Acc: 0.939453125\n",
      "Step 668 -- Train loss: 0.17424596846103668, Train Acc: 0.940673828125 Test Acc: 0.943359375\n",
      "Step 669 -- Train loss: 0.17369365692138672, Train Acc: 0.94287109375 Test Acc: 0.9384765625\n",
      "Step 670 -- Train loss: 0.1734238564968109, Train Acc: 0.941162109375 Test Acc: 0.9404296875\n",
      "Step 671 -- Train loss: 0.17460863292217255, Train Acc: 0.941650390625 Test Acc: 0.9404296875\n",
      "Step 672 -- Train loss: 0.17390701174736023, Train Acc: 0.94189453125 Test Acc: 0.939453125\n",
      "Step 673 -- Train loss: 0.17309676110744476, Train Acc: 0.94287109375 Test Acc: 0.9404296875\n",
      "Step 674 -- Train loss: 0.17406588792800903, Train Acc: 0.94140625 Test Acc: 0.94140625\n",
      "Step 675 -- Train loss: 0.17428173124790192, Train Acc: 0.941162109375 Test Acc: 0.943359375\n",
      "Step 676 -- Train loss: 0.17356380820274353, Train Acc: 0.9423828125 Test Acc: 0.94140625\n",
      "Step 677 -- Train loss: 0.1739351600408554, Train Acc: 0.9404296875 Test Acc: 0.94140625\n",
      "Step 678 -- Train loss: 0.17332951724529266, Train Acc: 0.941650390625 Test Acc: 0.9375\n",
      "Step 679 -- Train loss: 0.174918994307518, Train Acc: 0.94189453125 Test Acc: 0.94140625\n",
      "Step 680 -- Train loss: 0.1760374903678894, Train Acc: 0.94091796875 Test Acc: 0.9423828125\n",
      "Step 681 -- Train loss: 0.17541874945163727, Train Acc: 0.940185546875 Test Acc: 0.943359375\n",
      "Step 682 -- Train loss: 0.17394806444644928, Train Acc: 0.9423828125 Test Acc: 0.9462890625\n",
      "Step 683 -- Train loss: 0.17389757931232452, Train Acc: 0.94140625 Test Acc: 0.939453125\n",
      "Step 684 -- Train loss: 0.17397081851959229, Train Acc: 0.94189453125 Test Acc: 0.9423828125\n",
      "Step 685 -- Train loss: 0.1748371124267578, Train Acc: 0.9404296875 Test Acc: 0.9453125\n",
      "Step 686 -- Train loss: 0.17415298521518707, Train Acc: 0.94091796875 Test Acc: 0.9404296875\n",
      "Step 687 -- Train loss: 0.17393378913402557, Train Acc: 0.94091796875 Test Acc: 0.94140625\n",
      "Step 688 -- Train loss: 0.17436759173870087, Train Acc: 0.942138671875 Test Acc: 0.9443359375\n",
      "Step 689 -- Train loss: 0.17388367652893066, Train Acc: 0.94091796875 Test Acc: 0.94140625\n",
      "Step 690 -- Train loss: 0.17445391416549683, Train Acc: 0.94287109375 Test Acc: 0.9404296875\n",
      "Step 691 -- Train loss: 0.1740986555814743, Train Acc: 0.94189453125 Test Acc: 0.947265625\n",
      "Step 692 -- Train loss: 0.1733483076095581, Train Acc: 0.942626953125 Test Acc: 0.9404296875\n",
      "Step 693 -- Train loss: 0.17451201379299164, Train Acc: 0.942138671875 Test Acc: 0.943359375\n",
      "Step 694 -- Train loss: 0.17416299879550934, Train Acc: 0.94287109375 Test Acc: 0.9404296875\n",
      "Step 695 -- Train loss: 0.1743609458208084, Train Acc: 0.941162109375 Test Acc: 0.939453125\n",
      "Step 696 -- Train loss: 0.17376728355884552, Train Acc: 0.94189453125 Test Acc: 0.9423828125\n",
      "Step 697 -- Train loss: 0.17433075606822968, Train Acc: 0.942626953125 Test Acc: 0.939453125\n",
      "Step 698 -- Train loss: 0.17430752515792847, Train Acc: 0.94091796875 Test Acc: 0.9423828125\n",
      "Step 699 -- Train loss: 0.17383503913879395, Train Acc: 0.941650390625 Test Acc: 0.9423828125\n",
      "Step 700 -- Train loss: 0.17424407601356506, Train Acc: 0.94091796875 Test Acc: 0.9423828125\n",
      "Step 701 -- Train loss: 0.17409272491931915, Train Acc: 0.94091796875 Test Acc: 0.94140625\n",
      "Step 702 -- Train loss: 0.17369447648525238, Train Acc: 0.9453125 Test Acc: 0.9404296875\n",
      "Step 703 -- Train loss: 0.1749080866575241, Train Acc: 0.940673828125 Test Acc: 0.9375\n",
      "Step 704 -- Train loss: 0.17400015890598297, Train Acc: 0.940673828125 Test Acc: 0.9404296875\n",
      "Step 705 -- Train loss: 0.17470766603946686, Train Acc: 0.939697265625 Test Acc: 0.94140625\n",
      "Step 706 -- Train loss: 0.17406229674816132, Train Acc: 0.94091796875 Test Acc: 0.9443359375\n",
      "Step 707 -- Train loss: 0.17369861900806427, Train Acc: 0.94189453125 Test Acc: 0.94140625\n",
      "Step 708 -- Train loss: 0.17428909242153168, Train Acc: 0.941650390625 Test Acc: 0.9482421875\n",
      "Step 709 -- Train loss: 0.17368249595165253, Train Acc: 0.943115234375 Test Acc: 0.9384765625\n",
      "Step 710 -- Train loss: 0.17399197816848755, Train Acc: 0.943359375 Test Acc: 0.939453125\n",
      "Step 711 -- Train loss: 0.17452627420425415, Train Acc: 0.94091796875 Test Acc: 0.94140625\n",
      "Step 712 -- Train loss: 0.17369696497917175, Train Acc: 0.941162109375 Test Acc: 0.943359375\n",
      "Step 713 -- Train loss: 0.1743207573890686, Train Acc: 0.93994140625 Test Acc: 0.9453125\n",
      "Step 714 -- Train loss: 0.1745954006910324, Train Acc: 0.94287109375 Test Acc: 0.939453125\n",
      "Step 715 -- Train loss: 0.1733511984348297, Train Acc: 0.94140625 Test Acc: 0.9423828125\n",
      "Step 716 -- Train loss: 0.1739715039730072, Train Acc: 0.94189453125 Test Acc: 0.9384765625\n",
      "Step 717 -- Train loss: 0.17444749176502228, Train Acc: 0.94189453125 Test Acc: 0.9443359375\n",
      "Step 718 -- Train loss: 0.1740860641002655, Train Acc: 0.9404296875 Test Acc: 0.9423828125\n",
      "Step 719 -- Train loss: 0.1754883974790573, Train Acc: 0.94189453125 Test Acc: 0.939453125\n",
      "Step 720 -- Train loss: 0.17320020496845245, Train Acc: 0.941650390625 Test Acc: 0.9423828125\n",
      "Step 721 -- Train loss: 0.17557960748672485, Train Acc: 0.941162109375 Test Acc: 0.939453125\n",
      "Step 722 -- Train loss: 0.17412789165973663, Train Acc: 0.941650390625 Test Acc: 0.939453125\n",
      "Step 723 -- Train loss: 0.17446981370449066, Train Acc: 0.941162109375 Test Acc: 0.939453125\n",
      "Step 724 -- Train loss: 0.17398647964000702, Train Acc: 0.9423828125 Test Acc: 0.94140625\n",
      "Step 725 -- Train loss: 0.17418991029262543, Train Acc: 0.94140625 Test Acc: 0.94140625\n",
      "Step 726 -- Train loss: 0.17296470701694489, Train Acc: 0.942138671875 Test Acc: 0.943359375\n",
      "Step 727 -- Train loss: 0.17375479638576508, Train Acc: 0.94140625 Test Acc: 0.939453125\n",
      "Step 728 -- Train loss: 0.17404507100582123, Train Acc: 0.94140625 Test Acc: 0.943359375\n",
      "Step 729 -- Train loss: 0.17412497103214264, Train Acc: 0.941162109375 Test Acc: 0.943359375\n",
      "Step 730 -- Train loss: 0.17554587125778198, Train Acc: 0.941162109375 Test Acc: 0.94140625\n",
      "Step 731 -- Train loss: 0.17426513135433197, Train Acc: 0.9423828125 Test Acc: 0.943359375\n",
      "Step 732 -- Train loss: 0.1746290773153305, Train Acc: 0.94140625 Test Acc: 0.9384765625\n",
      "Step 733 -- Train loss: 0.17396311461925507, Train Acc: 0.941162109375 Test Acc: 0.9375\n",
      "Step 734 -- Train loss: 0.17368632555007935, Train Acc: 0.94189453125 Test Acc: 0.9423828125\n",
      "Step 735 -- Train loss: 0.1744697391986847, Train Acc: 0.94140625 Test Acc: 0.9443359375\n",
      "Step 736 -- Train loss: 0.17393028736114502, Train Acc: 0.942626953125 Test Acc: 0.94140625\n",
      "Step 737 -- Train loss: 0.1737951785326004, Train Acc: 0.94091796875 Test Acc: 0.939453125\n",
      "Step 738 -- Train loss: 0.17375552654266357, Train Acc: 0.942138671875 Test Acc: 0.9384765625\n",
      "Step 739 -- Train loss: 0.17520588636398315, Train Acc: 0.94140625 Test Acc: 0.9462890625\n",
      "Step 740 -- Train loss: 0.1738937348127365, Train Acc: 0.9404296875 Test Acc: 0.9423828125\n",
      "Step 741 -- Train loss: 0.1739954650402069, Train Acc: 0.94140625 Test Acc: 0.9423828125\n",
      "Step 742 -- Train loss: 0.17401330173015594, Train Acc: 0.941162109375 Test Acc: 0.9384765625\n",
      "Step 743 -- Train loss: 0.17373239994049072, Train Acc: 0.94140625 Test Acc: 0.943359375\n",
      "Step 744 -- Train loss: 0.17410604655742645, Train Acc: 0.941650390625 Test Acc: 0.9404296875\n",
      "Step 745 -- Train loss: 0.17418766021728516, Train Acc: 0.942138671875 Test Acc: 0.9443359375\n",
      "Step 746 -- Train loss: 0.1734411120414734, Train Acc: 0.9404296875 Test Acc: 0.939453125\n",
      "Step 747 -- Train loss: 0.17397069931030273, Train Acc: 0.941650390625 Test Acc: 0.94140625\n",
      "Step 748 -- Train loss: 0.17316094040870667, Train Acc: 0.942138671875 Test Acc: 0.9404296875\n",
      "Step 749 -- Train loss: 0.17521049082279205, Train Acc: 0.941650390625 Test Acc: 0.9404296875\n",
      "Step 750 -- Train loss: 0.17460569739341736, Train Acc: 0.94140625 Test Acc: 0.939453125\n",
      "Step 751 -- Train loss: 0.1744987815618515, Train Acc: 0.941162109375 Test Acc: 0.939453125\n",
      "Step 752 -- Train loss: 0.1746167093515396, Train Acc: 0.94091796875 Test Acc: 0.9404296875\n",
      "Step 753 -- Train loss: 0.1740758717060089, Train Acc: 0.94091796875 Test Acc: 0.9453125\n",
      "Step 754 -- Train loss: 0.17393562197685242, Train Acc: 0.94189453125 Test Acc: 0.94140625\n",
      "Step 755 -- Train loss: 0.17389215528964996, Train Acc: 0.94189453125 Test Acc: 0.9443359375\n",
      "Step 756 -- Train loss: 0.17444895207881927, Train Acc: 0.940185546875 Test Acc: 0.9375\n",
      "Step 757 -- Train loss: 0.1744004338979721, Train Acc: 0.9423828125 Test Acc: 0.94140625\n",
      "Step 758 -- Train loss: 0.17430008947849274, Train Acc: 0.940673828125 Test Acc: 0.9404296875\n",
      "Step 759 -- Train loss: 0.17418693006038666, Train Acc: 0.94189453125 Test Acc: 0.943359375\n",
      "Step 760 -- Train loss: 0.1737184226512909, Train Acc: 0.940673828125 Test Acc: 0.9404296875\n",
      "Step 761 -- Train loss: 0.17394541203975677, Train Acc: 0.94189453125 Test Acc: 0.9404296875\n",
      "Step 762 -- Train loss: 0.1740049421787262, Train Acc: 0.94091796875 Test Acc: 0.9423828125\n",
      "Step 763 -- Train loss: 0.17361889779567719, Train Acc: 0.9423828125 Test Acc: 0.9404296875\n",
      "Step 764 -- Train loss: 0.1747664511203766, Train Acc: 0.9423828125 Test Acc: 0.943359375\n",
      "Step 765 -- Train loss: 0.17366617918014526, Train Acc: 0.9423828125 Test Acc: 0.943359375\n",
      "Step 766 -- Train loss: 0.17397820949554443, Train Acc: 0.941162109375 Test Acc: 0.9384765625\n",
      "Step 767 -- Train loss: 0.17473557591438293, Train Acc: 0.941162109375 Test Acc: 0.94140625\n",
      "Step 768 -- Train loss: 0.17361144721508026, Train Acc: 0.941650390625 Test Acc: 0.94140625\n",
      "Step 769 -- Train loss: 0.17372725903987885, Train Acc: 0.9423828125 Test Acc: 0.9404296875\n",
      "Step 770 -- Train loss: 0.1742735207080841, Train Acc: 0.942626953125 Test Acc: 0.9423828125\n",
      "Step 771 -- Train loss: 0.17360690236091614, Train Acc: 0.941650390625 Test Acc: 0.939453125\n",
      "Step 772 -- Train loss: 0.17449696362018585, Train Acc: 0.941650390625 Test Acc: 0.939453125\n",
      "Step 773 -- Train loss: 0.17360380291938782, Train Acc: 0.9423828125 Test Acc: 0.94140625\n",
      "Step 774 -- Train loss: 0.17415103316307068, Train Acc: 0.94140625 Test Acc: 0.9443359375\n",
      "Step 775 -- Train loss: 0.17390532791614532, Train Acc: 0.94091796875 Test Acc: 0.9423828125\n",
      "Step 776 -- Train loss: 0.17356137931346893, Train Acc: 0.94287109375 Test Acc: 0.9384765625\n",
      "Step 777 -- Train loss: 0.1743701696395874, Train Acc: 0.94091796875 Test Acc: 0.947265625\n",
      "Step 778 -- Train loss: 0.1738859862089157, Train Acc: 0.942626953125 Test Acc: 0.9384765625\n",
      "Step 779 -- Train loss: 0.17471331357955933, Train Acc: 0.941162109375 Test Acc: 0.94140625\n",
      "Step 780 -- Train loss: 0.17467100918293, Train Acc: 0.94140625 Test Acc: 0.943359375\n",
      "Step 781 -- Train loss: 0.17467506229877472, Train Acc: 0.94189453125 Test Acc: 0.9404296875\n",
      "Step 782 -- Train loss: 0.17334963381290436, Train Acc: 0.9423828125 Test Acc: 0.939453125\n",
      "Step 783 -- Train loss: 0.1740826964378357, Train Acc: 0.943115234375 Test Acc: 0.9404296875\n",
      "Step 784 -- Train loss: 0.17389200627803802, Train Acc: 0.940673828125 Test Acc: 0.939453125\n",
      "Step 785 -- Train loss: 0.17422990500926971, Train Acc: 0.940185546875 Test Acc: 0.9423828125\n",
      "Step 786 -- Train loss: 0.1738196164369583, Train Acc: 0.94189453125 Test Acc: 0.9423828125\n",
      "Step 787 -- Train loss: 0.17435702681541443, Train Acc: 0.941650390625 Test Acc: 0.94140625\n",
      "Step 788 -- Train loss: 0.1738075613975525, Train Acc: 0.939697265625 Test Acc: 0.9384765625\n",
      "Step 789 -- Train loss: 0.17399628460407257, Train Acc: 0.94140625 Test Acc: 0.9423828125\n",
      "Step 790 -- Train loss: 0.17357489466667175, Train Acc: 0.942626953125 Test Acc: 0.939453125\n",
      "Step 791 -- Train loss: 0.17410023510456085, Train Acc: 0.940185546875 Test Acc: 0.9423828125\n",
      "Step 792 -- Train loss: 0.17441894114017487, Train Acc: 0.942138671875 Test Acc: 0.939453125\n",
      "Step 793 -- Train loss: 0.17371663451194763, Train Acc: 0.9423828125 Test Acc: 0.9443359375\n",
      "Step 794 -- Train loss: 0.17379744350910187, Train Acc: 0.942138671875 Test Acc: 0.94140625\n",
      "Step 795 -- Train loss: 0.17434927821159363, Train Acc: 0.941162109375 Test Acc: 0.939453125\n",
      "Step 796 -- Train loss: 0.17420171201229095, Train Acc: 0.94140625 Test Acc: 0.94140625\n",
      "Step 797 -- Train loss: 0.1745646893978119, Train Acc: 0.940673828125 Test Acc: 0.9423828125\n",
      "Step 798 -- Train loss: 0.17420260608196259, Train Acc: 0.941650390625 Test Acc: 0.94140625\n",
      "Step 799 -- Train loss: 0.17330393195152283, Train Acc: 0.94189453125 Test Acc: 0.94140625\n",
      "Step 800 -- Train loss: 0.17366550862789154, Train Acc: 0.94140625 Test Acc: 0.9423828125\n",
      "Step 801 -- Train loss: 0.17311842739582062, Train Acc: 0.941650390625 Test Acc: 0.9453125\n",
      "Step 802 -- Train loss: 0.17397719621658325, Train Acc: 0.941650390625 Test Acc: 0.9453125\n",
      "Step 803 -- Train loss: 0.1740223914384842, Train Acc: 0.94189453125 Test Acc: 0.94140625\n",
      "Step 804 -- Train loss: 0.17484763264656067, Train Acc: 0.942626953125 Test Acc: 0.94140625\n",
      "Step 805 -- Train loss: 0.1747875213623047, Train Acc: 0.942138671875 Test Acc: 0.939453125\n",
      "Step 806 -- Train loss: 0.1740567982196808, Train Acc: 0.943359375 Test Acc: 0.9404296875\n",
      "Step 807 -- Train loss: 0.17415334284305573, Train Acc: 0.9404296875 Test Acc: 0.9404296875\n",
      "Step 808 -- Train loss: 0.17459391057491302, Train Acc: 0.940185546875 Test Acc: 0.9404296875\n",
      "Step 809 -- Train loss: 0.17427408695220947, Train Acc: 0.9423828125 Test Acc: 0.94140625\n",
      "Step 810 -- Train loss: 0.17498551309108734, Train Acc: 0.94140625 Test Acc: 0.9423828125\n",
      "Step 811 -- Train loss: 0.17335908114910126, Train Acc: 0.943359375 Test Acc: 0.9423828125\n",
      "Step 812 -- Train loss: 0.17350295186042786, Train Acc: 0.94287109375 Test Acc: 0.94140625\n",
      "Step 813 -- Train loss: 0.17436985671520233, Train Acc: 0.941650390625 Test Acc: 0.94140625\n",
      "Step 814 -- Train loss: 0.17524336278438568, Train Acc: 0.941162109375 Test Acc: 0.939453125\n",
      "Step 815 -- Train loss: 0.17445150017738342, Train Acc: 0.94189453125 Test Acc: 0.9453125\n",
      "Step 816 -- Train loss: 0.17477647960186005, Train Acc: 0.941650390625 Test Acc: 0.9404296875\n",
      "Step 817 -- Train loss: 0.1749073565006256, Train Acc: 0.9404296875 Test Acc: 0.9453125\n",
      "Step 818 -- Train loss: 0.17442628741264343, Train Acc: 0.93994140625 Test Acc: 0.9404296875\n",
      "Step 819 -- Train loss: 0.17360356450080872, Train Acc: 0.942626953125 Test Acc: 0.94140625\n",
      "Step 820 -- Train loss: 0.1739416867494583, Train Acc: 0.941650390625 Test Acc: 0.9453125\n",
      "Step 821 -- Train loss: 0.17387260496616364, Train Acc: 0.94140625 Test Acc: 0.94140625\n",
      "Step 822 -- Train loss: 0.17344769835472107, Train Acc: 0.942138671875 Test Acc: 0.939453125\n",
      "Step 823 -- Train loss: 0.17287099361419678, Train Acc: 0.9423828125 Test Acc: 0.9404296875\n",
      "Step 824 -- Train loss: 0.17400877177715302, Train Acc: 0.9423828125 Test Acc: 0.943359375\n",
      "Step 825 -- Train loss: 0.17473502457141876, Train Acc: 0.941162109375 Test Acc: 0.939453125\n",
      "Step 826 -- Train loss: 0.17517393827438354, Train Acc: 0.9423828125 Test Acc: 0.94140625\n",
      "Step 827 -- Train loss: 0.17411524057388306, Train Acc: 0.94091796875 Test Acc: 0.9443359375\n",
      "Step 828 -- Train loss: 0.17484433948993683, Train Acc: 0.94091796875 Test Acc: 0.9423828125\n",
      "Step 829 -- Train loss: 0.17437680065631866, Train Acc: 0.94189453125 Test Acc: 0.94140625\n",
      "Step 830 -- Train loss: 0.1742388755083084, Train Acc: 0.942138671875 Test Acc: 0.943359375\n",
      "Step 831 -- Train loss: 0.1736678034067154, Train Acc: 0.9423828125 Test Acc: 0.9404296875\n",
      "Step 832 -- Train loss: 0.17412488162517548, Train Acc: 0.94091796875 Test Acc: 0.9404296875\n",
      "Step 833 -- Train loss: 0.17383208870887756, Train Acc: 0.941650390625 Test Acc: 0.9384765625\n",
      "Step 834 -- Train loss: 0.17379766702651978, Train Acc: 0.94287109375 Test Acc: 0.9423828125\n",
      "Step 835 -- Train loss: 0.1738126426935196, Train Acc: 0.94287109375 Test Acc: 0.94140625\n",
      "Step 836 -- Train loss: 0.17418572306632996, Train Acc: 0.94091796875 Test Acc: 0.9423828125\n",
      "Step 837 -- Train loss: 0.17425976693630219, Train Acc: 0.940673828125 Test Acc: 0.9404296875\n",
      "Step 838 -- Train loss: 0.17444269359111786, Train Acc: 0.939453125 Test Acc: 0.9423828125\n",
      "Step 839 -- Train loss: 0.17444320023059845, Train Acc: 0.94140625 Test Acc: 0.9443359375\n",
      "Step 840 -- Train loss: 0.1740715205669403, Train Acc: 0.94189453125 Test Acc: 0.939453125\n",
      "Step 841 -- Train loss: 0.17422549426555634, Train Acc: 0.9423828125 Test Acc: 0.94140625\n",
      "Step 842 -- Train loss: 0.17345312237739563, Train Acc: 0.94189453125 Test Acc: 0.94140625\n",
      "Step 843 -- Train loss: 0.17374654114246368, Train Acc: 0.943359375 Test Acc: 0.939453125\n",
      "Step 844 -- Train loss: 0.17442016303539276, Train Acc: 0.942138671875 Test Acc: 0.9462890625\n",
      "Step 845 -- Train loss: 0.17446556687355042, Train Acc: 0.9404296875 Test Acc: 0.9423828125\n",
      "Step 846 -- Train loss: 0.17417092621326447, Train Acc: 0.9423828125 Test Acc: 0.9404296875\n",
      "Step 847 -- Train loss: 0.1732829064130783, Train Acc: 0.9423828125 Test Acc: 0.9404296875\n",
      "Step 848 -- Train loss: 0.17381490767002106, Train Acc: 0.94140625 Test Acc: 0.9404296875\n",
      "Step 849 -- Train loss: 0.17406898736953735, Train Acc: 0.942626953125 Test Acc: 0.9384765625\n",
      "Step 850 -- Train loss: 0.17370541393756866, Train Acc: 0.94140625 Test Acc: 0.94140625\n",
      "Step 851 -- Train loss: 0.17400820553302765, Train Acc: 0.940673828125 Test Acc: 0.943359375\n",
      "Step 852 -- Train loss: 0.1739465445280075, Train Acc: 0.939453125 Test Acc: 0.94140625\n",
      "Step 853 -- Train loss: 0.1736360490322113, Train Acc: 0.942138671875 Test Acc: 0.94140625\n",
      "Step 854 -- Train loss: 0.17466075718402863, Train Acc: 0.941162109375 Test Acc: 0.94140625\n",
      "Step 855 -- Train loss: 0.17336636781692505, Train Acc: 0.940673828125 Test Acc: 0.9404296875\n",
      "Step 856 -- Train loss: 0.17397014796733856, Train Acc: 0.940673828125 Test Acc: 0.94140625\n",
      "Step 857 -- Train loss: 0.17343910038471222, Train Acc: 0.94287109375 Test Acc: 0.943359375\n",
      "Step 858 -- Train loss: 0.17374111711978912, Train Acc: 0.94140625 Test Acc: 0.94140625\n",
      "Step 859 -- Train loss: 0.17461220920085907, Train Acc: 0.940185546875 Test Acc: 0.9404296875\n",
      "Step 860 -- Train loss: 0.17480957508087158, Train Acc: 0.941162109375 Test Acc: 0.94140625\n",
      "Step 861 -- Train loss: 0.1733083426952362, Train Acc: 0.942626953125 Test Acc: 0.94140625\n",
      "Step 862 -- Train loss: 0.1739383339881897, Train Acc: 0.9423828125 Test Acc: 0.939453125\n",
      "Step 863 -- Train loss: 0.17404450476169586, Train Acc: 0.941650390625 Test Acc: 0.939453125\n",
      "Step 864 -- Train loss: 0.17472200095653534, Train Acc: 0.94091796875 Test Acc: 0.943359375\n",
      "Step 865 -- Train loss: 0.1738838404417038, Train Acc: 0.942138671875 Test Acc: 0.9423828125\n",
      "Step 866 -- Train loss: 0.1744302660226822, Train Acc: 0.93994140625 Test Acc: 0.943359375\n",
      "Step 867 -- Train loss: 0.17371314764022827, Train Acc: 0.94140625 Test Acc: 0.94140625\n",
      "Step 868 -- Train loss: 0.17334440350532532, Train Acc: 0.94140625 Test Acc: 0.9423828125\n",
      "Step 869 -- Train loss: 0.17447976768016815, Train Acc: 0.941162109375 Test Acc: 0.9443359375\n",
      "Step 870 -- Train loss: 0.17395682632923126, Train Acc: 0.94189453125 Test Acc: 0.9453125\n",
      "Step 871 -- Train loss: 0.17458508908748627, Train Acc: 0.94287109375 Test Acc: 0.9404296875\n",
      "Step 872 -- Train loss: 0.17437966167926788, Train Acc: 0.941162109375 Test Acc: 0.943359375\n",
      "Step 873 -- Train loss: 0.1749291568994522, Train Acc: 0.941650390625 Test Acc: 0.9423828125\n",
      "Step 874 -- Train loss: 0.17274975776672363, Train Acc: 0.942138671875 Test Acc: 0.939453125\n",
      "Step 875 -- Train loss: 0.17344357073307037, Train Acc: 0.9443359375 Test Acc: 0.9453125\n",
      "Step 876 -- Train loss: 0.17507663369178772, Train Acc: 0.938720703125 Test Acc: 0.943359375\n",
      "Step 877 -- Train loss: 0.17396093904972076, Train Acc: 0.941650390625 Test Acc: 0.9384765625\n",
      "Step 878 -- Train loss: 0.17399762570858002, Train Acc: 0.9423828125 Test Acc: 0.9404296875\n",
      "Step 879 -- Train loss: 0.1736985445022583, Train Acc: 0.941650390625 Test Acc: 0.94140625\n",
      "Step 880 -- Train loss: 0.1737508326768875, Train Acc: 0.941650390625 Test Acc: 0.94140625\n",
      "Step 881 -- Train loss: 0.1737069934606552, Train Acc: 0.94189453125 Test Acc: 0.9404296875\n",
      "Step 882 -- Train loss: 0.17480076849460602, Train Acc: 0.940185546875 Test Acc: 0.94140625\n",
      "Step 883 -- Train loss: 0.17425519227981567, Train Acc: 0.941650390625 Test Acc: 0.94140625\n",
      "Step 884 -- Train loss: 0.1752321571111679, Train Acc: 0.941162109375 Test Acc: 0.9404296875\n",
      "Step 885 -- Train loss: 0.17496398091316223, Train Acc: 0.9404296875 Test Acc: 0.939453125\n",
      "Step 886 -- Train loss: 0.17381906509399414, Train Acc: 0.941650390625 Test Acc: 0.9404296875\n",
      "Step 887 -- Train loss: 0.17339737713336945, Train Acc: 0.94189453125 Test Acc: 0.94140625\n",
      "Step 888 -- Train loss: 0.17400453984737396, Train Acc: 0.94189453125 Test Acc: 0.9404296875\n",
      "Step 889 -- Train loss: 0.17413893342018127, Train Acc: 0.941650390625 Test Acc: 0.9423828125\n",
      "Step 890 -- Train loss: 0.17346513271331787, Train Acc: 0.94189453125 Test Acc: 0.9423828125\n",
      "Step 891 -- Train loss: 0.17455290257930756, Train Acc: 0.939697265625 Test Acc: 0.9404296875\n",
      "Step 892 -- Train loss: 0.17380790412425995, Train Acc: 0.941650390625 Test Acc: 0.943359375\n",
      "Step 893 -- Train loss: 0.17405006289482117, Train Acc: 0.941162109375 Test Acc: 0.9404296875\n",
      "Step 894 -- Train loss: 0.1734846979379654, Train Acc: 0.9423828125 Test Acc: 0.9423828125\n",
      "Step 895 -- Train loss: 0.17417314648628235, Train Acc: 0.942138671875 Test Acc: 0.94140625\n",
      "Step 896 -- Train loss: 0.17363709211349487, Train Acc: 0.942626953125 Test Acc: 0.94140625\n",
      "Step 897 -- Train loss: 0.17440655827522278, Train Acc: 0.9404296875 Test Acc: 0.94140625\n",
      "Step 898 -- Train loss: 0.17427664995193481, Train Acc: 0.940185546875 Test Acc: 0.943359375\n",
      "Step 899 -- Train loss: 0.17407682538032532, Train Acc: 0.941162109375 Test Acc: 0.9423828125\n",
      "Step 900 -- Train loss: 0.1740315556526184, Train Acc: 0.941162109375 Test Acc: 0.9443359375\n",
      "Step 901 -- Train loss: 0.17369458079338074, Train Acc: 0.941162109375 Test Acc: 0.9443359375\n",
      "Step 902 -- Train loss: 0.17407840490341187, Train Acc: 0.9404296875 Test Acc: 0.94140625\n",
      "Step 903 -- Train loss: 0.17449727654457092, Train Acc: 0.94189453125 Test Acc: 0.9423828125\n",
      "Step 904 -- Train loss: 0.17354141175746918, Train Acc: 0.94091796875 Test Acc: 0.94140625\n",
      "Step 905 -- Train loss: 0.17449069023132324, Train Acc: 0.940673828125 Test Acc: 0.9404296875\n",
      "Step 906 -- Train loss: 0.17362897098064423, Train Acc: 0.9423828125 Test Acc: 0.9443359375\n",
      "Step 907 -- Train loss: 0.1742972433567047, Train Acc: 0.941162109375 Test Acc: 0.9423828125\n",
      "Step 908 -- Train loss: 0.17343725264072418, Train Acc: 0.941650390625 Test Acc: 0.9384765625\n",
      "Step 909 -- Train loss: 0.17372065782546997, Train Acc: 0.941650390625 Test Acc: 0.9404296875\n",
      "Step 910 -- Train loss: 0.17502743005752563, Train Acc: 0.93994140625 Test Acc: 0.94140625\n",
      "Step 911 -- Train loss: 0.1738465428352356, Train Acc: 0.942626953125 Test Acc: 0.94140625\n",
      "Step 912 -- Train loss: 0.1736525595188141, Train Acc: 0.94482421875 Test Acc: 0.9404296875\n",
      "Step 913 -- Train loss: 0.1730295568704605, Train Acc: 0.94091796875 Test Acc: 0.94140625\n",
      "Step 914 -- Train loss: 0.17345736920833588, Train Acc: 0.94287109375 Test Acc: 0.943359375\n",
      "Step 915 -- Train loss: 0.17425750195980072, Train Acc: 0.940673828125 Test Acc: 0.9404296875\n",
      "Step 916 -- Train loss: 0.17457197606563568, Train Acc: 0.942138671875 Test Acc: 0.9423828125\n",
      "Step 917 -- Train loss: 0.17413140833377838, Train Acc: 0.94287109375 Test Acc: 0.94140625\n",
      "Step 918 -- Train loss: 0.17528276145458221, Train Acc: 0.939208984375 Test Acc: 0.9423828125\n",
      "Step 919 -- Train loss: 0.17457927763462067, Train Acc: 0.940673828125 Test Acc: 0.9404296875\n",
      "Step 920 -- Train loss: 0.17393074929714203, Train Acc: 0.941650390625 Test Acc: 0.939453125\n",
      "Step 921 -- Train loss: 0.1736890822649002, Train Acc: 0.941650390625 Test Acc: 0.943359375\n",
      "Step 922 -- Train loss: 0.17350304126739502, Train Acc: 0.94140625 Test Acc: 0.9404296875\n",
      "Step 923 -- Train loss: 0.1731976717710495, Train Acc: 0.9404296875 Test Acc: 0.9423828125\n",
      "Step 924 -- Train loss: 0.17474070191383362, Train Acc: 0.940673828125 Test Acc: 0.939453125\n",
      "Step 925 -- Train loss: 0.17440447211265564, Train Acc: 0.941162109375 Test Acc: 0.9404296875\n",
      "Step 926 -- Train loss: 0.17390038073062897, Train Acc: 0.941162109375 Test Acc: 0.9443359375\n",
      "Step 927 -- Train loss: 0.17399877309799194, Train Acc: 0.94091796875 Test Acc: 0.939453125\n",
      "Step 928 -- Train loss: 0.17347532510757446, Train Acc: 0.942626953125 Test Acc: 0.943359375\n",
      "Step 929 -- Train loss: 0.17375317215919495, Train Acc: 0.941650390625 Test Acc: 0.94140625\n",
      "Step 930 -- Train loss: 0.17423252761363983, Train Acc: 0.942626953125 Test Acc: 0.9423828125\n",
      "Step 931 -- Train loss: 0.1753261834383011, Train Acc: 0.94189453125 Test Acc: 0.9384765625\n",
      "Step 932 -- Train loss: 0.17350725829601288, Train Acc: 0.9423828125 Test Acc: 0.939453125\n",
      "Step 933 -- Train loss: 0.17386966943740845, Train Acc: 0.942138671875 Test Acc: 0.943359375\n",
      "Step 934 -- Train loss: 0.17354494333267212, Train Acc: 0.943115234375 Test Acc: 0.9423828125\n",
      "Step 935 -- Train loss: 0.17366883158683777, Train Acc: 0.9423828125 Test Acc: 0.9404296875\n",
      "Step 936 -- Train loss: 0.17405715584754944, Train Acc: 0.94091796875 Test Acc: 0.939453125\n",
      "Step 937 -- Train loss: 0.17369545996189117, Train Acc: 0.942138671875 Test Acc: 0.9384765625\n",
      "Step 938 -- Train loss: 0.17465707659721375, Train Acc: 0.94140625 Test Acc: 0.943359375\n",
      "Step 939 -- Train loss: 0.17364688217639923, Train Acc: 0.941650390625 Test Acc: 0.9404296875\n",
      "Step 940 -- Train loss: 0.1745091676712036, Train Acc: 0.94189453125 Test Acc: 0.9404296875\n",
      "Step 941 -- Train loss: 0.17455501854419708, Train Acc: 0.9423828125 Test Acc: 0.94140625\n",
      "Step 942 -- Train loss: 0.1740688532590866, Train Acc: 0.942626953125 Test Acc: 0.939453125\n",
      "Step 943 -- Train loss: 0.1734888106584549, Train Acc: 0.9423828125 Test Acc: 0.939453125\n",
      "Step 944 -- Train loss: 0.17489761114120483, Train Acc: 0.941162109375 Test Acc: 0.939453125\n",
      "Step 945 -- Train loss: 0.17372047901153564, Train Acc: 0.94287109375 Test Acc: 0.94140625\n",
      "Step 946 -- Train loss: 0.17405319213867188, Train Acc: 0.9423828125 Test Acc: 0.9384765625\n",
      "Step 947 -- Train loss: 0.17341302335262299, Train Acc: 0.941650390625 Test Acc: 0.9443359375\n",
      "Step 948 -- Train loss: 0.17366908490657806, Train Acc: 0.942626953125 Test Acc: 0.9384765625\n",
      "Step 949 -- Train loss: 0.17308688163757324, Train Acc: 0.941162109375 Test Acc: 0.9404296875\n",
      "Step 950 -- Train loss: 0.1740451455116272, Train Acc: 0.94189453125 Test Acc: 0.94140625\n",
      "Step 951 -- Train loss: 0.17337316274642944, Train Acc: 0.9423828125 Test Acc: 0.94140625\n",
      "Step 952 -- Train loss: 0.17399859428405762, Train Acc: 0.94140625 Test Acc: 0.9423828125\n",
      "Step 953 -- Train loss: 0.17500950396060944, Train Acc: 0.94140625 Test Acc: 0.943359375\n",
      "Step 954 -- Train loss: 0.174165278673172, Train Acc: 0.9404296875 Test Acc: 0.94140625\n",
      "Step 955 -- Train loss: 0.17453671991825104, Train Acc: 0.9404296875 Test Acc: 0.939453125\n",
      "Step 956 -- Train loss: 0.17434082925319672, Train Acc: 0.9423828125 Test Acc: 0.9423828125\n",
      "Step 957 -- Train loss: 0.17428988218307495, Train Acc: 0.940185546875 Test Acc: 0.9423828125\n",
      "Step 958 -- Train loss: 0.17490732669830322, Train Acc: 0.941162109375 Test Acc: 0.9404296875\n",
      "Step 959 -- Train loss: 0.17342276871204376, Train Acc: 0.94189453125 Test Acc: 0.9404296875\n",
      "Step 960 -- Train loss: 0.17363695800304413, Train Acc: 0.94140625 Test Acc: 0.9443359375\n",
      "Step 961 -- Train loss: 0.17395013570785522, Train Acc: 0.940673828125 Test Acc: 0.9384765625\n",
      "Step 962 -- Train loss: 0.17332684993743896, Train Acc: 0.941650390625 Test Acc: 0.94140625\n",
      "Step 963 -- Train loss: 0.17462848126888275, Train Acc: 0.9404296875 Test Acc: 0.94140625\n",
      "Step 964 -- Train loss: 0.17445127665996552, Train Acc: 0.940673828125 Test Acc: 0.939453125\n",
      "Step 965 -- Train loss: 0.17442139983177185, Train Acc: 0.93994140625 Test Acc: 0.939453125\n",
      "Step 966 -- Train loss: 0.1745203286409378, Train Acc: 0.941162109375 Test Acc: 0.94140625\n",
      "Step 967 -- Train loss: 0.17354947328567505, Train Acc: 0.94189453125 Test Acc: 0.94140625\n",
      "Step 968 -- Train loss: 0.17398755252361298, Train Acc: 0.9404296875 Test Acc: 0.94140625\n",
      "Step 969 -- Train loss: 0.17447981238365173, Train Acc: 0.941162109375 Test Acc: 0.94140625\n",
      "Step 970 -- Train loss: 0.17353150248527527, Train Acc: 0.94189453125 Test Acc: 0.94140625\n",
      "Step 971 -- Train loss: 0.17458681762218475, Train Acc: 0.940185546875 Test Acc: 0.9453125\n",
      "Step 972 -- Train loss: 0.17412307858467102, Train Acc: 0.94091796875 Test Acc: 0.94140625\n",
      "Step 973 -- Train loss: 0.17207147181034088, Train Acc: 0.943359375 Test Acc: 0.94140625\n",
      "Step 974 -- Train loss: 0.1742076724767685, Train Acc: 0.94140625 Test Acc: 0.94140625\n",
      "Step 975 -- Train loss: 0.17391037940979004, Train Acc: 0.942138671875 Test Acc: 0.939453125\n",
      "Step 976 -- Train loss: 0.17355790734291077, Train Acc: 0.94091796875 Test Acc: 0.94140625\n",
      "Step 977 -- Train loss: 0.17370858788490295, Train Acc: 0.94287109375 Test Acc: 0.939453125\n",
      "Step 978 -- Train loss: 0.17283542454242706, Train Acc: 0.9423828125 Test Acc: 0.94140625\n",
      "Step 979 -- Train loss: 0.17382608354091644, Train Acc: 0.942138671875 Test Acc: 0.94140625\n",
      "Step 980 -- Train loss: 0.17422416806221008, Train Acc: 0.941162109375 Test Acc: 0.94140625\n",
      "Step 981 -- Train loss: 0.17472286522388458, Train Acc: 0.9404296875 Test Acc: 0.939453125\n",
      "Step 982 -- Train loss: 0.1746096909046173, Train Acc: 0.940673828125 Test Acc: 0.9404296875\n",
      "Step 983 -- Train loss: 0.17328497767448425, Train Acc: 0.941650390625 Test Acc: 0.9443359375\n",
      "Step 984 -- Train loss: 0.17440256476402283, Train Acc: 0.9423828125 Test Acc: 0.9384765625\n",
      "Step 985 -- Train loss: 0.17408663034439087, Train Acc: 0.94140625 Test Acc: 0.9384765625\n",
      "Step 986 -- Train loss: 0.1739552766084671, Train Acc: 0.9404296875 Test Acc: 0.939453125\n",
      "Step 987 -- Train loss: 0.17360444366931915, Train Acc: 0.942138671875 Test Acc: 0.9404296875\n",
      "Step 988 -- Train loss: 0.17310374975204468, Train Acc: 0.940185546875 Test Acc: 0.9423828125\n",
      "Step 989 -- Train loss: 0.17464545369148254, Train Acc: 0.941162109375 Test Acc: 0.9404296875\n",
      "Step 990 -- Train loss: 0.17291326820850372, Train Acc: 0.94189453125 Test Acc: 0.943359375\n",
      "Step 991 -- Train loss: 0.17514900863170624, Train Acc: 0.94140625 Test Acc: 0.939453125\n",
      "Step 992 -- Train loss: 0.17435139417648315, Train Acc: 0.942138671875 Test Acc: 0.9453125\n",
      "Step 993 -- Train loss: 0.17400529980659485, Train Acc: 0.93994140625 Test Acc: 0.939453125\n",
      "Step 994 -- Train loss: 0.17398253083229065, Train Acc: 0.94140625 Test Acc: 0.94140625\n",
      "Step 995 -- Train loss: 0.17347431182861328, Train Acc: 0.94140625 Test Acc: 0.943359375\n",
      "Step 996 -- Train loss: 0.17439889907836914, Train Acc: 0.9404296875 Test Acc: 0.9423828125\n",
      "Step 997 -- Train loss: 0.1743794083595276, Train Acc: 0.940185546875 Test Acc: 0.9423828125\n",
      "Step 998 -- Train loss: 0.17409738898277283, Train Acc: 0.9423828125 Test Acc: 0.9404296875\n",
      "Step 999 -- Train loss: 0.1738656610250473, Train Acc: 0.94140625 Test Acc: 0.9404296875\n",
      "Step 1000 -- Train loss: 0.1731366515159607, Train Acc: 0.943359375 Test Acc: 0.9404296875\n",
      "Step 1001 -- Train loss: 0.17430686950683594, Train Acc: 0.94091796875 Test Acc: 0.9404296875\n",
      "Step 1002 -- Train loss: 0.17410151660442352, Train Acc: 0.94189453125 Test Acc: 0.9443359375\n",
      "Step 1003 -- Train loss: 0.17355525493621826, Train Acc: 0.942626953125 Test Acc: 0.939453125\n",
      "Step 1004 -- Train loss: 0.1730944812297821, Train Acc: 0.941162109375 Test Acc: 0.9384765625\n",
      "Step 1005 -- Train loss: 0.17412371933460236, Train Acc: 0.943115234375 Test Acc: 0.9423828125\n",
      "Step 1006 -- Train loss: 0.1728827953338623, Train Acc: 0.94140625 Test Acc: 0.939453125\n",
      "Step 1007 -- Train loss: 0.17489446699619293, Train Acc: 0.941162109375 Test Acc: 0.9384765625\n",
      "Step 1008 -- Train loss: 0.1743544489145279, Train Acc: 0.940673828125 Test Acc: 0.939453125\n",
      "Step 1009 -- Train loss: 0.1739206463098526, Train Acc: 0.942626953125 Test Acc: 0.9423828125\n",
      "Step 1010 -- Train loss: 0.17454639077186584, Train Acc: 0.940673828125 Test Acc: 0.94140625\n",
      "Step 1011 -- Train loss: 0.1733965426683426, Train Acc: 0.94091796875 Test Acc: 0.9462890625\n",
      "Step 1012 -- Train loss: 0.17347091436386108, Train Acc: 0.942138671875 Test Acc: 0.9404296875\n",
      "Step 1013 -- Train loss: 0.1736082285642624, Train Acc: 0.94140625 Test Acc: 0.943359375\n",
      "Step 1014 -- Train loss: 0.1743173450231552, Train Acc: 0.94091796875 Test Acc: 0.9384765625\n",
      "Step 1015 -- Train loss: 0.17441070079803467, Train Acc: 0.94091796875 Test Acc: 0.9453125\n",
      "Step 1016 -- Train loss: 0.17434251308441162, Train Acc: 0.94189453125 Test Acc: 0.9384765625\n",
      "Step 1017 -- Train loss: 0.17253057658672333, Train Acc: 0.943115234375 Test Acc: 0.9423828125\n",
      "Step 1018 -- Train loss: 0.17418049275875092, Train Acc: 0.941650390625 Test Acc: 0.943359375\n",
      "Step 1019 -- Train loss: 0.17313632369041443, Train Acc: 0.94140625 Test Acc: 0.94140625\n",
      "Step 1020 -- Train loss: 0.1747596561908722, Train Acc: 0.941162109375 Test Acc: 0.939453125\n",
      "Step 1021 -- Train loss: 0.1745366007089615, Train Acc: 0.939453125 Test Acc: 0.943359375\n",
      "Step 1022 -- Train loss: 0.17339809238910675, Train Acc: 0.9423828125 Test Acc: 0.94140625\n",
      "Step 1023 -- Train loss: 0.17395198345184326, Train Acc: 0.94287109375 Test Acc: 0.9423828125\n",
      "Step 1024 -- Train loss: 0.1732938438653946, Train Acc: 0.9423828125 Test Acc: 0.9423828125\n",
      "Step 1025 -- Train loss: 0.17373953759670258, Train Acc: 0.94287109375 Test Acc: 0.9404296875\n",
      "Step 1026 -- Train loss: 0.1741425096988678, Train Acc: 0.941650390625 Test Acc: 0.9404296875\n",
      "Step 1027 -- Train loss: 0.17538921535015106, Train Acc: 0.941162109375 Test Acc: 0.939453125\n",
      "Step 1028 -- Train loss: 0.1746072769165039, Train Acc: 0.940673828125 Test Acc: 0.939453125\n",
      "Step 1029 -- Train loss: 0.1738700270652771, Train Acc: 0.942626953125 Test Acc: 0.9443359375\n",
      "Step 1030 -- Train loss: 0.17442667484283447, Train Acc: 0.94140625 Test Acc: 0.943359375\n",
      "Step 1031 -- Train loss: 0.17427310347557068, Train Acc: 0.9423828125 Test Acc: 0.9404296875\n",
      "Step 1032 -- Train loss: 0.17424538731575012, Train Acc: 0.941650390625 Test Acc: 0.9404296875\n",
      "Step 1033 -- Train loss: 0.1731864959001541, Train Acc: 0.94140625 Test Acc: 0.9443359375\n",
      "Step 1034 -- Train loss: 0.17415010929107666, Train Acc: 0.9423828125 Test Acc: 0.9423828125\n",
      "Step 1035 -- Train loss: 0.17262299358844757, Train Acc: 0.9423828125 Test Acc: 0.94140625\n",
      "Step 1036 -- Train loss: 0.17348958551883698, Train Acc: 0.943115234375 Test Acc: 0.94140625\n",
      "Step 1037 -- Train loss: 0.1732441633939743, Train Acc: 0.943603515625 Test Acc: 0.94140625\n",
      "Step 1038 -- Train loss: 0.17458292841911316, Train Acc: 0.941650390625 Test Acc: 0.94140625\n",
      "Step 1039 -- Train loss: 0.17413713037967682, Train Acc: 0.940673828125 Test Acc: 0.9423828125\n",
      "Step 1040 -- Train loss: 0.17406535148620605, Train Acc: 0.942626953125 Test Acc: 0.9384765625\n",
      "Step 1041 -- Train loss: 0.17419452965259552, Train Acc: 0.94140625 Test Acc: 0.9423828125\n",
      "Step 1042 -- Train loss: 0.17468291521072388, Train Acc: 0.940185546875 Test Acc: 0.939453125\n",
      "Step 1043 -- Train loss: 0.17314313352108002, Train Acc: 0.944091796875 Test Acc: 0.9423828125\n",
      "Step 1044 -- Train loss: 0.17340104281902313, Train Acc: 0.942626953125 Test Acc: 0.94140625\n",
      "Step 1045 -- Train loss: 0.17432230710983276, Train Acc: 0.941162109375 Test Acc: 0.9453125\n",
      "Step 1046 -- Train loss: 0.17400069534778595, Train Acc: 0.94189453125 Test Acc: 0.94140625\n",
      "Step 1047 -- Train loss: 0.17408064007759094, Train Acc: 0.94091796875 Test Acc: 0.939453125\n",
      "Step 1048 -- Train loss: 0.173592209815979, Train Acc: 0.94189453125 Test Acc: 0.939453125\n",
      "Step 1049 -- Train loss: 0.1739538311958313, Train Acc: 0.941162109375 Test Acc: 0.9423828125\n",
      "Step 1050 -- Train loss: 0.17338652908802032, Train Acc: 0.943359375 Test Acc: 0.9404296875\n",
      "Step 1051 -- Train loss: 0.17471757531166077, Train Acc: 0.939697265625 Test Acc: 0.939453125\n",
      "Step 1052 -- Train loss: 0.17390167713165283, Train Acc: 0.941650390625 Test Acc: 0.9404296875\n",
      "Step 1053 -- Train loss: 0.17378546297550201, Train Acc: 0.94287109375 Test Acc: 0.9384765625\n",
      "Step 1054 -- Train loss: 0.17315644025802612, Train Acc: 0.942138671875 Test Acc: 0.943359375\n",
      "Step 1055 -- Train loss: 0.17367960512638092, Train Acc: 0.9423828125 Test Acc: 0.9375\n",
      "Step 1056 -- Train loss: 0.17399892210960388, Train Acc: 0.940673828125 Test Acc: 0.9453125\n",
      "Step 1057 -- Train loss: 0.17420677840709686, Train Acc: 0.93994140625 Test Acc: 0.9384765625\n",
      "Step 1058 -- Train loss: 0.1734190434217453, Train Acc: 0.9423828125 Test Acc: 0.94140625\n",
      "Step 1059 -- Train loss: 0.1731104552745819, Train Acc: 0.9423828125 Test Acc: 0.9423828125\n",
      "Step 1060 -- Train loss: 0.17450937628746033, Train Acc: 0.94140625 Test Acc: 0.94140625\n",
      "Step 1061 -- Train loss: 0.17375242710113525, Train Acc: 0.94287109375 Test Acc: 0.9404296875\n",
      "Step 1062 -- Train loss: 0.17420056462287903, Train Acc: 0.942626953125 Test Acc: 0.9375\n",
      "Step 1063 -- Train loss: 0.17390935122966766, Train Acc: 0.93994140625 Test Acc: 0.9423828125\n",
      "Step 1064 -- Train loss: 0.17369817197322845, Train Acc: 0.94189453125 Test Acc: 0.939453125\n",
      "Step 1065 -- Train loss: 0.1734062135219574, Train Acc: 0.940185546875 Test Acc: 0.9443359375\n",
      "Step 1066 -- Train loss: 0.17446023225784302, Train Acc: 0.94091796875 Test Acc: 0.94140625\n",
      "Step 1067 -- Train loss: 0.17425434291362762, Train Acc: 0.94189453125 Test Acc: 0.943359375\n",
      "Step 1068 -- Train loss: 0.17390963435173035, Train Acc: 0.94287109375 Test Acc: 0.9404296875\n",
      "Step 1069 -- Train loss: 0.17373985052108765, Train Acc: 0.941162109375 Test Acc: 0.94140625\n",
      "Step 1070 -- Train loss: 0.17375704646110535, Train Acc: 0.940673828125 Test Acc: 0.939453125\n",
      "Step 1071 -- Train loss: 0.17453908920288086, Train Acc: 0.94140625 Test Acc: 0.9384765625\n",
      "Step 1072 -- Train loss: 0.17393112182617188, Train Acc: 0.94140625 Test Acc: 0.94140625\n",
      "Step 1073 -- Train loss: 0.17399704456329346, Train Acc: 0.940673828125 Test Acc: 0.94140625\n",
      "Step 1074 -- Train loss: 0.1731058657169342, Train Acc: 0.943359375 Test Acc: 0.9404296875\n",
      "Step 1075 -- Train loss: 0.17398220300674438, Train Acc: 0.941650390625 Test Acc: 0.943359375\n",
      "Step 1076 -- Train loss: 0.17422164976596832, Train Acc: 0.9404296875 Test Acc: 0.9423828125\n",
      "Step 1077 -- Train loss: 0.17370259761810303, Train Acc: 0.942138671875 Test Acc: 0.9404296875\n",
      "Step 1078 -- Train loss: 0.1743605136871338, Train Acc: 0.9404296875 Test Acc: 0.94140625\n",
      "Step 1079 -- Train loss: 0.17309169471263885, Train Acc: 0.94287109375 Test Acc: 0.9404296875\n",
      "Step 1080 -- Train loss: 0.17415548861026764, Train Acc: 0.94189453125 Test Acc: 0.9423828125\n",
      "Step 1081 -- Train loss: 0.17467954754829407, Train Acc: 0.9404296875 Test Acc: 0.9404296875\n",
      "Step 1082 -- Train loss: 0.17426611483097076, Train Acc: 0.94140625 Test Acc: 0.9423828125\n",
      "Step 1083 -- Train loss: 0.17377687990665436, Train Acc: 0.941162109375 Test Acc: 0.939453125\n",
      "Step 1084 -- Train loss: 0.17368638515472412, Train Acc: 0.94091796875 Test Acc: 0.94140625\n",
      "Step 1085 -- Train loss: 0.17393194139003754, Train Acc: 0.94140625 Test Acc: 0.9384765625\n",
      "Step 1086 -- Train loss: 0.17325545847415924, Train Acc: 0.94189453125 Test Acc: 0.939453125\n",
      "Step 1087 -- Train loss: 0.17283523082733154, Train Acc: 0.942138671875 Test Acc: 0.943359375\n",
      "Step 1088 -- Train loss: 0.1749826818704605, Train Acc: 0.93994140625 Test Acc: 0.94140625\n",
      "Step 1089 -- Train loss: 0.17362987995147705, Train Acc: 0.941650390625 Test Acc: 0.94140625\n",
      "Step 1090 -- Train loss: 0.17369279265403748, Train Acc: 0.941650390625 Test Acc: 0.9404296875\n",
      "Step 1091 -- Train loss: 0.1737586408853531, Train Acc: 0.941650390625 Test Acc: 0.943359375\n",
      "Step 1092 -- Train loss: 0.17379242181777954, Train Acc: 0.941162109375 Test Acc: 0.94140625\n",
      "Step 1093 -- Train loss: 0.17362435162067413, Train Acc: 0.94287109375 Test Acc: 0.9423828125\n",
      "Step 1094 -- Train loss: 0.17481932044029236, Train Acc: 0.941162109375 Test Acc: 0.9404296875\n",
      "Step 1095 -- Train loss: 0.17365840077400208, Train Acc: 0.94287109375 Test Acc: 0.9384765625\n",
      "Step 1096 -- Train loss: 0.17378756403923035, Train Acc: 0.941650390625 Test Acc: 0.94140625\n",
      "Step 1097 -- Train loss: 0.17400459945201874, Train Acc: 0.94189453125 Test Acc: 0.9404296875\n",
      "Step 1098 -- Train loss: 0.17461344599723816, Train Acc: 0.94140625 Test Acc: 0.939453125\n",
      "Step 1099 -- Train loss: 0.1740361452102661, Train Acc: 0.9404296875 Test Acc: 0.9443359375\n",
      "Step 1100 -- Train loss: 0.1743457317352295, Train Acc: 0.940673828125 Test Acc: 0.947265625\n",
      "Step 1101 -- Train loss: 0.17395132780075073, Train Acc: 0.94091796875 Test Acc: 0.9423828125\n",
      "Step 1102 -- Train loss: 0.17415103316307068, Train Acc: 0.9404296875 Test Acc: 0.94140625\n",
      "Step 1103 -- Train loss: 0.17309890687465668, Train Acc: 0.942626953125 Test Acc: 0.9404296875\n",
      "Step 1104 -- Train loss: 0.1745070219039917, Train Acc: 0.940673828125 Test Acc: 0.9404296875\n",
      "Step 1105 -- Train loss: 0.17351090908050537, Train Acc: 0.941650390625 Test Acc: 0.9423828125\n",
      "Step 1106 -- Train loss: 0.17466171085834503, Train Acc: 0.9404296875 Test Acc: 0.9423828125\n",
      "Step 1107 -- Train loss: 0.17353728413581848, Train Acc: 0.942138671875 Test Acc: 0.9404296875\n",
      "Step 1108 -- Train loss: 0.1741810441017151, Train Acc: 0.94091796875 Test Acc: 0.943359375\n",
      "Step 1109 -- Train loss: 0.17419640719890594, Train Acc: 0.93994140625 Test Acc: 0.9384765625\n",
      "Step 1110 -- Train loss: 0.17429548501968384, Train Acc: 0.942626953125 Test Acc: 0.9443359375\n",
      "Step 1111 -- Train loss: 0.1738705039024353, Train Acc: 0.943603515625 Test Acc: 0.9404296875\n",
      "Step 1112 -- Train loss: 0.17392419278621674, Train Acc: 0.941650390625 Test Acc: 0.9443359375\n",
      "Step 1113 -- Train loss: 0.17428132891654968, Train Acc: 0.938720703125 Test Acc: 0.9384765625\n",
      "Step 1114 -- Train loss: 0.17349755764007568, Train Acc: 0.9404296875 Test Acc: 0.943359375\n",
      "Step 1115 -- Train loss: 0.17381134629249573, Train Acc: 0.941162109375 Test Acc: 0.9443359375\n",
      "Step 1116 -- Train loss: 0.17372815310955048, Train Acc: 0.94091796875 Test Acc: 0.943359375\n",
      "Step 1117 -- Train loss: 0.17429837584495544, Train Acc: 0.940673828125 Test Acc: 0.9384765625\n",
      "Step 1118 -- Train loss: 0.17472879588603973, Train Acc: 0.94140625 Test Acc: 0.9423828125\n",
      "Step 1119 -- Train loss: 0.1742379367351532, Train Acc: 0.941650390625 Test Acc: 0.94140625\n",
      "Step 1120 -- Train loss: 0.1743244081735611, Train Acc: 0.94140625 Test Acc: 0.9453125\n",
      "Step 1121 -- Train loss: 0.17333254218101501, Train Acc: 0.93994140625 Test Acc: 0.939453125\n",
      "Step 1122 -- Train loss: 0.17400424182415009, Train Acc: 0.94140625 Test Acc: 0.94140625\n",
      "Step 1123 -- Train loss: 0.17378631234169006, Train Acc: 0.9423828125 Test Acc: 0.9404296875\n",
      "Step 1124 -- Train loss: 0.17352138459682465, Train Acc: 0.943603515625 Test Acc: 0.9404296875\n",
      "Step 1125 -- Train loss: 0.17364013195037842, Train Acc: 0.941650390625 Test Acc: 0.94140625\n",
      "Step 1126 -- Train loss: 0.17332085967063904, Train Acc: 0.941650390625 Test Acc: 0.943359375\n",
      "Step 1127 -- Train loss: 0.17371200025081635, Train Acc: 0.94140625 Test Acc: 0.9404296875\n",
      "Step 1128 -- Train loss: 0.1743839979171753, Train Acc: 0.94091796875 Test Acc: 0.9384765625\n",
      "Step 1129 -- Train loss: 0.1739538460969925, Train Acc: 0.94189453125 Test Acc: 0.9423828125\n",
      "Step 1130 -- Train loss: 0.17367826402187347, Train Acc: 0.94189453125 Test Acc: 0.939453125\n",
      "Step 1131 -- Train loss: 0.17392253875732422, Train Acc: 0.941650390625 Test Acc: 0.939453125\n",
      "Step 1132 -- Train loss: 0.17386090755462646, Train Acc: 0.94091796875 Test Acc: 0.9404296875\n",
      "Step 1133 -- Train loss: 0.17427413165569305, Train Acc: 0.939453125 Test Acc: 0.943359375\n",
      "Step 1134 -- Train loss: 0.17357636988162994, Train Acc: 0.942138671875 Test Acc: 0.9404296875\n",
      "Step 1135 -- Train loss: 0.17321084439754486, Train Acc: 0.941650390625 Test Acc: 0.94140625\n",
      "Step 1136 -- Train loss: 0.173639178276062, Train Acc: 0.940673828125 Test Acc: 0.939453125\n",
      "Step 1137 -- Train loss: 0.17348513007164001, Train Acc: 0.941162109375 Test Acc: 0.9404296875\n",
      "Step 1138 -- Train loss: 0.17393909394741058, Train Acc: 0.94140625 Test Acc: 0.94140625\n",
      "Step 1139 -- Train loss: 0.17461669445037842, Train Acc: 0.940185546875 Test Acc: 0.9423828125\n",
      "Step 1140 -- Train loss: 0.1730407029390335, Train Acc: 0.94287109375 Test Acc: 0.94140625\n",
      "Step 1141 -- Train loss: 0.1735614687204361, Train Acc: 0.94091796875 Test Acc: 0.9384765625\n",
      "Step 1142 -- Train loss: 0.17408473789691925, Train Acc: 0.94091796875 Test Acc: 0.94140625\n",
      "Step 1143 -- Train loss: 0.17440573871135712, Train Acc: 0.941650390625 Test Acc: 0.9423828125\n",
      "Step 1144 -- Train loss: 0.17404930293560028, Train Acc: 0.940673828125 Test Acc: 0.9423828125\n",
      "Step 1145 -- Train loss: 0.17377984523773193, Train Acc: 0.94287109375 Test Acc: 0.9404296875\n",
      "Step 1146 -- Train loss: 0.1734488308429718, Train Acc: 0.9423828125 Test Acc: 0.939453125\n",
      "Step 1147 -- Train loss: 0.1735115647315979, Train Acc: 0.94140625 Test Acc: 0.939453125\n",
      "Step 1148 -- Train loss: 0.1738889366388321, Train Acc: 0.941162109375 Test Acc: 0.94140625\n",
      "Step 1149 -- Train loss: 0.17303767800331116, Train Acc: 0.94189453125 Test Acc: 0.94140625\n",
      "Step 1150 -- Train loss: 0.173188254237175, Train Acc: 0.94287109375 Test Acc: 0.9375\n",
      "Step 1151 -- Train loss: 0.174546480178833, Train Acc: 0.9423828125 Test Acc: 0.943359375\n",
      "Step 1152 -- Train loss: 0.17294983565807343, Train Acc: 0.944091796875 Test Acc: 0.939453125\n",
      "Step 1153 -- Train loss: 0.1745825707912445, Train Acc: 0.939697265625 Test Acc: 0.939453125\n",
      "Step 1154 -- Train loss: 0.17411287128925323, Train Acc: 0.942138671875 Test Acc: 0.9443359375\n",
      "Step 1155 -- Train loss: 0.17355753481388092, Train Acc: 0.942138671875 Test Acc: 0.9384765625\n",
      "Step 1156 -- Train loss: 0.17445030808448792, Train Acc: 0.94140625 Test Acc: 0.9443359375\n",
      "Step 1157 -- Train loss: 0.1736847460269928, Train Acc: 0.941162109375 Test Acc: 0.947265625\n",
      "Step 1158 -- Train loss: 0.173294797539711, Train Acc: 0.94140625 Test Acc: 0.9423828125\n",
      "Step 1159 -- Train loss: 0.17378737032413483, Train Acc: 0.942138671875 Test Acc: 0.9404296875\n",
      "Step 1160 -- Train loss: 0.17359381914138794, Train Acc: 0.9404296875 Test Acc: 0.9404296875\n",
      "Step 1161 -- Train loss: 0.17414535582065582, Train Acc: 0.940673828125 Test Acc: 0.94140625\n",
      "Step 1162 -- Train loss: 0.17422601580619812, Train Acc: 0.9404296875 Test Acc: 0.9404296875\n",
      "Step 1163 -- Train loss: 0.17313292622566223, Train Acc: 0.940185546875 Test Acc: 0.94140625\n",
      "Step 1164 -- Train loss: 0.17424730956554413, Train Acc: 0.94140625 Test Acc: 0.9443359375\n",
      "Step 1165 -- Train loss: 0.17328479886054993, Train Acc: 0.94189453125 Test Acc: 0.9404296875\n",
      "Step 1166 -- Train loss: 0.17401617765426636, Train Acc: 0.942626953125 Test Acc: 0.943359375\n",
      "Step 1167 -- Train loss: 0.1729971468448639, Train Acc: 0.941162109375 Test Acc: 0.9423828125\n",
      "Step 1168 -- Train loss: 0.17536361515522003, Train Acc: 0.943603515625 Test Acc: 0.9384765625\n",
      "Step 1169 -- Train loss: 0.17360305786132812, Train Acc: 0.941650390625 Test Acc: 0.9423828125\n",
      "Step 1170 -- Train loss: 0.17398501932621002, Train Acc: 0.94189453125 Test Acc: 0.9404296875\n",
      "Step 1171 -- Train loss: 0.17429706454277039, Train Acc: 0.940673828125 Test Acc: 0.94140625\n",
      "Step 1172 -- Train loss: 0.17397479712963104, Train Acc: 0.940185546875 Test Acc: 0.9423828125\n",
      "Step 1173 -- Train loss: 0.1741379350423813, Train Acc: 0.941162109375 Test Acc: 0.9404296875\n",
      "Step 1174 -- Train loss: 0.17453932762145996, Train Acc: 0.94140625 Test Acc: 0.9404296875\n",
      "Step 1175 -- Train loss: 0.17443417012691498, Train Acc: 0.94189453125 Test Acc: 0.9453125\n",
      "Step 1176 -- Train loss: 0.17401719093322754, Train Acc: 0.941162109375 Test Acc: 0.943359375\n",
      "Step 1177 -- Train loss: 0.17465773224830627, Train Acc: 0.938720703125 Test Acc: 0.94140625\n",
      "Step 1178 -- Train loss: 0.17407990992069244, Train Acc: 0.94140625 Test Acc: 0.9404296875\n",
      "Step 1179 -- Train loss: 0.1730545163154602, Train Acc: 0.9443359375 Test Acc: 0.9404296875\n",
      "Step 1180 -- Train loss: 0.17437611520290375, Train Acc: 0.940673828125 Test Acc: 0.9404296875\n",
      "Step 1181 -- Train loss: 0.17418663203716278, Train Acc: 0.94189453125 Test Acc: 0.939453125\n",
      "Step 1182 -- Train loss: 0.17476820945739746, Train Acc: 0.94140625 Test Acc: 0.94140625\n",
      "Step 1183 -- Train loss: 0.17315645515918732, Train Acc: 0.94287109375 Test Acc: 0.9404296875\n",
      "Step 1184 -- Train loss: 0.1741465926170349, Train Acc: 0.941650390625 Test Acc: 0.9423828125\n",
      "Step 1185 -- Train loss: 0.1740761548280716, Train Acc: 0.94140625 Test Acc: 0.9423828125\n",
      "Step 1186 -- Train loss: 0.17410607635974884, Train Acc: 0.941162109375 Test Acc: 0.94140625\n",
      "Step 1187 -- Train loss: 0.17354196310043335, Train Acc: 0.94091796875 Test Acc: 0.943359375\n",
      "Step 1188 -- Train loss: 0.17383547127246857, Train Acc: 0.94140625 Test Acc: 0.943359375\n",
      "Step 1189 -- Train loss: 0.17341701686382294, Train Acc: 0.94287109375 Test Acc: 0.9404296875\n",
      "Step 1190 -- Train loss: 0.17374737560749054, Train Acc: 0.939453125 Test Acc: 0.9404296875\n",
      "Step 1191 -- Train loss: 0.17338457703590393, Train Acc: 0.94384765625 Test Acc: 0.9443359375\n",
      "Step 1192 -- Train loss: 0.17306891083717346, Train Acc: 0.942138671875 Test Acc: 0.943359375\n",
      "Step 1193 -- Train loss: 0.1743311733007431, Train Acc: 0.941162109375 Test Acc: 0.943359375\n",
      "Step 1194 -- Train loss: 0.17344744503498077, Train Acc: 0.942138671875 Test Acc: 0.9453125\n",
      "Step 1195 -- Train loss: 0.1740248054265976, Train Acc: 0.94091796875 Test Acc: 0.9453125\n",
      "Step 1196 -- Train loss: 0.173956960439682, Train Acc: 0.942626953125 Test Acc: 0.9423828125\n",
      "Step 1197 -- Train loss: 0.1734500676393509, Train Acc: 0.942626953125 Test Acc: 0.943359375\n",
      "Step 1198 -- Train loss: 0.17335975170135498, Train Acc: 0.942138671875 Test Acc: 0.9423828125\n",
      "Step 1199 -- Train loss: 0.17370402812957764, Train Acc: 0.943115234375 Test Acc: 0.94140625\n",
      "Step 1200 -- Train loss: 0.17366944253444672, Train Acc: 0.940673828125 Test Acc: 0.939453125\n",
      "Step 1201 -- Train loss: 0.17394837737083435, Train Acc: 0.942138671875 Test Acc: 0.939453125\n",
      "Step 1202 -- Train loss: 0.17334464192390442, Train Acc: 0.94091796875 Test Acc: 0.9404296875\n",
      "Step 1203 -- Train loss: 0.17456983029842377, Train Acc: 0.941162109375 Test Acc: 0.94140625\n",
      "Step 1204 -- Train loss: 0.17344820499420166, Train Acc: 0.94091796875 Test Acc: 0.9384765625\n",
      "Step 1205 -- Train loss: 0.17324812710285187, Train Acc: 0.943115234375 Test Acc: 0.9423828125\n",
      "Step 1206 -- Train loss: 0.17298929393291473, Train Acc: 0.942626953125 Test Acc: 0.943359375\n",
      "Step 1207 -- Train loss: 0.1725919544696808, Train Acc: 0.943115234375 Test Acc: 0.9423828125\n",
      "Step 1208 -- Train loss: 0.17383591830730438, Train Acc: 0.941162109375 Test Acc: 0.9404296875\n",
      "Step 1209 -- Train loss: 0.17455732822418213, Train Acc: 0.94140625 Test Acc: 0.9404296875\n",
      "Step 1210 -- Train loss: 0.17446208000183105, Train Acc: 0.94189453125 Test Acc: 0.943359375\n",
      "Step 1211 -- Train loss: 0.17413592338562012, Train Acc: 0.94140625 Test Acc: 0.94140625\n",
      "Step 1212 -- Train loss: 0.1735982894897461, Train Acc: 0.941650390625 Test Acc: 0.9423828125\n",
      "Step 1213 -- Train loss: 0.17397457361221313, Train Acc: 0.94091796875 Test Acc: 0.939453125\n",
      "Step 1214 -- Train loss: 0.1738460808992386, Train Acc: 0.94091796875 Test Acc: 0.9423828125\n",
      "Step 1215 -- Train loss: 0.1736203283071518, Train Acc: 0.94140625 Test Acc: 0.9404296875\n",
      "Step 1216 -- Train loss: 0.17445439100265503, Train Acc: 0.94287109375 Test Acc: 0.9404296875\n",
      "Step 1217 -- Train loss: 0.17364059388637543, Train Acc: 0.94189453125 Test Acc: 0.9423828125\n",
      "Step 1218 -- Train loss: 0.17385505139827728, Train Acc: 0.94140625 Test Acc: 0.9384765625\n",
      "Step 1219 -- Train loss: 0.17377659678459167, Train Acc: 0.941162109375 Test Acc: 0.9423828125\n",
      "Step 1220 -- Train loss: 0.17393633723258972, Train Acc: 0.942626953125 Test Acc: 0.939453125\n",
      "Step 1221 -- Train loss: 0.17438636720180511, Train Acc: 0.940185546875 Test Acc: 0.94140625\n",
      "Step 1222 -- Train loss: 0.17356756329536438, Train Acc: 0.94140625 Test Acc: 0.9404296875\n",
      "Step 1223 -- Train loss: 0.1733769327402115, Train Acc: 0.941650390625 Test Acc: 0.9404296875\n",
      "Step 1224 -- Train loss: 0.17365063726902008, Train Acc: 0.94140625 Test Acc: 0.94140625\n",
      "Step 1225 -- Train loss: 0.17321990430355072, Train Acc: 0.942138671875 Test Acc: 0.9423828125\n",
      "Step 1226 -- Train loss: 0.1741851568222046, Train Acc: 0.9423828125 Test Acc: 0.94140625\n",
      "Step 1227 -- Train loss: 0.17449671030044556, Train Acc: 0.940673828125 Test Acc: 0.939453125\n",
      "Step 1228 -- Train loss: 0.17362134158611298, Train Acc: 0.941162109375 Test Acc: 0.9423828125\n",
      "Step 1229 -- Train loss: 0.17400121688842773, Train Acc: 0.941162109375 Test Acc: 0.9453125\n",
      "Step 1230 -- Train loss: 0.1736958920955658, Train Acc: 0.9423828125 Test Acc: 0.9404296875\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "config = DotMap(config)\n",
    "\n",
    "config.model.vocab_size = max(config.data.p, config.data.max_num) + 1\n",
    "config.model.block_size = 2 * config.data.num_tokens + 1\n",
    "\n",
    "data_sampler = MovingWindowSum(\n",
    "    min_num=config.data.min_num,\n",
    "    max_num=config.data.max_num,\n",
    "    k=config.data.k,\n",
    "    p=config.data.p,\n",
    ")\n",
    "\n",
    "model = GPTSoftmax(config.model, return_att=True).to(device)\n",
    "optim = Adam(model.parameters(), lr=config.train.lr)\n",
    "\n",
    "if config.train.wandb:\n",
    "    wandb_run_name = 'mws_softmax_attention_loss_noinput_lr0.0005_2kstep'\n",
    "    wandb.login(key=\"\")\n",
    "    wandb.init(project=\"loss_plateau_tf\", name=wandb_run_name, config=config)\n",
    "    wandb.watch(model)\n",
    "\n",
    "for step in range(config.train.num_steps):\n",
    "    train_step(\n",
    "        model=model,\n",
    "        optim=optim,\n",
    "        data_sampler=data_sampler,\n",
    "        step=step,\n",
    "        config=config,\n",
    "    )\n",
    "\n",
    "if config.train.wandb:\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba5e0f9-ca5f-449a-b912-ab65685ae6e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
