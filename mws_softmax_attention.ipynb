{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "343d31a9-529b-444e-a8ae-02753a3804f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import yaml\n",
    "import argparse\n",
    "from dotmap import DotMap\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.optim import Adam\n",
    "from torch.nn.functional import cosine_similarity\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c24b0e8-ad65-4a41-8e7f-558a9778b101",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"./src\")  # make sure Python can find src/\n",
    "from model_softmax import GPTSoftmax\n",
    "from data import MovingWindowSum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5fd65fd6-1b99-46c0-bcf1-8fbe17e2da44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "def train_step(\n",
    "    model,\n",
    "    optim,\n",
    "    data_sampler,\n",
    "    step,\n",
    "    config,\n",
    "):\n",
    "    n_train, n_test, num_tokens = (\n",
    "        config.data.n_train,\n",
    "        config.data.n_test,\n",
    "        config.data.num_tokens,\n",
    "    )\n",
    "\n",
    "    data = data_sampler.sample(\n",
    "        num_samples=n_train + n_test,\n",
    "        num_tokens=num_tokens,\n",
    "    )\n",
    "\n",
    "    train_data = data[:n_train, :]\n",
    "    test_data = data[n_train:, :]\n",
    "\n",
    "    prompt_len = num_tokens + 1\n",
    "    gen_len = num_tokens\n",
    "    acc_start = num_tokens + 1\n",
    "\n",
    "    model.train()\n",
    "    optim.zero_grad(set_to_none=True)\n",
    "\n",
    "    _, _, _, loss = model(\n",
    "        train_data[:, :-1], targets=train_data[:, 1:]\n",
    "    )\n",
    "    loss.backward()\n",
    "\n",
    "    if config.train.grad_clip > 0:\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), config.train.grad_clip)\n",
    "\n",
    "    optim.step()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Log train loss, train / test acc, repetition frequency\n",
    "        attn_map, pre_lm_h, _, train_loss = model(train_data[:, :-1], targets=train_data[:, 1:])\n",
    "\n",
    "        train_pred = model.generate(\n",
    "            idx=train_data[:, :prompt_len],\n",
    "            max_new_tokens=gen_len,\n",
    "        )\n",
    "        test_pred = model.generate(\n",
    "            idx=test_data[:, :prompt_len],\n",
    "            max_new_tokens=gen_len,\n",
    "        )\n",
    "\n",
    "        train_acc = torch.mean(\n",
    "            (train_pred[:, acc_start:] == train_data[:, acc_start:]).to(float)\n",
    "        ).item()\n",
    "        test_acc = torch.mean(\n",
    "            (test_pred[:, acc_start:] == test_data[:, acc_start:]).to(float)\n",
    "        ).item()\n",
    "\n",
    "        data_repeat_frac = torch.mean((test_data[:, acc_start:-1] == test_data[:, acc_start+1:]).to(float))\n",
    "        model_repeat_frac = torch.mean((test_pred[:, acc_start:-1] == test_pred[:, acc_start+1:]).to(float))\n",
    "\n",
    "        # Log attention progress measure\n",
    "        attn_map_output_seq = attn_map[:, :, acc_start-1:]\n",
    "        att_mask = torch.zeros_like(attn_map_output_seq).to(device)\n",
    "\n",
    "        att_mask[:, :, 0, 0] = 1\n",
    "        for i in range(num_tokens - 1):\n",
    "            att_mask[:, :, i + 1, i : i + 2] = 1\n",
    "\n",
    "        att_prog_measure = torch.mean(\n",
    "            torch.sum(torch.abs(attn_map_output_seq) * att_mask, dim=(-3, -2, -1)) /\n",
    "            torch.sum(torch.abs(attn_map_output_seq), dim=(-3, -2, -1)),\n",
    "            dim=0\n",
    "        )\n",
    "\n",
    "        # Log pair-wise cosine similarity between hidden states\n",
    "        embed_start = acc_start - 1\n",
    "        embed_len = gen_len\n",
    "\n",
    "        logit_cs = torch.zeros((embed_len, embed_len))\n",
    "\n",
    "        for i_1 in range(embed_start, embed_start + embed_len):\n",
    "            for i_2 in range(embed_start, i_1):\n",
    "                logit_cs[i_1 - embed_start, i_2 - embed_start] = torch.mean(\n",
    "                    (\n",
    "                        cosine_similarity(\n",
    "                            pre_lm_h[:, i_1, :], pre_lm_h[:, i_2, :], dim=-1\n",
    "                        )\n",
    "                    ), dim=0\n",
    "                )\n",
    "\n",
    "        # Log plots for cosine similarity, attention map\n",
    "        logit_fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(30, 15))\n",
    "\n",
    "        im1 = ax[0].imshow(logit_cs)\n",
    "        ax[0].set_title(\"avg pre_lm_h cosine sim\")\n",
    "        cb1 = logit_fig.colorbar(im1, location=\"right\", shrink=0.99, pad=0.02, ax=ax[0])\n",
    "\n",
    "        avg_attn_map = torch.mean(attn_map, dim=0).squeeze().detach().cpu().numpy()\n",
    "\n",
    "        im2 = ax[1].imshow(avg_attn_map)\n",
    "        ax[1].set_title(\"att map\")\n",
    "        cb4 = logit_fig.colorbar(im2, location=\"right\", shrink=0.99, pad=0.02, ax=ax[1])\n",
    "        ax[1].set_xticks(range(avg_attn_map.shape[-1]))\n",
    "        ax[1].set_yticks(range(avg_attn_map.shape[-2]))\n",
    "\n",
    "        for i1 in range(embed_len):\n",
    "            for i2 in range(embed_len):\n",
    "                text1 = ax[0].text(\n",
    "                    i2,\n",
    "                    i1,\n",
    "                    round(logit_cs[i1, i2].item(), 2),\n",
    "                    ha=\"center\",\n",
    "                    va=\"center\",\n",
    "                    color=\"w\",\n",
    "                )\n",
    "\n",
    "\n",
    "        print(\n",
    "            f\"Step {step} -- Train loss: {train_loss}, Train Acc: {train_acc} Test Acc: {test_acc}\"\n",
    "        )\n",
    "        # print(f\"input: {test_data[0]} \\n predicted:{test_pred[0]}\")\n",
    "\n",
    "        if config.train.wandb:\n",
    "\n",
    "            log_data = {\n",
    "                \"train_loss\": train_loss,\n",
    "                \"train_acc\": train_acc,\n",
    "                \"test_acc\": test_acc,\n",
    "                \"data_repeat_frac\": data_repeat_frac,\n",
    "                \"model_repeat_frac\": model_repeat_frac,\n",
    "                \"att_prog_measure\": att_prog_measure,\n",
    "                \"pre_lm_h_cosine_sim\": logit_fig,\n",
    "                \"mean_cosine_sim\": torch.sum(logit_cs[:, 1:]) / (0.5 * (gen_len-1) * (gen_len-2))\n",
    "            }\n",
    "\n",
    "            for output_pos in range(gen_len):\n",
    "                log_data.update(\n",
    "                    {\n",
    "                        f\"idx{output_pos}_check\": torch.mean(\n",
    "                            (train_pred[:, acc_start + output_pos] == train_data[:, acc_start + output_pos]).to(float)\n",
    "                        ).item()\n",
    "                    }\n",
    "                )\n",
    "\n",
    "                if output_pos < gen_len-1:\n",
    "                    log_data.update(\n",
    "                        {\n",
    "                            f\"mean_cosine_sim_{output_pos}\": torch.sum(logit_cs[:, output_pos]) / (gen_len-1-output_pos)\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "            wandb.log(log_data)\n",
    "\n",
    "        plt.close()\n",
    "        del (\n",
    "            logit_fig,\n",
    "            ax,\n",
    "            logit_cs,\n",
    "        )\n",
    "\n",
    "        if config.train.save_ckpt:\n",
    "            if (step == 0) or ((step + 1) % config.train.ckpt_freq == 0):\n",
    "                model.train()\n",
    "                torch.save(\n",
    "                    {\n",
    "                        \"epoch\": step,\n",
    "                        \"model\": model.state_dict(),\n",
    "                        \"optim\": optim.state_dict(),\n",
    "                        \"train_loss\": train_loss,\n",
    "                        \"test_acc\": test_acc,\n",
    "                    },\n",
    "                    \"./mws_k2_l1_h1_a16_n16.tar\",\n",
    "                )\n",
    "                print(f\"saved state at epoch {step} to {f'./mws_k2_l1_h1_a16_n16.tar'}\")\n",
    "\n",
    "                if config.train.wandb:\n",
    "                    model_wandb = wandb.Artifact(\n",
    "                        f\"model_step{step}\", type=\"model\"\n",
    "                    )\n",
    "                    model_wandb.add_file(f\"./mws_k2_l1_h1_a16_n16.tar\")\n",
    "                    wandb.log_artifact(model_wandb)\n",
    "                    print(\"model uploaded to wandb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "697f626b-eb89-47e1-8c0f-3b8d247b8017",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "config = {\n",
    "'model':\n",
    "  {\n",
    "    'n_layer': 1,\n",
    "    'n_head': 1,\n",
    "    'n_embd': 256,\n",
    "    'linear': True,\n",
    "  },\n",
    "\n",
    "'data':\n",
    "  {\n",
    "    'name': 'window',\n",
    "    'min_num': 1,\n",
    "    'max_num': 16,\n",
    "    'k': 2,\n",
    "    'p': 17,\n",
    "    'sep': 17,\n",
    "    'cot': False,\n",
    "    'num_tokens': 16,\n",
    "    'n_train': 256,\n",
    "    'n_test': 64,\n",
    "    'fixed_len': True,\n",
    "  },\n",
    "\n",
    "'train':\n",
    "  {\n",
    "    'lr': 0.0001,\n",
    "    'grad_clip': -1,\n",
    "    'num_steps': 1000,\n",
    "    'norm_type': \"none_rank\",\n",
    "    'wandb': True,\n",
    "    'save_ckpt': False,\n",
    "    'ckpt_freq': 20,\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ffb76f-4348-4914-8a05-2da5464ba155",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f45111d2-5e03-4980-8621-5a78b871c468",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.22.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jyue/private/tf-loss-plateau/wandb/run-20251010_003520-11gdga8l</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/wth_ucsd/loss_plateau_tf/runs/11gdga8l' target=\"_blank\">mws_softmax_attention_1000_steps</a></strong> to <a href='https://wandb.ai/wth_ucsd/loss_plateau_tf' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/wth_ucsd/loss_plateau_tf' target=\"_blank\">https://wandb.ai/wth_ucsd/loss_plateau_tf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/wth_ucsd/loss_plateau_tf/runs/11gdga8l' target=\"_blank\">https://wandb.ai/wth_ucsd/loss_plateau_tf/runs/11gdga8l</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0 -- Train loss: 2.889148473739624, Train Acc: 0.059326171875 Test Acc: 0.0517578125\n",
      "Step 1 -- Train loss: 2.8604555130004883, Train Acc: 0.06689453125 Test Acc: 0.0576171875\n",
      "Step 2 -- Train loss: 2.8493106365203857, Train Acc: 0.0615234375 Test Acc: 0.0576171875\n",
      "Step 3 -- Train loss: 2.826770067214966, Train Acc: 0.063232421875 Test Acc: 0.060546875\n",
      "Step 4 -- Train loss: 2.8070619106292725, Train Acc: 0.061767578125 Test Acc: 0.0595703125\n",
      "Step 5 -- Train loss: 2.797037124633789, Train Acc: 0.0625 Test Acc: 0.0615234375\n",
      "Step 6 -- Train loss: 2.780869483947754, Train Acc: 0.060791015625 Test Acc: 0.0537109375\n",
      "Step 7 -- Train loss: 2.766249179840088, Train Acc: 0.059814453125 Test Acc: 0.064453125\n",
      "Step 8 -- Train loss: 2.760578155517578, Train Acc: 0.063232421875 Test Acc: 0.0546875\n",
      "Step 9 -- Train loss: 2.756134271621704, Train Acc: 0.057861328125 Test Acc: 0.0556640625\n",
      "Step 10 -- Train loss: 2.7473807334899902, Train Acc: 0.067626953125 Test Acc: 0.072265625\n",
      "Step 11 -- Train loss: 2.7451953887939453, Train Acc: 0.06591796875 Test Acc: 0.07421875\n",
      "Step 12 -- Train loss: 2.741246461868286, Train Acc: 0.059814453125 Test Acc: 0.0556640625\n",
      "Step 13 -- Train loss: 2.7352981567382812, Train Acc: 0.060791015625 Test Acc: 0.0498046875\n",
      "Step 14 -- Train loss: 2.732285499572754, Train Acc: 0.057373046875 Test Acc: 0.0732421875\n",
      "Step 15 -- Train loss: 2.7338814735412598, Train Acc: 0.0517578125 Test Acc: 0.0576171875\n",
      "Step 16 -- Train loss: 2.7300190925598145, Train Acc: 0.064453125 Test Acc: 0.05078125\n",
      "Step 17 -- Train loss: 2.7319486141204834, Train Acc: 0.059814453125 Test Acc: 0.0673828125\n",
      "Step 18 -- Train loss: 2.729008913040161, Train Acc: 0.06103515625 Test Acc: 0.0634765625\n",
      "Step 19 -- Train loss: 2.726685047149658, Train Acc: 0.071533203125 Test Acc: 0.0546875\n",
      "Step 20 -- Train loss: 2.725562572479248, Train Acc: 0.066650390625 Test Acc: 0.0625\n",
      "Step 21 -- Train loss: 2.72550630569458, Train Acc: 0.057861328125 Test Acc: 0.0634765625\n",
      "Step 22 -- Train loss: 2.725203275680542, Train Acc: 0.062255859375 Test Acc: 0.0556640625\n",
      "Step 23 -- Train loss: 2.7245371341705322, Train Acc: 0.057861328125 Test Acc: 0.0546875\n",
      "Step 24 -- Train loss: 2.725440740585327, Train Acc: 0.06298828125 Test Acc: 0.0478515625\n",
      "Step 25 -- Train loss: 2.7238497734069824, Train Acc: 0.06689453125 Test Acc: 0.0556640625\n",
      "Step 26 -- Train loss: 2.723846435546875, Train Acc: 0.053955078125 Test Acc: 0.0576171875\n",
      "Step 27 -- Train loss: 2.723670721054077, Train Acc: 0.0634765625 Test Acc: 0.0546875\n",
      "Step 28 -- Train loss: 2.7227423191070557, Train Acc: 0.068115234375 Test Acc: 0.0576171875\n",
      "Step 29 -- Train loss: 2.7230358123779297, Train Acc: 0.0595703125 Test Acc: 0.064453125\n",
      "Step 30 -- Train loss: 2.7211549282073975, Train Acc: 0.055908203125 Test Acc: 0.0556640625\n",
      "Step 31 -- Train loss: 2.7213118076324463, Train Acc: 0.059326171875 Test Acc: 0.060546875\n",
      "Step 32 -- Train loss: 2.722883939743042, Train Acc: 0.059326171875 Test Acc: 0.052734375\n",
      "Step 33 -- Train loss: 2.72194242477417, Train Acc: 0.06103515625 Test Acc: 0.0732421875\n",
      "Step 34 -- Train loss: 2.7205801010131836, Train Acc: 0.060791015625 Test Acc: 0.0634765625\n",
      "Step 35 -- Train loss: 2.7219815254211426, Train Acc: 0.0673828125 Test Acc: 0.0546875\n",
      "Step 36 -- Train loss: 2.7206709384918213, Train Acc: 0.06103515625 Test Acc: 0.056640625\n",
      "Step 37 -- Train loss: 2.720292806625366, Train Acc: 0.056640625 Test Acc: 0.068359375\n",
      "Step 38 -- Train loss: 2.720473051071167, Train Acc: 0.058837890625 Test Acc: 0.056640625\n",
      "Step 39 -- Train loss: 2.719348669052124, Train Acc: 0.065673828125 Test Acc: 0.060546875\n",
      "Step 40 -- Train loss: 2.721222162246704, Train Acc: 0.059326171875 Test Acc: 0.0498046875\n",
      "Step 41 -- Train loss: 2.720393180847168, Train Acc: 0.0595703125 Test Acc: 0.0595703125\n",
      "Step 42 -- Train loss: 2.7194809913635254, Train Acc: 0.058349609375 Test Acc: 0.0576171875\n",
      "Step 43 -- Train loss: 2.7180473804473877, Train Acc: 0.067138671875 Test Acc: 0.0634765625\n",
      "Step 44 -- Train loss: 2.720566987991333, Train Acc: 0.060302734375 Test Acc: 0.0634765625\n",
      "Step 45 -- Train loss: 2.719135046005249, Train Acc: 0.060302734375 Test Acc: 0.064453125\n",
      "Step 46 -- Train loss: 2.720080852508545, Train Acc: 0.063720703125 Test Acc: 0.0693359375\n",
      "Step 47 -- Train loss: 2.72100830078125, Train Acc: 0.06396484375 Test Acc: 0.068359375\n",
      "Step 48 -- Train loss: 2.7198846340179443, Train Acc: 0.059326171875 Test Acc: 0.0732421875\n",
      "Step 49 -- Train loss: 2.71842622756958, Train Acc: 0.064697265625 Test Acc: 0.046875\n",
      "Step 50 -- Train loss: 2.7186784744262695, Train Acc: 0.056884765625 Test Acc: 0.0634765625\n",
      "Step 51 -- Train loss: 2.7191572189331055, Train Acc: 0.06201171875 Test Acc: 0.0517578125\n",
      "Step 52 -- Train loss: 2.7190628051757812, Train Acc: 0.061767578125 Test Acc: 0.06640625\n",
      "Step 53 -- Train loss: 2.718574047088623, Train Acc: 0.06201171875 Test Acc: 0.056640625\n",
      "Step 54 -- Train loss: 2.720088005065918, Train Acc: 0.0634765625 Test Acc: 0.06640625\n",
      "Step 55 -- Train loss: 2.719177484512329, Train Acc: 0.0625 Test Acc: 0.0771484375\n",
      "Step 56 -- Train loss: 2.7191059589385986, Train Acc: 0.06201171875 Test Acc: 0.0537109375\n",
      "Step 57 -- Train loss: 2.719621181488037, Train Acc: 0.0576171875 Test Acc: 0.064453125\n",
      "Step 58 -- Train loss: 2.717916488647461, Train Acc: 0.062255859375 Test Acc: 0.0439453125\n",
      "Step 59 -- Train loss: 2.717910051345825, Train Acc: 0.0576171875 Test Acc: 0.0751953125\n",
      "Step 60 -- Train loss: 2.718573570251465, Train Acc: 0.0703125 Test Acc: 0.068359375\n",
      "Step 61 -- Train loss: 2.719341993331909, Train Acc: 0.05908203125 Test Acc: 0.0615234375\n",
      "Step 62 -- Train loss: 2.717818021774292, Train Acc: 0.060302734375 Test Acc: 0.064453125\n",
      "Step 63 -- Train loss: 2.71783447265625, Train Acc: 0.05322265625 Test Acc: 0.0595703125\n",
      "Step 64 -- Train loss: 2.7187604904174805, Train Acc: 0.066650390625 Test Acc: 0.0712890625\n",
      "Step 65 -- Train loss: 2.719029426574707, Train Acc: 0.05712890625 Test Acc: 0.064453125\n",
      "Step 66 -- Train loss: 2.717484474182129, Train Acc: 0.064453125 Test Acc: 0.05859375\n",
      "Step 67 -- Train loss: 2.7183966636657715, Train Acc: 0.061279296875 Test Acc: 0.068359375\n",
      "Step 68 -- Train loss: 2.7195956707000732, Train Acc: 0.060546875 Test Acc: 0.0615234375\n",
      "Step 69 -- Train loss: 2.7182347774505615, Train Acc: 0.060302734375 Test Acc: 0.0537109375\n",
      "Step 70 -- Train loss: 2.7180936336517334, Train Acc: 0.059326171875 Test Acc: 0.0634765625\n",
      "Step 71 -- Train loss: 2.7178146839141846, Train Acc: 0.067138671875 Test Acc: 0.0712890625\n",
      "Step 72 -- Train loss: 2.7176473140716553, Train Acc: 0.063720703125 Test Acc: 0.0654296875\n",
      "Step 73 -- Train loss: 2.7184674739837646, Train Acc: 0.061279296875 Test Acc: 0.0458984375\n",
      "Step 74 -- Train loss: 2.7189691066741943, Train Acc: 0.061279296875 Test Acc: 0.0693359375\n",
      "Step 75 -- Train loss: 2.7176194190979004, Train Acc: 0.059814453125 Test Acc: 0.064453125\n",
      "Step 76 -- Train loss: 2.7173383235931396, Train Acc: 0.065185546875 Test Acc: 0.0654296875\n",
      "Step 77 -- Train loss: 2.7157297134399414, Train Acc: 0.068115234375 Test Acc: 0.0654296875\n",
      "Step 78 -- Train loss: 2.7195069789886475, Train Acc: 0.06201171875 Test Acc: 0.0673828125\n",
      "Step 79 -- Train loss: 2.7185449600219727, Train Acc: 0.058837890625 Test Acc: 0.0576171875\n",
      "Step 80 -- Train loss: 2.718954086303711, Train Acc: 0.05908203125 Test Acc: 0.060546875\n",
      "Step 81 -- Train loss: 2.718803882598877, Train Acc: 0.056396484375 Test Acc: 0.0537109375\n",
      "Step 82 -- Train loss: 2.716190814971924, Train Acc: 0.063232421875 Test Acc: 0.060546875\n",
      "Step 83 -- Train loss: 2.716710329055786, Train Acc: 0.066650390625 Test Acc: 0.0498046875\n",
      "Step 84 -- Train loss: 2.717268943786621, Train Acc: 0.060302734375 Test Acc: 0.05859375\n",
      "Step 85 -- Train loss: 2.7173383235931396, Train Acc: 0.055908203125 Test Acc: 0.060546875\n",
      "Step 86 -- Train loss: 2.717566728591919, Train Acc: 0.064453125 Test Acc: 0.0615234375\n",
      "Step 87 -- Train loss: 2.718061685562134, Train Acc: 0.065185546875 Test Acc: 0.06640625\n",
      "Step 88 -- Train loss: 2.717968702316284, Train Acc: 0.0625 Test Acc: 0.0517578125\n",
      "Step 89 -- Train loss: 2.717374086380005, Train Acc: 0.060302734375 Test Acc: 0.05078125\n",
      "Step 90 -- Train loss: 2.718228578567505, Train Acc: 0.058349609375 Test Acc: 0.0537109375\n",
      "Step 91 -- Train loss: 2.7182374000549316, Train Acc: 0.06689453125 Test Acc: 0.052734375\n",
      "Step 92 -- Train loss: 2.7166237831115723, Train Acc: 0.069091796875 Test Acc: 0.046875\n",
      "Step 93 -- Train loss: 2.7182905673980713, Train Acc: 0.06103515625 Test Acc: 0.072265625\n",
      "Step 94 -- Train loss: 2.7168049812316895, Train Acc: 0.063720703125 Test Acc: 0.0625\n",
      "Step 95 -- Train loss: 2.71698260307312, Train Acc: 0.063720703125 Test Acc: 0.056640625\n",
      "Step 96 -- Train loss: 2.7150933742523193, Train Acc: 0.06591796875 Test Acc: 0.06640625\n",
      "Step 97 -- Train loss: 2.7162413597106934, Train Acc: 0.057861328125 Test Acc: 0.0498046875\n",
      "Step 98 -- Train loss: 2.716783046722412, Train Acc: 0.06689453125 Test Acc: 0.0546875\n",
      "Step 99 -- Train loss: 2.717855930328369, Train Acc: 0.062255859375 Test Acc: 0.0703125\n",
      "Step 100 -- Train loss: 2.7170627117156982, Train Acc: 0.060546875 Test Acc: 0.0517578125\n",
      "Step 101 -- Train loss: 2.716484785079956, Train Acc: 0.0537109375 Test Acc: 0.05078125\n",
      "Step 102 -- Train loss: 2.7154123783111572, Train Acc: 0.065185546875 Test Acc: 0.0625\n",
      "Step 103 -- Train loss: 2.7169792652130127, Train Acc: 0.063720703125 Test Acc: 0.06640625\n",
      "Step 104 -- Train loss: 2.716346025466919, Train Acc: 0.058837890625 Test Acc: 0.05859375\n",
      "Step 105 -- Train loss: 2.7164430618286133, Train Acc: 0.065185546875 Test Acc: 0.0517578125\n",
      "Step 106 -- Train loss: 2.7170543670654297, Train Acc: 0.068359375 Test Acc: 0.0751953125\n",
      "Step 107 -- Train loss: 2.7169370651245117, Train Acc: 0.0634765625 Test Acc: 0.0625\n",
      "Step 108 -- Train loss: 2.716895341873169, Train Acc: 0.066650390625 Test Acc: 0.0556640625\n",
      "Step 109 -- Train loss: 2.7166941165924072, Train Acc: 0.061279296875 Test Acc: 0.0595703125\n",
      "Step 110 -- Train loss: 2.716885805130005, Train Acc: 0.0654296875 Test Acc: 0.068359375\n",
      "Step 111 -- Train loss: 2.717475414276123, Train Acc: 0.053955078125 Test Acc: 0.0546875\n",
      "Step 112 -- Train loss: 2.715843677520752, Train Acc: 0.06396484375 Test Acc: 0.0615234375\n",
      "Step 113 -- Train loss: 2.715843439102173, Train Acc: 0.063232421875 Test Acc: 0.05859375\n",
      "Step 114 -- Train loss: 2.7180919647216797, Train Acc: 0.060302734375 Test Acc: 0.0546875\n",
      "Step 115 -- Train loss: 2.7166624069213867, Train Acc: 0.0615234375 Test Acc: 0.0556640625\n",
      "Step 116 -- Train loss: 2.718172550201416, Train Acc: 0.064453125 Test Acc: 0.064453125\n",
      "Step 117 -- Train loss: 2.716792583465576, Train Acc: 0.065673828125 Test Acc: 0.0625\n",
      "Step 118 -- Train loss: 2.717116117477417, Train Acc: 0.059814453125 Test Acc: 0.06640625\n",
      "Step 119 -- Train loss: 2.716876983642578, Train Acc: 0.054931640625 Test Acc: 0.052734375\n",
      "Step 120 -- Train loss: 2.7153725624084473, Train Acc: 0.068359375 Test Acc: 0.06640625\n",
      "Step 121 -- Train loss: 2.7146968841552734, Train Acc: 0.0625 Test Acc: 0.064453125\n",
      "Step 122 -- Train loss: 2.715404748916626, Train Acc: 0.061767578125 Test Acc: 0.064453125\n",
      "Step 123 -- Train loss: 2.716247081756592, Train Acc: 0.067626953125 Test Acc: 0.0703125\n",
      "Step 124 -- Train loss: 2.7173073291778564, Train Acc: 0.06640625 Test Acc: 0.0595703125\n",
      "Step 125 -- Train loss: 2.716827630996704, Train Acc: 0.055908203125 Test Acc: 0.0625\n",
      "Step 126 -- Train loss: 2.7161948680877686, Train Acc: 0.062255859375 Test Acc: 0.0546875\n",
      "Step 127 -- Train loss: 2.7149486541748047, Train Acc: 0.060546875 Test Acc: 0.0625\n",
      "Step 128 -- Train loss: 2.715587854385376, Train Acc: 0.06396484375 Test Acc: 0.05078125\n",
      "Step 129 -- Train loss: 2.7163913249969482, Train Acc: 0.067138671875 Test Acc: 0.0615234375\n",
      "Step 130 -- Train loss: 2.715818166732788, Train Acc: 0.057861328125 Test Acc: 0.0625\n",
      "Step 131 -- Train loss: 2.717055559158325, Train Acc: 0.0595703125 Test Acc: 0.068359375\n",
      "Step 132 -- Train loss: 2.7159950733184814, Train Acc: 0.066650390625 Test Acc: 0.0556640625\n",
      "Step 133 -- Train loss: 2.7152135372161865, Train Acc: 0.061767578125 Test Acc: 0.064453125\n",
      "Step 134 -- Train loss: 2.7168221473693848, Train Acc: 0.06201171875 Test Acc: 0.0576171875\n",
      "Step 135 -- Train loss: 2.7151381969451904, Train Acc: 0.06591796875 Test Acc: 0.0615234375\n",
      "Step 136 -- Train loss: 2.717167377471924, Train Acc: 0.061279296875 Test Acc: 0.0625\n",
      "Step 137 -- Train loss: 2.716076612472534, Train Acc: 0.065673828125 Test Acc: 0.0771484375\n",
      "Step 138 -- Train loss: 2.716055154800415, Train Acc: 0.0537109375 Test Acc: 0.06640625\n",
      "Step 139 -- Train loss: 2.7161078453063965, Train Acc: 0.064208984375 Test Acc: 0.078125\n",
      "Step 140 -- Train loss: 2.7147090435028076, Train Acc: 0.063720703125 Test Acc: 0.0615234375\n",
      "Step 141 -- Train loss: 2.715902328491211, Train Acc: 0.06201171875 Test Acc: 0.060546875\n",
      "Step 142 -- Train loss: 2.715569257736206, Train Acc: 0.060546875 Test Acc: 0.060546875\n",
      "Step 143 -- Train loss: 2.7151780128479004, Train Acc: 0.066162109375 Test Acc: 0.05078125\n",
      "Step 144 -- Train loss: 2.714987277984619, Train Acc: 0.0615234375 Test Acc: 0.0517578125\n",
      "Step 145 -- Train loss: 2.7148325443267822, Train Acc: 0.061767578125 Test Acc: 0.0576171875\n",
      "Step 146 -- Train loss: 2.7160520553588867, Train Acc: 0.06591796875 Test Acc: 0.0810546875\n",
      "Step 147 -- Train loss: 2.717010259628296, Train Acc: 0.05859375 Test Acc: 0.0537109375\n",
      "Step 148 -- Train loss: 2.7163219451904297, Train Acc: 0.067138671875 Test Acc: 0.0673828125\n",
      "Step 149 -- Train loss: 2.7154691219329834, Train Acc: 0.070556640625 Test Acc: 0.064453125\n",
      "Step 150 -- Train loss: 2.713940382003784, Train Acc: 0.0595703125 Test Acc: 0.0654296875\n",
      "Step 151 -- Train loss: 2.7153916358947754, Train Acc: 0.059326171875 Test Acc: 0.0576171875\n",
      "Step 152 -- Train loss: 2.7153406143188477, Train Acc: 0.067626953125 Test Acc: 0.0595703125\n",
      "Step 153 -- Train loss: 2.715090036392212, Train Acc: 0.06591796875 Test Acc: 0.0517578125\n",
      "Step 154 -- Train loss: 2.7151193618774414, Train Acc: 0.06298828125 Test Acc: 0.056640625\n",
      "Step 155 -- Train loss: 2.714609146118164, Train Acc: 0.05859375 Test Acc: 0.052734375\n",
      "Step 156 -- Train loss: 2.715045690536499, Train Acc: 0.06591796875 Test Acc: 0.064453125\n",
      "Step 157 -- Train loss: 2.7146337032318115, Train Acc: 0.063232421875 Test Acc: 0.0654296875\n",
      "Step 158 -- Train loss: 2.7147390842437744, Train Acc: 0.0625 Test Acc: 0.0673828125\n",
      "Step 159 -- Train loss: 2.713887929916382, Train Acc: 0.068115234375 Test Acc: 0.0615234375\n",
      "Step 160 -- Train loss: 2.7133541107177734, Train Acc: 0.071533203125 Test Acc: 0.068359375\n",
      "Step 161 -- Train loss: 2.713886022567749, Train Acc: 0.06787109375 Test Acc: 0.0537109375\n",
      "Step 162 -- Train loss: 2.7140355110168457, Train Acc: 0.06494140625 Test Acc: 0.064453125\n",
      "Step 163 -- Train loss: 2.7140049934387207, Train Acc: 0.056640625 Test Acc: 0.0673828125\n",
      "Step 164 -- Train loss: 2.7132773399353027, Train Acc: 0.070068359375 Test Acc: 0.0625\n",
      "Step 165 -- Train loss: 2.7139058113098145, Train Acc: 0.06591796875 Test Acc: 0.06640625\n",
      "Step 166 -- Train loss: 2.7145118713378906, Train Acc: 0.066650390625 Test Acc: 0.0576171875\n",
      "Step 167 -- Train loss: 2.713547945022583, Train Acc: 0.074951171875 Test Acc: 0.06640625\n",
      "Step 168 -- Train loss: 2.7135088443756104, Train Acc: 0.073974609375 Test Acc: 0.0546875\n",
      "Step 169 -- Train loss: 2.711616277694702, Train Acc: 0.072021484375 Test Acc: 0.0673828125\n",
      "Step 170 -- Train loss: 2.7132155895233154, Train Acc: 0.07470703125 Test Acc: 0.0654296875\n",
      "Step 171 -- Train loss: 2.7126822471618652, Train Acc: 0.07080078125 Test Acc: 0.052734375\n",
      "Step 172 -- Train loss: 2.711441993713379, Train Acc: 0.072265625 Test Acc: 0.060546875\n",
      "Step 173 -- Train loss: 2.712254762649536, Train Acc: 0.073486328125 Test Acc: 0.0625\n",
      "Step 174 -- Train loss: 2.7112770080566406, Train Acc: 0.084716796875 Test Acc: 0.0654296875\n",
      "Step 175 -- Train loss: 2.7119717597961426, Train Acc: 0.075439453125 Test Acc: 0.072265625\n",
      "Step 176 -- Train loss: 2.7112534046173096, Train Acc: 0.073974609375 Test Acc: 0.06640625\n",
      "Step 177 -- Train loss: 2.711087703704834, Train Acc: 0.078857421875 Test Acc: 0.09375\n",
      "Step 178 -- Train loss: 2.7099366188049316, Train Acc: 0.084228515625 Test Acc: 0.0751953125\n",
      "Step 179 -- Train loss: 2.7115280628204346, Train Acc: 0.0791015625 Test Acc: 0.0927734375\n",
      "Step 180 -- Train loss: 2.7100977897644043, Train Acc: 0.079345703125 Test Acc: 0.083984375\n",
      "Step 181 -- Train loss: 2.709617853164673, Train Acc: 0.09423828125 Test Acc: 0.0888671875\n",
      "Step 182 -- Train loss: 2.7079339027404785, Train Acc: 0.084228515625 Test Acc: 0.080078125\n",
      "Step 183 -- Train loss: 2.710531711578369, Train Acc: 0.0830078125 Test Acc: 0.1005859375\n",
      "Step 184 -- Train loss: 2.708326816558838, Train Acc: 0.0927734375 Test Acc: 0.083984375\n",
      "Step 185 -- Train loss: 2.706446409225464, Train Acc: 0.0908203125 Test Acc: 0.111328125\n",
      "Step 186 -- Train loss: 2.705587148666382, Train Acc: 0.080322265625 Test Acc: 0.08203125\n",
      "Step 187 -- Train loss: 2.7039201259613037, Train Acc: 0.09033203125 Test Acc: 0.08984375\n",
      "Step 188 -- Train loss: 2.7059476375579834, Train Acc: 0.091796875 Test Acc: 0.0927734375\n",
      "Step 189 -- Train loss: 2.7041940689086914, Train Acc: 0.1015625 Test Acc: 0.1005859375\n",
      "Step 190 -- Train loss: 2.7018356323242188, Train Acc: 0.095458984375 Test Acc: 0.0888671875\n",
      "Step 191 -- Train loss: 2.701643466949463, Train Acc: 0.0947265625 Test Acc: 0.0927734375\n",
      "Step 192 -- Train loss: 2.6995487213134766, Train Acc: 0.1015625 Test Acc: 0.087890625\n",
      "Step 193 -- Train loss: 2.700140953063965, Train Acc: 0.09130859375 Test Acc: 0.0947265625\n",
      "Step 194 -- Train loss: 2.698007106781006, Train Acc: 0.10546875 Test Acc: 0.1123046875\n",
      "Step 195 -- Train loss: 2.695312738418579, Train Acc: 0.0986328125 Test Acc: 0.091796875\n",
      "Step 196 -- Train loss: 2.694887161254883, Train Acc: 0.101318359375 Test Acc: 0.0888671875\n",
      "Step 197 -- Train loss: 2.6929702758789062, Train Acc: 0.1064453125 Test Acc: 0.11328125\n",
      "Step 198 -- Train loss: 2.6899163722991943, Train Acc: 0.117919921875 Test Acc: 0.1201171875\n",
      "Step 199 -- Train loss: 2.685114622116089, Train Acc: 0.11474609375 Test Acc: 0.1123046875\n",
      "Step 200 -- Train loss: 2.6829938888549805, Train Acc: 0.116455078125 Test Acc: 0.103515625\n",
      "Step 201 -- Train loss: 2.6814181804656982, Train Acc: 0.111572265625 Test Acc: 0.11328125\n",
      "Step 202 -- Train loss: 2.6799304485321045, Train Acc: 0.1171875 Test Acc: 0.1083984375\n",
      "Step 203 -- Train loss: 2.6750218868255615, Train Acc: 0.111083984375 Test Acc: 0.1083984375\n",
      "Step 204 -- Train loss: 2.6714885234832764, Train Acc: 0.11669921875 Test Acc: 0.1103515625\n",
      "Step 205 -- Train loss: 2.668020248413086, Train Acc: 0.12109375 Test Acc: 0.109375\n",
      "Step 206 -- Train loss: 2.6649651527404785, Train Acc: 0.111328125 Test Acc: 0.109375\n",
      "Step 207 -- Train loss: 2.661398410797119, Train Acc: 0.122314453125 Test Acc: 0.1181640625\n",
      "Step 208 -- Train loss: 2.6610031127929688, Train Acc: 0.115966796875 Test Acc: 0.111328125\n",
      "Step 209 -- Train loss: 2.6584014892578125, Train Acc: 0.115966796875 Test Acc: 0.1083984375\n",
      "Step 210 -- Train loss: 2.656519651412964, Train Acc: 0.11572265625 Test Acc: 0.111328125\n",
      "Step 211 -- Train loss: 2.6565122604370117, Train Acc: 0.11083984375 Test Acc: 0.1123046875\n",
      "Step 212 -- Train loss: 2.6529459953308105, Train Acc: 0.11572265625 Test Acc: 0.1171875\n",
      "Step 213 -- Train loss: 2.6527533531188965, Train Acc: 0.114013671875 Test Acc: 0.1103515625\n",
      "Step 214 -- Train loss: 2.6501071453094482, Train Acc: 0.111328125 Test Acc: 0.11328125\n",
      "Step 215 -- Train loss: 2.6492650508880615, Train Acc: 0.107666015625 Test Acc: 0.1015625\n",
      "Step 216 -- Train loss: 2.649172306060791, Train Acc: 0.114013671875 Test Acc: 0.1298828125\n",
      "Step 217 -- Train loss: 2.647210121154785, Train Acc: 0.116455078125 Test Acc: 0.119140625\n",
      "Step 218 -- Train loss: 2.6446826457977295, Train Acc: 0.112060546875 Test Acc: 0.1162109375\n",
      "Step 219 -- Train loss: 2.6441783905029297, Train Acc: 0.116455078125 Test Acc: 0.1171875\n",
      "Step 220 -- Train loss: 2.643126964569092, Train Acc: 0.11572265625 Test Acc: 0.1142578125\n",
      "Step 221 -- Train loss: 2.6455588340759277, Train Acc: 0.115966796875 Test Acc: 0.115234375\n",
      "Step 222 -- Train loss: 2.643538236618042, Train Acc: 0.11376953125 Test Acc: 0.115234375\n",
      "Step 223 -- Train loss: 2.642634630203247, Train Acc: 0.117431640625 Test Acc: 0.1162109375\n",
      "Step 224 -- Train loss: 2.640019178390503, Train Acc: 0.123779296875 Test Acc: 0.1171875\n",
      "Step 225 -- Train loss: 2.640336275100708, Train Acc: 0.118896484375 Test Acc: 0.1181640625\n",
      "Step 226 -- Train loss: 2.638784646987915, Train Acc: 0.1201171875 Test Acc: 0.1123046875\n",
      "Step 227 -- Train loss: 2.6398003101348877, Train Acc: 0.12451171875 Test Acc: 0.115234375\n",
      "Step 228 -- Train loss: 2.639172077178955, Train Acc: 0.124267578125 Test Acc: 0.1103515625\n",
      "Step 229 -- Train loss: 2.6388680934906006, Train Acc: 0.119873046875 Test Acc: 0.1181640625\n",
      "Step 230 -- Train loss: 2.638162612915039, Train Acc: 0.11962890625 Test Acc: 0.1181640625\n",
      "Step 231 -- Train loss: 2.638859987258911, Train Acc: 0.122314453125 Test Acc: 0.119140625\n",
      "Step 232 -- Train loss: 2.6369056701660156, Train Acc: 0.11962890625 Test Acc: 0.1162109375\n",
      "Step 233 -- Train loss: 2.638087034225464, Train Acc: 0.116943359375 Test Acc: 0.125\n",
      "Step 234 -- Train loss: 2.637995719909668, Train Acc: 0.111083984375 Test Acc: 0.1181640625\n",
      "Step 235 -- Train loss: 2.6401748657226562, Train Acc: 0.1220703125 Test Acc: 0.1142578125\n",
      "Step 236 -- Train loss: 2.638066291809082, Train Acc: 0.11376953125 Test Acc: 0.123046875\n",
      "Step 237 -- Train loss: 2.636101007461548, Train Acc: 0.12060546875 Test Acc: 0.111328125\n",
      "Step 238 -- Train loss: 2.63576340675354, Train Acc: 0.12060546875 Test Acc: 0.111328125\n",
      "Step 239 -- Train loss: 2.636622428894043, Train Acc: 0.124755859375 Test Acc: 0.1083984375\n",
      "Step 240 -- Train loss: 2.6371400356292725, Train Acc: 0.11767578125 Test Acc: 0.119140625\n",
      "Step 241 -- Train loss: 2.6354470252990723, Train Acc: 0.109619140625 Test Acc: 0.1181640625\n",
      "Step 242 -- Train loss: 2.6358046531677246, Train Acc: 0.12255859375 Test Acc: 0.10546875\n",
      "Step 243 -- Train loss: 2.635746479034424, Train Acc: 0.1201171875 Test Acc: 0.1259765625\n",
      "Step 244 -- Train loss: 2.635115146636963, Train Acc: 0.123291015625 Test Acc: 0.1171875\n",
      "Step 245 -- Train loss: 2.63495135307312, Train Acc: 0.119873046875 Test Acc: 0.1123046875\n",
      "Step 246 -- Train loss: 2.6341469287872314, Train Acc: 0.12353515625 Test Acc: 0.1201171875\n",
      "Step 247 -- Train loss: 2.636131763458252, Train Acc: 0.117919921875 Test Acc: 0.1220703125\n",
      "Step 248 -- Train loss: 2.634082078933716, Train Acc: 0.120849609375 Test Acc: 0.1103515625\n",
      "Step 249 -- Train loss: 2.6338276863098145, Train Acc: 0.126220703125 Test Acc: 0.123046875\n",
      "Step 250 -- Train loss: 2.633746385574341, Train Acc: 0.114990234375 Test Acc: 0.115234375\n",
      "Step 251 -- Train loss: 2.6338298320770264, Train Acc: 0.1181640625 Test Acc: 0.107421875\n",
      "Step 252 -- Train loss: 2.6344778537750244, Train Acc: 0.113037109375 Test Acc: 0.1240234375\n",
      "Step 253 -- Train loss: 2.6329100131988525, Train Acc: 0.1201171875 Test Acc: 0.1376953125\n",
      "Step 254 -- Train loss: 2.633619546890259, Train Acc: 0.11767578125 Test Acc: 0.1240234375\n",
      "Step 255 -- Train loss: 2.632835865020752, Train Acc: 0.12255859375 Test Acc: 0.1181640625\n",
      "Step 256 -- Train loss: 2.6342968940734863, Train Acc: 0.12548828125 Test Acc: 0.115234375\n",
      "Step 257 -- Train loss: 2.6332316398620605, Train Acc: 0.11962890625 Test Acc: 0.115234375\n",
      "Step 258 -- Train loss: 2.632502794265747, Train Acc: 0.121337890625 Test Acc: 0.1201171875\n",
      "Step 259 -- Train loss: 2.632195234298706, Train Acc: 0.1240234375 Test Acc: 0.1171875\n",
      "Step 260 -- Train loss: 2.6330888271331787, Train Acc: 0.117919921875 Test Acc: 0.126953125\n",
      "Step 261 -- Train loss: 2.6316137313842773, Train Acc: 0.124267578125 Test Acc: 0.1318359375\n",
      "Step 262 -- Train loss: 2.6327950954437256, Train Acc: 0.119140625 Test Acc: 0.1259765625\n",
      "Step 263 -- Train loss: 2.631688117980957, Train Acc: 0.125 Test Acc: 0.1103515625\n",
      "Step 264 -- Train loss: 2.6317479610443115, Train Acc: 0.119384765625 Test Acc: 0.1201171875\n",
      "Step 265 -- Train loss: 2.630783796310425, Train Acc: 0.11962890625 Test Acc: 0.1318359375\n",
      "Step 266 -- Train loss: 2.632408618927002, Train Acc: 0.127685546875 Test Acc: 0.1142578125\n",
      "Step 267 -- Train loss: 2.6309211254119873, Train Acc: 0.120849609375 Test Acc: 0.115234375\n",
      "Step 268 -- Train loss: 2.6324639320373535, Train Acc: 0.125 Test Acc: 0.1220703125\n",
      "Step 269 -- Train loss: 2.6312010288238525, Train Acc: 0.1162109375 Test Acc: 0.126953125\n",
      "Step 270 -- Train loss: 2.632140874862671, Train Acc: 0.1162109375 Test Acc: 0.123046875\n",
      "Step 271 -- Train loss: 2.631563901901245, Train Acc: 0.120849609375 Test Acc: 0.12109375\n",
      "Step 272 -- Train loss: 2.6314613819122314, Train Acc: 0.12841796875 Test Acc: 0.123046875\n",
      "Step 273 -- Train loss: 2.631126642227173, Train Acc: 0.1201171875 Test Acc: 0.1220703125\n",
      "Step 274 -- Train loss: 2.6312789916992188, Train Acc: 0.123779296875 Test Acc: 0.1201171875\n",
      "Step 275 -- Train loss: 2.6306264400482178, Train Acc: 0.114990234375 Test Acc: 0.142578125\n",
      "Step 276 -- Train loss: 2.6295034885406494, Train Acc: 0.13427734375 Test Acc: 0.1298828125\n",
      "Step 277 -- Train loss: 2.6291956901550293, Train Acc: 0.1318359375 Test Acc: 0.119140625\n",
      "Step 278 -- Train loss: 2.6307528018951416, Train Acc: 0.132080078125 Test Acc: 0.130859375\n",
      "Step 279 -- Train loss: 2.631941318511963, Train Acc: 0.125 Test Acc: 0.119140625\n",
      "Step 280 -- Train loss: 2.631075859069824, Train Acc: 0.122314453125 Test Acc: 0.1220703125\n",
      "Step 281 -- Train loss: 2.6302051544189453, Train Acc: 0.1181640625 Test Acc: 0.123046875\n",
      "Step 282 -- Train loss: 2.629366159439087, Train Acc: 0.1240234375 Test Acc: 0.11328125\n",
      "Step 283 -- Train loss: 2.63104510307312, Train Acc: 0.122802734375 Test Acc: 0.130859375\n",
      "Step 284 -- Train loss: 2.6302244663238525, Train Acc: 0.1162109375 Test Acc: 0.1162109375\n",
      "Step 285 -- Train loss: 2.6295244693756104, Train Acc: 0.122802734375 Test Acc: 0.1181640625\n",
      "Step 286 -- Train loss: 2.629970073699951, Train Acc: 0.12744140625 Test Acc: 0.1123046875\n",
      "Step 287 -- Train loss: 2.629671335220337, Train Acc: 0.11962890625 Test Acc: 0.1181640625\n",
      "Step 288 -- Train loss: 2.6292896270751953, Train Acc: 0.123291015625 Test Acc: 0.1083984375\n",
      "Step 289 -- Train loss: 2.6301093101501465, Train Acc: 0.123779296875 Test Acc: 0.1240234375\n",
      "Step 290 -- Train loss: 2.6289191246032715, Train Acc: 0.116455078125 Test Acc: 0.119140625\n",
      "Step 291 -- Train loss: 2.6289725303649902, Train Acc: 0.1240234375 Test Acc: 0.1171875\n",
      "Step 292 -- Train loss: 2.6303601264953613, Train Acc: 0.120849609375 Test Acc: 0.1220703125\n",
      "Step 293 -- Train loss: 2.6275718212127686, Train Acc: 0.1259765625 Test Acc: 0.119140625\n",
      "Step 294 -- Train loss: 2.628296136856079, Train Acc: 0.122314453125 Test Acc: 0.1259765625\n",
      "Step 295 -- Train loss: 2.6300160884857178, Train Acc: 0.117431640625 Test Acc: 0.1318359375\n",
      "Step 296 -- Train loss: 2.627258539199829, Train Acc: 0.116943359375 Test Acc: 0.1142578125\n",
      "Step 297 -- Train loss: 2.6270885467529297, Train Acc: 0.12255859375 Test Acc: 0.126953125\n",
      "Step 298 -- Train loss: 2.6289172172546387, Train Acc: 0.118408203125 Test Acc: 0.130859375\n",
      "Step 299 -- Train loss: 2.626952648162842, Train Acc: 0.12353515625 Test Acc: 0.1142578125\n",
      "Step 300 -- Train loss: 2.626315116882324, Train Acc: 0.128662109375 Test Acc: 0.1142578125\n",
      "Step 301 -- Train loss: 2.6272225379943848, Train Acc: 0.12109375 Test Acc: 0.1240234375\n",
      "Step 302 -- Train loss: 2.625310182571411, Train Acc: 0.134033203125 Test Acc: 0.115234375\n",
      "Step 303 -- Train loss: 2.6241958141326904, Train Acc: 0.125 Test Acc: 0.1298828125\n",
      "Step 304 -- Train loss: 2.626296043395996, Train Acc: 0.130126953125 Test Acc: 0.1201171875\n",
      "Step 305 -- Train loss: 2.6250364780426025, Train Acc: 0.12646484375 Test Acc: 0.130859375\n",
      "Step 306 -- Train loss: 2.6248419284820557, Train Acc: 0.13037109375 Test Acc: 0.11328125\n",
      "Step 307 -- Train loss: 2.6260218620300293, Train Acc: 0.1201171875 Test Acc: 0.1357421875\n",
      "Step 308 -- Train loss: 2.625667095184326, Train Acc: 0.12255859375 Test Acc: 0.1201171875\n",
      "Step 309 -- Train loss: 2.628145217895508, Train Acc: 0.12158203125 Test Acc: 0.119140625\n",
      "Step 310 -- Train loss: 2.6238620281219482, Train Acc: 0.1279296875 Test Acc: 0.125\n",
      "Step 311 -- Train loss: 2.6260719299316406, Train Acc: 0.124755859375 Test Acc: 0.11328125\n",
      "Step 312 -- Train loss: 2.626917600631714, Train Acc: 0.12939453125 Test Acc: 0.1435546875\n",
      "Step 313 -- Train loss: 2.6223604679107666, Train Acc: 0.12939453125 Test Acc: 0.12109375\n",
      "Step 314 -- Train loss: 2.6251301765441895, Train Acc: 0.1298828125 Test Acc: 0.1318359375\n",
      "Step 315 -- Train loss: 2.6250410079956055, Train Acc: 0.12548828125 Test Acc: 0.1220703125\n",
      "Step 316 -- Train loss: 2.626058340072632, Train Acc: 0.119384765625 Test Acc: 0.1162109375\n",
      "Step 317 -- Train loss: 2.625880718231201, Train Acc: 0.126708984375 Test Acc: 0.1201171875\n",
      "Step 318 -- Train loss: 2.6239852905273438, Train Acc: 0.123291015625 Test Acc: 0.126953125\n",
      "Step 319 -- Train loss: 2.6239349842071533, Train Acc: 0.123046875 Test Acc: 0.115234375\n",
      "Step 320 -- Train loss: 2.6244335174560547, Train Acc: 0.128662109375 Test Acc: 0.123046875\n",
      "Step 321 -- Train loss: 2.6235227584838867, Train Acc: 0.124267578125 Test Acc: 0.126953125\n",
      "Step 322 -- Train loss: 2.623919725418091, Train Acc: 0.12158203125 Test Acc: 0.1298828125\n",
      "Step 323 -- Train loss: 2.6265323162078857, Train Acc: 0.125 Test Acc: 0.142578125\n",
      "Step 324 -- Train loss: 2.6210458278656006, Train Acc: 0.126220703125 Test Acc: 0.134765625\n",
      "Step 325 -- Train loss: 2.6220955848693848, Train Acc: 0.13037109375 Test Acc: 0.1337890625\n",
      "Step 326 -- Train loss: 2.6238975524902344, Train Acc: 0.123291015625 Test Acc: 0.126953125\n",
      "Step 327 -- Train loss: 2.624685525894165, Train Acc: 0.1279296875 Test Acc: 0.1298828125\n",
      "Step 328 -- Train loss: 2.623828411102295, Train Acc: 0.129150390625 Test Acc: 0.1240234375\n",
      "Step 329 -- Train loss: 2.6239871978759766, Train Acc: 0.125244140625 Test Acc: 0.1298828125\n",
      "Step 330 -- Train loss: 2.623659133911133, Train Acc: 0.12548828125 Test Acc: 0.1259765625\n",
      "Step 331 -- Train loss: 2.6215944290161133, Train Acc: 0.12890625 Test Acc: 0.1298828125\n",
      "Step 332 -- Train loss: 2.6270363330841064, Train Acc: 0.127685546875 Test Acc: 0.12890625\n",
      "Step 333 -- Train loss: 2.6234233379364014, Train Acc: 0.12890625 Test Acc: 0.123046875\n",
      "Step 334 -- Train loss: 2.622753620147705, Train Acc: 0.12841796875 Test Acc: 0.1337890625\n",
      "Step 335 -- Train loss: 2.623309373855591, Train Acc: 0.126708984375 Test Acc: 0.1259765625\n",
      "Step 336 -- Train loss: 2.625396728515625, Train Acc: 0.1240234375 Test Acc: 0.1220703125\n",
      "Step 337 -- Train loss: 2.6236696243286133, Train Acc: 0.12255859375 Test Acc: 0.123046875\n",
      "Step 338 -- Train loss: 2.6241869926452637, Train Acc: 0.126708984375 Test Acc: 0.1337890625\n",
      "Step 339 -- Train loss: 2.6224145889282227, Train Acc: 0.125732421875 Test Acc: 0.1162109375\n",
      "Step 340 -- Train loss: 2.6255476474761963, Train Acc: 0.123291015625 Test Acc: 0.1181640625\n",
      "Step 341 -- Train loss: 2.6242852210998535, Train Acc: 0.125 Test Acc: 0.1162109375\n",
      "Step 342 -- Train loss: 2.621026039123535, Train Acc: 0.121826171875 Test Acc: 0.1201171875\n",
      "Step 343 -- Train loss: 2.6232919692993164, Train Acc: 0.121337890625 Test Acc: 0.1328125\n",
      "Step 344 -- Train loss: 2.622920274734497, Train Acc: 0.1318359375 Test Acc: 0.12109375\n",
      "Step 345 -- Train loss: 2.6201651096343994, Train Acc: 0.129150390625 Test Acc: 0.1171875\n",
      "Step 346 -- Train loss: 2.6236448287963867, Train Acc: 0.123291015625 Test Acc: 0.1279296875\n",
      "Step 347 -- Train loss: 2.6211483478546143, Train Acc: 0.12939453125 Test Acc: 0.1201171875\n",
      "Step 348 -- Train loss: 2.6240530014038086, Train Acc: 0.1240234375 Test Acc: 0.134765625\n",
      "Step 349 -- Train loss: 2.621081829071045, Train Acc: 0.12646484375 Test Acc: 0.1171875\n",
      "Step 350 -- Train loss: 2.6231606006622314, Train Acc: 0.12548828125 Test Acc: 0.12890625\n",
      "Step 351 -- Train loss: 2.6196532249450684, Train Acc: 0.125244140625 Test Acc: 0.115234375\n",
      "Step 352 -- Train loss: 2.623777151107788, Train Acc: 0.126953125 Test Acc: 0.11328125\n",
      "Step 353 -- Train loss: 2.622616767883301, Train Acc: 0.127197265625 Test Acc: 0.12109375\n",
      "Step 354 -- Train loss: 2.6229662895202637, Train Acc: 0.1279296875 Test Acc: 0.138671875\n",
      "Step 355 -- Train loss: 2.621112585067749, Train Acc: 0.12939453125 Test Acc: 0.1259765625\n",
      "Step 356 -- Train loss: 2.620572328567505, Train Acc: 0.123779296875 Test Acc: 0.123046875\n",
      "Step 357 -- Train loss: 2.622333526611328, Train Acc: 0.12939453125 Test Acc: 0.1103515625\n",
      "Step 358 -- Train loss: 2.6230976581573486, Train Acc: 0.130615234375 Test Acc: 0.126953125\n",
      "Step 359 -- Train loss: 2.6218223571777344, Train Acc: 0.13037109375 Test Acc: 0.1240234375\n",
      "Step 360 -- Train loss: 2.6220662593841553, Train Acc: 0.127197265625 Test Acc: 0.1357421875\n",
      "Step 361 -- Train loss: 2.623464822769165, Train Acc: 0.122802734375 Test Acc: 0.1201171875\n",
      "Step 362 -- Train loss: 2.621328353881836, Train Acc: 0.125244140625 Test Acc: 0.1240234375\n",
      "Step 363 -- Train loss: 2.621619701385498, Train Acc: 0.132568359375 Test Acc: 0.1123046875\n",
      "Step 364 -- Train loss: 2.6201770305633545, Train Acc: 0.130126953125 Test Acc: 0.12109375\n",
      "Step 365 -- Train loss: 2.621530532836914, Train Acc: 0.120849609375 Test Acc: 0.130859375\n",
      "Step 366 -- Train loss: 2.6216938495635986, Train Acc: 0.126708984375 Test Acc: 0.130859375\n",
      "Step 367 -- Train loss: 2.6228363513946533, Train Acc: 0.13037109375 Test Acc: 0.12890625\n",
      "Step 368 -- Train loss: 2.620382785797119, Train Acc: 0.126708984375 Test Acc: 0.1328125\n",
      "Step 369 -- Train loss: 2.6226539611816406, Train Acc: 0.13330078125 Test Acc: 0.1240234375\n",
      "Step 370 -- Train loss: 2.622680425643921, Train Acc: 0.12646484375 Test Acc: 0.1240234375\n",
      "Step 371 -- Train loss: 2.6209237575531006, Train Acc: 0.13134765625 Test Acc: 0.126953125\n",
      "Step 372 -- Train loss: 2.626115083694458, Train Acc: 0.120361328125 Test Acc: 0.1240234375\n",
      "Step 373 -- Train loss: 2.6195993423461914, Train Acc: 0.127685546875 Test Acc: 0.1279296875\n",
      "Step 374 -- Train loss: 2.620802879333496, Train Acc: 0.1328125 Test Acc: 0.126953125\n",
      "Step 375 -- Train loss: 2.619889974594116, Train Acc: 0.124267578125 Test Acc: 0.1337890625\n",
      "Step 376 -- Train loss: 2.621973991394043, Train Acc: 0.131103515625 Test Acc: 0.1162109375\n",
      "Step 377 -- Train loss: 2.6170451641082764, Train Acc: 0.1337890625 Test Acc: 0.1396484375\n",
      "Step 378 -- Train loss: 2.6207234859466553, Train Acc: 0.1298828125 Test Acc: 0.1181640625\n",
      "Step 379 -- Train loss: 2.621074914932251, Train Acc: 0.125732421875 Test Acc: 0.111328125\n",
      "Step 380 -- Train loss: 2.61984920501709, Train Acc: 0.129150390625 Test Acc: 0.1298828125\n",
      "Step 381 -- Train loss: 2.621312379837036, Train Acc: 0.122314453125 Test Acc: 0.1259765625\n",
      "Step 382 -- Train loss: 2.6202120780944824, Train Acc: 0.125244140625 Test Acc: 0.1357421875\n",
      "Step 383 -- Train loss: 2.6199164390563965, Train Acc: 0.13037109375 Test Acc: 0.119140625\n",
      "Step 384 -- Train loss: 2.6207869052886963, Train Acc: 0.134033203125 Test Acc: 0.1298828125\n",
      "Step 385 -- Train loss: 2.6213948726654053, Train Acc: 0.12890625 Test Acc: 0.126953125\n",
      "Step 386 -- Train loss: 2.6165173053741455, Train Acc: 0.130859375 Test Acc: 0.1240234375\n",
      "Step 387 -- Train loss: 2.6187002658843994, Train Acc: 0.13818359375 Test Acc: 0.13671875\n",
      "Step 388 -- Train loss: 2.6193881034851074, Train Acc: 0.131103515625 Test Acc: 0.1416015625\n",
      "Step 389 -- Train loss: 2.6171822547912598, Train Acc: 0.129638671875 Test Acc: 0.1279296875\n",
      "Step 390 -- Train loss: 2.618955612182617, Train Acc: 0.1337890625 Test Acc: 0.1279296875\n",
      "Step 391 -- Train loss: 2.620593547821045, Train Acc: 0.12841796875 Test Acc: 0.123046875\n",
      "Step 392 -- Train loss: 2.621232509613037, Train Acc: 0.12646484375 Test Acc: 0.1240234375\n",
      "Step 393 -- Train loss: 2.6161749362945557, Train Acc: 0.1240234375 Test Acc: 0.1357421875\n",
      "Step 394 -- Train loss: 2.621481418609619, Train Acc: 0.125 Test Acc: 0.1318359375\n",
      "Step 395 -- Train loss: 2.617450714111328, Train Acc: 0.1318359375 Test Acc: 0.146484375\n",
      "Step 396 -- Train loss: 2.617204427719116, Train Acc: 0.136962890625 Test Acc: 0.1259765625\n",
      "Step 397 -- Train loss: 2.619215488433838, Train Acc: 0.132080078125 Test Acc: 0.1318359375\n",
      "Step 398 -- Train loss: 2.619349718093872, Train Acc: 0.128662109375 Test Acc: 0.1396484375\n",
      "Step 399 -- Train loss: 2.6181414127349854, Train Acc: 0.135009765625 Test Acc: 0.1240234375\n",
      "Step 400 -- Train loss: 2.618098735809326, Train Acc: 0.132080078125 Test Acc: 0.1376953125\n",
      "Step 401 -- Train loss: 2.615687370300293, Train Acc: 0.127197265625 Test Acc: 0.1337890625\n",
      "Step 402 -- Train loss: 2.6180551052093506, Train Acc: 0.125244140625 Test Acc: 0.125\n",
      "Step 403 -- Train loss: 2.615257740020752, Train Acc: 0.1328125 Test Acc: 0.12109375\n",
      "Step 404 -- Train loss: 2.619560718536377, Train Acc: 0.1279296875 Test Acc: 0.1357421875\n",
      "Step 405 -- Train loss: 2.619574546813965, Train Acc: 0.12548828125 Test Acc: 0.1220703125\n",
      "Step 406 -- Train loss: 2.6187098026275635, Train Acc: 0.1337890625 Test Acc: 0.13671875\n",
      "Step 407 -- Train loss: 2.6169230937957764, Train Acc: 0.12939453125 Test Acc: 0.125\n",
      "Step 408 -- Train loss: 2.6177031993865967, Train Acc: 0.133544921875 Test Acc: 0.126953125\n",
      "Step 409 -- Train loss: 2.6180620193481445, Train Acc: 0.1376953125 Test Acc: 0.134765625\n",
      "Step 410 -- Train loss: 2.618802547454834, Train Acc: 0.132080078125 Test Acc: 0.138671875\n",
      "Step 411 -- Train loss: 2.6183507442474365, Train Acc: 0.1318359375 Test Acc: 0.134765625\n",
      "Step 412 -- Train loss: 2.617197036743164, Train Acc: 0.13916015625 Test Acc: 0.1298828125\n",
      "Step 413 -- Train loss: 2.6166110038757324, Train Acc: 0.1357421875 Test Acc: 0.1416015625\n",
      "Step 414 -- Train loss: 2.617352247238159, Train Acc: 0.131103515625 Test Acc: 0.146484375\n",
      "Step 415 -- Train loss: 2.6145436763763428, Train Acc: 0.134033203125 Test Acc: 0.1396484375\n",
      "Step 416 -- Train loss: 2.616694450378418, Train Acc: 0.1318359375 Test Acc: 0.15234375\n",
      "Step 417 -- Train loss: 2.616187810897827, Train Acc: 0.132080078125 Test Acc: 0.134765625\n",
      "Step 418 -- Train loss: 2.6150128841400146, Train Acc: 0.1328125 Test Acc: 0.1435546875\n",
      "Step 419 -- Train loss: 2.616290330886841, Train Acc: 0.13330078125 Test Acc: 0.13671875\n",
      "Step 420 -- Train loss: 2.6142406463623047, Train Acc: 0.131591796875 Test Acc: 0.1328125\n",
      "Step 421 -- Train loss: 2.616791009902954, Train Acc: 0.132080078125 Test Acc: 0.134765625\n",
      "Step 422 -- Train loss: 2.615600824356079, Train Acc: 0.135009765625 Test Acc: 0.1337890625\n",
      "Step 423 -- Train loss: 2.613436222076416, Train Acc: 0.14111328125 Test Acc: 0.1396484375\n",
      "Step 424 -- Train loss: 2.6179699897766113, Train Acc: 0.133056640625 Test Acc: 0.123046875\n",
      "Step 425 -- Train loss: 2.6121668815612793, Train Acc: 0.144287109375 Test Acc: 0.13671875\n",
      "Step 426 -- Train loss: 2.6127612590789795, Train Acc: 0.13427734375 Test Acc: 0.140625\n",
      "Step 427 -- Train loss: 2.611255168914795, Train Acc: 0.135009765625 Test Acc: 0.13671875\n",
      "Step 428 -- Train loss: 2.6136507987976074, Train Acc: 0.141845703125 Test Acc: 0.138671875\n",
      "Step 429 -- Train loss: 2.6157174110412598, Train Acc: 0.127685546875 Test Acc: 0.1552734375\n",
      "Step 430 -- Train loss: 2.6125292778015137, Train Acc: 0.137451171875 Test Acc: 0.11328125\n",
      "Step 431 -- Train loss: 2.6139512062072754, Train Acc: 0.1435546875 Test Acc: 0.140625\n",
      "Step 432 -- Train loss: 2.6153087615966797, Train Acc: 0.1328125 Test Acc: 0.119140625\n",
      "Step 433 -- Train loss: 2.6118335723876953, Train Acc: 0.13427734375 Test Acc: 0.1357421875\n",
      "Step 434 -- Train loss: 2.6129143238067627, Train Acc: 0.134765625 Test Acc: 0.126953125\n",
      "Step 435 -- Train loss: 2.6131112575531006, Train Acc: 0.14306640625 Test Acc: 0.1484375\n",
      "Step 436 -- Train loss: 2.6124320030212402, Train Acc: 0.135986328125 Test Acc: 0.126953125\n",
      "Step 437 -- Train loss: 2.6105332374572754, Train Acc: 0.134765625 Test Acc: 0.1474609375\n",
      "Step 438 -- Train loss: 2.6104023456573486, Train Acc: 0.136962890625 Test Acc: 0.1298828125\n",
      "Step 439 -- Train loss: 2.6108803749084473, Train Acc: 0.13671875 Test Acc: 0.12890625\n",
      "Step 440 -- Train loss: 2.6114797592163086, Train Acc: 0.146240234375 Test Acc: 0.15234375\n",
      "Step 441 -- Train loss: 2.6090314388275146, Train Acc: 0.140380859375 Test Acc: 0.140625\n",
      "Step 442 -- Train loss: 2.6103310585021973, Train Acc: 0.138427734375 Test Acc: 0.130859375\n",
      "Step 443 -- Train loss: 2.6126492023468018, Train Acc: 0.135498046875 Test Acc: 0.12890625\n",
      "Step 444 -- Train loss: 2.6085803508758545, Train Acc: 0.1435546875 Test Acc: 0.1484375\n",
      "Step 445 -- Train loss: 2.610443353652954, Train Acc: 0.149169921875 Test Acc: 0.1396484375\n",
      "Step 446 -- Train loss: 2.609531879425049, Train Acc: 0.143310546875 Test Acc: 0.1435546875\n",
      "Step 447 -- Train loss: 2.6103782653808594, Train Acc: 0.146240234375 Test Acc: 0.1455078125\n",
      "Step 448 -- Train loss: 2.6067686080932617, Train Acc: 0.150146484375 Test Acc: 0.1474609375\n",
      "Step 449 -- Train loss: 2.6104001998901367, Train Acc: 0.14306640625 Test Acc: 0.1318359375\n",
      "Step 450 -- Train loss: 2.607326030731201, Train Acc: 0.145263671875 Test Acc: 0.1484375\n",
      "Step 451 -- Train loss: 2.6072874069213867, Train Acc: 0.13818359375 Test Acc: 0.1494140625\n",
      "Step 452 -- Train loss: 2.6079421043395996, Train Acc: 0.1474609375 Test Acc: 0.1533203125\n",
      "Step 453 -- Train loss: 2.6082992553710938, Train Acc: 0.148193359375 Test Acc: 0.1337890625\n",
      "Step 454 -- Train loss: 2.604623794555664, Train Acc: 0.147216796875 Test Acc: 0.154296875\n",
      "Step 455 -- Train loss: 2.604170799255371, Train Acc: 0.14990234375 Test Acc: 0.15234375\n",
      "Step 456 -- Train loss: 2.606288433074951, Train Acc: 0.15087890625 Test Acc: 0.1484375\n",
      "Step 457 -- Train loss: 2.6036527156829834, Train Acc: 0.149169921875 Test Acc: 0.1337890625\n",
      "Step 458 -- Train loss: 2.6041386127471924, Train Acc: 0.14990234375 Test Acc: 0.1455078125\n",
      "Step 459 -- Train loss: 2.6039328575134277, Train Acc: 0.1533203125 Test Acc: 0.146484375\n",
      "Step 460 -- Train loss: 2.6021599769592285, Train Acc: 0.152587890625 Test Acc: 0.146484375\n",
      "Step 461 -- Train loss: 2.604496479034424, Train Acc: 0.156982421875 Test Acc: 0.142578125\n",
      "Step 462 -- Train loss: 2.602771520614624, Train Acc: 0.14892578125 Test Acc: 0.1611328125\n",
      "Step 463 -- Train loss: 2.603093147277832, Train Acc: 0.1484375 Test Acc: 0.1474609375\n",
      "Step 464 -- Train loss: 2.604405641555786, Train Acc: 0.161376953125 Test Acc: 0.1455078125\n",
      "Step 465 -- Train loss: 2.6021642684936523, Train Acc: 0.157470703125 Test Acc: 0.16796875\n",
      "Step 466 -- Train loss: 2.6051185131073, Train Acc: 0.1455078125 Test Acc: 0.1611328125\n",
      "Step 467 -- Train loss: 2.599924325942993, Train Acc: 0.15673828125 Test Acc: 0.1611328125\n",
      "Step 468 -- Train loss: 2.601449728012085, Train Acc: 0.156005859375 Test Acc: 0.1494140625\n",
      "Step 469 -- Train loss: 2.6003007888793945, Train Acc: 0.15673828125 Test Acc: 0.1533203125\n",
      "Step 470 -- Train loss: 2.5974979400634766, Train Acc: 0.1572265625 Test Acc: 0.150390625\n",
      "Step 471 -- Train loss: 2.603196620941162, Train Acc: 0.1572265625 Test Acc: 0.15625\n",
      "Step 472 -- Train loss: 2.6026859283447266, Train Acc: 0.1630859375 Test Acc: 0.1630859375\n",
      "Step 473 -- Train loss: 2.6029558181762695, Train Acc: 0.162109375 Test Acc: 0.166015625\n",
      "Step 474 -- Train loss: 2.6033618450164795, Train Acc: 0.153076171875 Test Acc: 0.146484375\n",
      "Step 475 -- Train loss: 2.5996413230895996, Train Acc: 0.163330078125 Test Acc: 0.1640625\n",
      "Step 476 -- Train loss: 2.599382162094116, Train Acc: 0.158203125 Test Acc: 0.1650390625\n",
      "Step 477 -- Train loss: 2.59737491607666, Train Acc: 0.16748046875 Test Acc: 0.1591796875\n",
      "Step 478 -- Train loss: 2.5948915481567383, Train Acc: 0.170166015625 Test Acc: 0.1650390625\n",
      "Step 479 -- Train loss: 2.5951387882232666, Train Acc: 0.1689453125 Test Acc: 0.1611328125\n",
      "Step 480 -- Train loss: 2.5955874919891357, Train Acc: 0.173095703125 Test Acc: 0.1640625\n",
      "Step 481 -- Train loss: 2.5931758880615234, Train Acc: 0.169921875 Test Acc: 0.1552734375\n",
      "Step 482 -- Train loss: 2.5928139686584473, Train Acc: 0.169189453125 Test Acc: 0.1591796875\n",
      "Step 483 -- Train loss: 2.591007709503174, Train Acc: 0.164794921875 Test Acc: 0.162109375\n",
      "Step 484 -- Train loss: 2.589555501937866, Train Acc: 0.173095703125 Test Acc: 0.1787109375\n",
      "Step 485 -- Train loss: 2.5897417068481445, Train Acc: 0.16796875 Test Acc: 0.169921875\n",
      "Step 486 -- Train loss: 2.5875349044799805, Train Acc: 0.1767578125 Test Acc: 0.166015625\n",
      "Step 487 -- Train loss: 2.590451717376709, Train Acc: 0.168701171875 Test Acc: 0.162109375\n",
      "Step 488 -- Train loss: 2.5859005451202393, Train Acc: 0.17822265625 Test Acc: 0.1572265625\n",
      "Step 489 -- Train loss: 2.5855045318603516, Train Acc: 0.172607421875 Test Acc: 0.154296875\n",
      "Step 490 -- Train loss: 2.5855374336242676, Train Acc: 0.1728515625 Test Acc: 0.1845703125\n",
      "Step 491 -- Train loss: 2.5814483165740967, Train Acc: 0.17333984375 Test Acc: 0.154296875\n",
      "Step 492 -- Train loss: 2.5845706462860107, Train Acc: 0.177734375 Test Acc: 0.1748046875\n",
      "Step 493 -- Train loss: 2.5813827514648438, Train Acc: 0.179443359375 Test Acc: 0.173828125\n",
      "Step 494 -- Train loss: 2.579637050628662, Train Acc: 0.177001953125 Test Acc: 0.1708984375\n",
      "Step 495 -- Train loss: 2.5767836570739746, Train Acc: 0.180419921875 Test Acc: 0.1728515625\n",
      "Step 496 -- Train loss: 2.5773890018463135, Train Acc: 0.178466796875 Test Acc: 0.1796875\n",
      "Step 497 -- Train loss: 2.578465223312378, Train Acc: 0.1796875 Test Acc: 0.1923828125\n",
      "Step 498 -- Train loss: 2.573211193084717, Train Acc: 0.17626953125 Test Acc: 0.171875\n",
      "Step 499 -- Train loss: 2.574249505996704, Train Acc: 0.185546875 Test Acc: 0.17578125\n",
      "Step 500 -- Train loss: 2.5708796977996826, Train Acc: 0.185302734375 Test Acc: 0.166015625\n",
      "Step 501 -- Train loss: 2.570190906524658, Train Acc: 0.179443359375 Test Acc: 0.1708984375\n",
      "Step 502 -- Train loss: 2.5719425678253174, Train Acc: 0.180908203125 Test Acc: 0.1806640625\n",
      "Step 503 -- Train loss: 2.5716519355773926, Train Acc: 0.17578125 Test Acc: 0.1728515625\n",
      "Step 504 -- Train loss: 2.5682530403137207, Train Acc: 0.182373046875 Test Acc: 0.1806640625\n",
      "Step 505 -- Train loss: 2.568160057067871, Train Acc: 0.174072265625 Test Acc: 0.1806640625\n",
      "Step 506 -- Train loss: 2.56996750831604, Train Acc: 0.1796875 Test Acc: 0.1826171875\n",
      "Step 507 -- Train loss: 2.5648856163024902, Train Acc: 0.1767578125 Test Acc: 0.171875\n",
      "Step 508 -- Train loss: 2.5651981830596924, Train Acc: 0.17529296875 Test Acc: 0.17578125\n",
      "Step 509 -- Train loss: 2.563915252685547, Train Acc: 0.1845703125 Test Acc: 0.1904296875\n",
      "Step 510 -- Train loss: 2.5611679553985596, Train Acc: 0.18115234375 Test Acc: 0.197265625\n",
      "Step 511 -- Train loss: 2.5645499229431152, Train Acc: 0.181884765625 Test Acc: 0.189453125\n",
      "Step 512 -- Train loss: 2.562596559524536, Train Acc: 0.184326171875 Test Acc: 0.181640625\n",
      "Step 513 -- Train loss: 2.561030864715576, Train Acc: 0.18359375 Test Acc: 0.1826171875\n",
      "Step 514 -- Train loss: 2.558684825897217, Train Acc: 0.17724609375 Test Acc: 0.185546875\n",
      "Step 515 -- Train loss: 2.559319496154785, Train Acc: 0.178466796875 Test Acc: 0.177734375\n",
      "Step 516 -- Train loss: 2.5607738494873047, Train Acc: 0.1787109375 Test Acc: 0.1767578125\n",
      "Step 517 -- Train loss: 2.559455156326294, Train Acc: 0.1826171875 Test Acc: 0.1826171875\n",
      "Step 518 -- Train loss: 2.555133581161499, Train Acc: 0.1826171875 Test Acc: 0.185546875\n",
      "Step 519 -- Train loss: 2.5585949420928955, Train Acc: 0.18310546875 Test Acc: 0.1728515625\n",
      "Step 520 -- Train loss: 2.555494785308838, Train Acc: 0.184814453125 Test Acc: 0.1806640625\n",
      "Step 521 -- Train loss: 2.5557968616485596, Train Acc: 0.175537109375 Test Acc: 0.177734375\n",
      "Step 522 -- Train loss: 2.5549190044403076, Train Acc: 0.18212890625 Test Acc: 0.1728515625\n",
      "Step 523 -- Train loss: 2.5576822757720947, Train Acc: 0.1796875 Test Acc: 0.173828125\n",
      "Step 524 -- Train loss: 2.55588960647583, Train Acc: 0.186767578125 Test Acc: 0.1806640625\n",
      "Step 525 -- Train loss: 2.5531647205352783, Train Acc: 0.178955078125 Test Acc: 0.185546875\n",
      "Step 526 -- Train loss: 2.552549123764038, Train Acc: 0.1884765625 Test Acc: 0.1875\n",
      "Step 527 -- Train loss: 2.5519328117370605, Train Acc: 0.184814453125 Test Acc: 0.1923828125\n",
      "Step 528 -- Train loss: 2.553501844406128, Train Acc: 0.18310546875 Test Acc: 0.1787109375\n",
      "Step 529 -- Train loss: 2.5510361194610596, Train Acc: 0.18408203125 Test Acc: 0.193359375\n",
      "Step 530 -- Train loss: 2.55303692817688, Train Acc: 0.18212890625 Test Acc: 0.181640625\n",
      "Step 531 -- Train loss: 2.5513644218444824, Train Acc: 0.186279296875 Test Acc: 0.1884765625\n",
      "Step 532 -- Train loss: 2.5502915382385254, Train Acc: 0.18017578125 Test Acc: 0.189453125\n",
      "Step 533 -- Train loss: 2.5510897636413574, Train Acc: 0.188720703125 Test Acc: 0.18359375\n",
      "Step 534 -- Train loss: 2.548814058303833, Train Acc: 0.18115234375 Test Acc: 0.177734375\n",
      "Step 535 -- Train loss: 2.549968719482422, Train Acc: 0.18701171875 Test Acc: 0.169921875\n",
      "Step 536 -- Train loss: 2.549002170562744, Train Acc: 0.183349609375 Test Acc: 0.1845703125\n",
      "Step 537 -- Train loss: 2.5474672317504883, Train Acc: 0.18115234375 Test Acc: 0.1845703125\n",
      "Step 538 -- Train loss: 2.5480024814605713, Train Acc: 0.182861328125 Test Acc: 0.193359375\n",
      "Step 539 -- Train loss: 2.548525094985962, Train Acc: 0.182861328125 Test Acc: 0.1884765625\n",
      "Step 540 -- Train loss: 2.547623872756958, Train Acc: 0.18505859375 Test Acc: 0.173828125\n",
      "Step 541 -- Train loss: 2.5491693019866943, Train Acc: 0.185546875 Test Acc: 0.1767578125\n",
      "Step 542 -- Train loss: 2.5481510162353516, Train Acc: 0.182861328125 Test Acc: 0.16796875\n",
      "Step 543 -- Train loss: 2.546226978302002, Train Acc: 0.184814453125 Test Acc: 0.177734375\n",
      "Step 544 -- Train loss: 2.545440435409546, Train Acc: 0.181396484375 Test Acc: 0.18359375\n",
      "Step 545 -- Train loss: 2.5448780059814453, Train Acc: 0.187255859375 Test Acc: 0.1953125\n",
      "Step 546 -- Train loss: 2.5477914810180664, Train Acc: 0.185546875 Test Acc: 0.19140625\n",
      "Step 547 -- Train loss: 2.542872190475464, Train Acc: 0.18359375 Test Acc: 0.1875\n",
      "Step 548 -- Train loss: 2.5429837703704834, Train Acc: 0.193359375 Test Acc: 0.181640625\n",
      "Step 549 -- Train loss: 2.5451910495758057, Train Acc: 0.1865234375 Test Acc: 0.18359375\n",
      "Step 550 -- Train loss: 2.5427980422973633, Train Acc: 0.18603515625 Test Acc: 0.18359375\n",
      "Step 551 -- Train loss: 2.5447936058044434, Train Acc: 0.184326171875 Test Acc: 0.177734375\n",
      "Step 552 -- Train loss: 2.543954849243164, Train Acc: 0.183837890625 Test Acc: 0.1796875\n",
      "Step 553 -- Train loss: 2.5434110164642334, Train Acc: 0.1875 Test Acc: 0.201171875\n",
      "Step 554 -- Train loss: 2.54091739654541, Train Acc: 0.192138671875 Test Acc: 0.19921875\n",
      "Step 555 -- Train loss: 2.5417673587799072, Train Acc: 0.19091796875 Test Acc: 0.1943359375\n",
      "Step 556 -- Train loss: 2.539294481277466, Train Acc: 0.18701171875 Test Acc: 0.1943359375\n",
      "Step 557 -- Train loss: 2.5397675037384033, Train Acc: 0.193115234375 Test Acc: 0.189453125\n",
      "Step 558 -- Train loss: 2.538985013961792, Train Acc: 0.197265625 Test Acc: 0.185546875\n",
      "Step 559 -- Train loss: 2.5392348766326904, Train Acc: 0.188232421875 Test Acc: 0.201171875\n",
      "Step 560 -- Train loss: 2.537442922592163, Train Acc: 0.1875 Test Acc: 0.1865234375\n",
      "Step 561 -- Train loss: 2.5358285903930664, Train Acc: 0.193115234375 Test Acc: 0.197265625\n",
      "Step 562 -- Train loss: 2.533834218978882, Train Acc: 0.19921875 Test Acc: 0.203125\n",
      "Step 563 -- Train loss: 2.5337960720062256, Train Acc: 0.200927734375 Test Acc: 0.2001953125\n",
      "Step 564 -- Train loss: 2.5321719646453857, Train Acc: 0.20556640625 Test Acc: 0.203125\n",
      "Step 565 -- Train loss: 2.5316319465637207, Train Acc: 0.204345703125 Test Acc: 0.205078125\n",
      "Step 566 -- Train loss: 2.5273802280426025, Train Acc: 0.2109375 Test Acc: 0.2060546875\n",
      "Step 567 -- Train loss: 2.5243375301361084, Train Acc: 0.213134765625 Test Acc: 0.2177734375\n",
      "Step 568 -- Train loss: 2.5154247283935547, Train Acc: 0.224365234375 Test Acc: 0.22265625\n",
      "Step 569 -- Train loss: 2.508755683898926, Train Acc: 0.2294921875 Test Acc: 0.2216796875\n",
      "Step 570 -- Train loss: 2.5053467750549316, Train Acc: 0.228271484375 Test Acc: 0.2314453125\n",
      "Step 571 -- Train loss: 2.492753505706787, Train Acc: 0.22998046875 Test Acc: 0.224609375\n",
      "Step 572 -- Train loss: 2.4810242652893066, Train Acc: 0.255615234375 Test Acc: 0.263671875\n",
      "Step 573 -- Train loss: 2.4669179916381836, Train Acc: 0.271484375 Test Acc: 0.2744140625\n",
      "Step 574 -- Train loss: 2.4529335498809814, Train Acc: 0.281982421875 Test Acc: 0.2568359375\n",
      "Step 575 -- Train loss: 2.4366953372955322, Train Acc: 0.303955078125 Test Acc: 0.2861328125\n",
      "Step 576 -- Train loss: 2.417947769165039, Train Acc: 0.3037109375 Test Acc: 0.294921875\n",
      "Step 577 -- Train loss: 2.3911359310150146, Train Acc: 0.336181640625 Test Acc: 0.3427734375\n",
      "Step 578 -- Train loss: 2.3816959857940674, Train Acc: 0.340087890625 Test Acc: 0.3447265625\n",
      "Step 579 -- Train loss: 2.350642442703247, Train Acc: 0.370361328125 Test Acc: 0.35546875\n",
      "Step 580 -- Train loss: 2.3261759281158447, Train Acc: 0.385009765625 Test Acc: 0.3603515625\n",
      "Step 581 -- Train loss: 2.30084228515625, Train Acc: 0.4150390625 Test Acc: 0.427734375\n",
      "Step 582 -- Train loss: 2.268918037414551, Train Acc: 0.4384765625 Test Acc: 0.4521484375\n",
      "Step 583 -- Train loss: 2.234518051147461, Train Acc: 0.47705078125 Test Acc: 0.4775390625\n",
      "Step 584 -- Train loss: 2.193655490875244, Train Acc: 0.511474609375 Test Acc: 0.529296875\n",
      "Step 585 -- Train loss: 2.1470067501068115, Train Acc: 0.542724609375 Test Acc: 0.5595703125\n",
      "Step 586 -- Train loss: 2.1022934913635254, Train Acc: 0.57763671875 Test Acc: 0.556640625\n",
      "Step 587 -- Train loss: 2.0521750450134277, Train Acc: 0.604248046875 Test Acc: 0.625\n",
      "Step 588 -- Train loss: 1.9967461824417114, Train Acc: 0.6474609375 Test Acc: 0.642578125\n",
      "Step 589 -- Train loss: 1.9491369724273682, Train Acc: 0.691162109375 Test Acc: 0.68359375\n",
      "Step 590 -- Train loss: 1.8990429639816284, Train Acc: 0.719482421875 Test Acc: 0.7265625\n",
      "Step 591 -- Train loss: 1.8558465242385864, Train Acc: 0.760009765625 Test Acc: 0.7509765625\n",
      "Step 592 -- Train loss: 1.7989940643310547, Train Acc: 0.786865234375 Test Acc: 0.787109375\n",
      "Step 593 -- Train loss: 1.7526837587356567, Train Acc: 0.802978515625 Test Acc: 0.8037109375\n",
      "Step 594 -- Train loss: 1.7171902656555176, Train Acc: 0.824462890625 Test Acc: 0.8193359375\n",
      "Step 595 -- Train loss: 1.6743558645248413, Train Acc: 0.8349609375 Test Acc: 0.841796875\n",
      "Step 596 -- Train loss: 1.6558715105056763, Train Acc: 0.8408203125 Test Acc: 0.837890625\n",
      "Step 597 -- Train loss: 1.6301029920578003, Train Acc: 0.84912109375 Test Acc: 0.8447265625\n",
      "Step 598 -- Train loss: 1.6148427724838257, Train Acc: 0.84912109375 Test Acc: 0.8447265625\n",
      "Step 599 -- Train loss: 1.592247724533081, Train Acc: 0.8603515625 Test Acc: 0.8603515625\n",
      "Step 600 -- Train loss: 1.5656993389129639, Train Acc: 0.88427734375 Test Acc: 0.876953125\n",
      "Step 601 -- Train loss: 1.5445106029510498, Train Acc: 0.89697265625 Test Acc: 0.8916015625\n",
      "Step 602 -- Train loss: 1.5109268426895142, Train Acc: 0.913330078125 Test Acc: 0.916015625\n",
      "Step 603 -- Train loss: 1.4790304899215698, Train Acc: 0.938232421875 Test Acc: 0.939453125\n",
      "Step 604 -- Train loss: 1.4563555717468262, Train Acc: 0.9482421875 Test Acc: 0.9453125\n",
      "Step 605 -- Train loss: 1.4416698217391968, Train Acc: 0.95458984375 Test Acc: 0.9443359375\n",
      "Step 606 -- Train loss: 1.4282528162002563, Train Acc: 0.95947265625 Test Acc: 0.9609375\n",
      "Step 607 -- Train loss: 1.4146466255187988, Train Acc: 0.96337890625 Test Acc: 0.96484375\n",
      "Step 608 -- Train loss: 1.3991725444793701, Train Acc: 0.972412109375 Test Acc: 0.9736328125\n",
      "Step 609 -- Train loss: 1.3829889297485352, Train Acc: 0.9853515625 Test Acc: 0.986328125\n",
      "Step 610 -- Train loss: 1.3681503534317017, Train Acc: 0.9970703125 Test Acc: 0.994140625\n",
      "Step 611 -- Train loss: 1.3587092161178589, Train Acc: 0.998046875 Test Acc: 0.99609375\n",
      "Step 612 -- Train loss: 1.3502315282821655, Train Acc: 0.998779296875 Test Acc: 0.998046875\n",
      "Step 613 -- Train loss: 1.3439245223999023, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 614 -- Train loss: 1.3405089378356934, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 615 -- Train loss: 1.33658766746521, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 616 -- Train loss: 1.3371953964233398, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 617 -- Train loss: 1.3354568481445312, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 618 -- Train loss: 1.3334060907363892, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 619 -- Train loss: 1.3318759202957153, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 620 -- Train loss: 1.3301464319229126, Train Acc: 0.999755859375 Test Acc: 1.0\n",
      "Step 621 -- Train loss: 1.329176425933838, Train Acc: 0.999755859375 Test Acc: 1.0\n",
      "Step 622 -- Train loss: 1.3262380361557007, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 623 -- Train loss: 1.3256205320358276, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 624 -- Train loss: 1.3246915340423584, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 625 -- Train loss: 1.325425148010254, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 626 -- Train loss: 1.3251373767852783, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 627 -- Train loss: 1.3207021951675415, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 628 -- Train loss: 1.3220477104187012, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 629 -- Train loss: 1.320156216621399, Train Acc: 0.999755859375 Test Acc: 0.9990234375\n",
      "Step 630 -- Train loss: 1.3206615447998047, Train Acc: 0.999755859375 Test Acc: 1.0\n",
      "Step 631 -- Train loss: 1.3187769651412964, Train Acc: 1.0 Test Acc: 0.9990234375\n",
      "Step 632 -- Train loss: 1.3204154968261719, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 633 -- Train loss: 1.318685531616211, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 634 -- Train loss: 1.319279432296753, Train Acc: 0.999267578125 Test Acc: 1.0\n",
      "Step 635 -- Train loss: 1.3173441886901855, Train Acc: 0.999755859375 Test Acc: 0.9990234375\n",
      "Step 636 -- Train loss: 1.3163126707077026, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 637 -- Train loss: 1.316325068473816, Train Acc: 1.0 Test Acc: 0.9990234375\n",
      "Step 638 -- Train loss: 1.316331386566162, Train Acc: 1.0 Test Acc: 0.9990234375\n",
      "Step 639 -- Train loss: 1.3141999244689941, Train Acc: 1.0 Test Acc: 0.9990234375\n",
      "Step 640 -- Train loss: 1.3160624504089355, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 641 -- Train loss: 1.315048336982727, Train Acc: 0.999755859375 Test Acc: 1.0\n",
      "Step 642 -- Train loss: 1.3149596452713013, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 643 -- Train loss: 1.313550591468811, Train Acc: 0.999755859375 Test Acc: 1.0\n",
      "Step 644 -- Train loss: 1.3151471614837646, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 645 -- Train loss: 1.3129223585128784, Train Acc: 0.999755859375 Test Acc: 1.0\n",
      "Step 646 -- Train loss: 1.3135730028152466, Train Acc: 0.99951171875 Test Acc: 1.0\n",
      "Step 647 -- Train loss: 1.3131812810897827, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 648 -- Train loss: 1.3128389120101929, Train Acc: 0.999755859375 Test Acc: 1.0\n",
      "Step 649 -- Train loss: 1.3133894205093384, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 650 -- Train loss: 1.3119717836380005, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 651 -- Train loss: 1.3119477033615112, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 652 -- Train loss: 1.3119443655014038, Train Acc: 1.0 Test Acc: 0.9990234375\n",
      "Step 653 -- Train loss: 1.3124855756759644, Train Acc: 0.99951171875 Test Acc: 1.0\n",
      "Step 654 -- Train loss: 1.311954140663147, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 655 -- Train loss: 1.311627745628357, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 656 -- Train loss: 1.3120957612991333, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 657 -- Train loss: 1.3113071918487549, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 658 -- Train loss: 1.3120039701461792, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 659 -- Train loss: 1.313828468322754, Train Acc: 0.99951171875 Test Acc: 1.0\n",
      "Step 660 -- Train loss: 1.3124713897705078, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 661 -- Train loss: 1.3114029169082642, Train Acc: 1.0 Test Acc: 0.9990234375\n",
      "Step 662 -- Train loss: 1.312423586845398, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 663 -- Train loss: 1.313781976699829, Train Acc: 0.999755859375 Test Acc: 1.0\n",
      "Step 664 -- Train loss: 1.3100672960281372, Train Acc: 0.999755859375 Test Acc: 1.0\n",
      "Step 665 -- Train loss: 1.3132508993148804, Train Acc: 0.999755859375 Test Acc: 1.0\n",
      "Step 666 -- Train loss: 1.3133069276809692, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 667 -- Train loss: 1.3098065853118896, Train Acc: 1.0 Test Acc: 0.9990234375\n",
      "Step 668 -- Train loss: 1.311923861503601, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 669 -- Train loss: 1.3111371994018555, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 670 -- Train loss: 1.3137706518173218, Train Acc: 0.9990234375 Test Acc: 1.0\n",
      "Step 671 -- Train loss: 1.3133418560028076, Train Acc: 0.99853515625 Test Acc: 1.0\n",
      "Step 672 -- Train loss: 1.3116495609283447, Train Acc: 0.999755859375 Test Acc: 1.0\n",
      "Step 673 -- Train loss: 1.3108144998550415, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 674 -- Train loss: 1.3134411573410034, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 675 -- Train loss: 1.3094857931137085, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 676 -- Train loss: 1.3096892833709717, Train Acc: 0.99951171875 Test Acc: 1.0\n",
      "Step 677 -- Train loss: 1.3094743490219116, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 678 -- Train loss: 1.3094863891601562, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 679 -- Train loss: 1.3101633787155151, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 680 -- Train loss: 1.3108857870101929, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 681 -- Train loss: 1.3090825080871582, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 682 -- Train loss: 1.308525800704956, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 683 -- Train loss: 1.3093578815460205, Train Acc: 0.999755859375 Test Acc: 1.0\n",
      "Step 684 -- Train loss: 1.309496521949768, Train Acc: 0.999755859375 Test Acc: 1.0\n",
      "Step 685 -- Train loss: 1.3088403940200806, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 686 -- Train loss: 1.3097736835479736, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 687 -- Train loss: 1.3084146976470947, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 688 -- Train loss: 1.3087273836135864, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 689 -- Train loss: 1.3080159425735474, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 690 -- Train loss: 1.3087716102600098, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 691 -- Train loss: 1.308542251586914, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 692 -- Train loss: 1.3084968328475952, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 693 -- Train loss: 1.307803750038147, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 694 -- Train loss: 1.308303713798523, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 695 -- Train loss: 1.3081886768341064, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 696 -- Train loss: 1.3087035417556763, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 697 -- Train loss: 1.3082419633865356, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 698 -- Train loss: 1.3089655637741089, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 699 -- Train loss: 1.3096089363098145, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 700 -- Train loss: 1.3086899518966675, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 701 -- Train loss: 1.3075038194656372, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 702 -- Train loss: 1.307492733001709, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 703 -- Train loss: 1.307974934577942, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 704 -- Train loss: 1.3073264360427856, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 705 -- Train loss: 1.3078275918960571, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 706 -- Train loss: 1.3074071407318115, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 707 -- Train loss: 1.307163119316101, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 708 -- Train loss: 1.30709969997406, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 709 -- Train loss: 1.3067798614501953, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 710 -- Train loss: 1.3068625926971436, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 711 -- Train loss: 1.3064299821853638, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 712 -- Train loss: 1.3062752485275269, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 713 -- Train loss: 1.3074406385421753, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 714 -- Train loss: 1.3071446418762207, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 715 -- Train loss: 1.3057096004486084, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 716 -- Train loss: 1.3069605827331543, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 717 -- Train loss: 1.3065952062606812, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 718 -- Train loss: 1.3078029155731201, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 719 -- Train loss: 1.3075844049453735, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 720 -- Train loss: 1.3061028718948364, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 721 -- Train loss: 1.306296467781067, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 722 -- Train loss: 1.3067991733551025, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 723 -- Train loss: 1.3069957494735718, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 724 -- Train loss: 1.3063708543777466, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 725 -- Train loss: 1.3052122592926025, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 726 -- Train loss: 1.3069496154785156, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 727 -- Train loss: 1.3072104454040527, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 728 -- Train loss: 1.305721402168274, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 729 -- Train loss: 1.3058865070343018, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 730 -- Train loss: 1.30582857131958, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 731 -- Train loss: 1.3059309720993042, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 732 -- Train loss: 1.3064301013946533, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 733 -- Train loss: 1.306335210800171, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 734 -- Train loss: 1.3069592714309692, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 735 -- Train loss: 1.305446982383728, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 736 -- Train loss: 1.3061460256576538, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 737 -- Train loss: 1.3054554462432861, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 738 -- Train loss: 1.3057124614715576, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 739 -- Train loss: 1.3050683736801147, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 740 -- Train loss: 1.3057668209075928, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 741 -- Train loss: 1.3057479858398438, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 742 -- Train loss: 1.305810809135437, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 743 -- Train loss: 1.305680751800537, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 744 -- Train loss: 1.3060983419418335, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 745 -- Train loss: 1.306589126586914, Train Acc: 1.0 Test Acc: 0.9990234375\n",
      "Step 746 -- Train loss: 1.3060519695281982, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 747 -- Train loss: 1.3062855005264282, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 748 -- Train loss: 1.3053882122039795, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 749 -- Train loss: 1.3052659034729004, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 750 -- Train loss: 1.305871605873108, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 751 -- Train loss: 1.305978536605835, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 752 -- Train loss: 1.3054393529891968, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 753 -- Train loss: 1.3055082559585571, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 754 -- Train loss: 1.3058305978775024, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 755 -- Train loss: 1.30543053150177, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 756 -- Train loss: 1.305215835571289, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 757 -- Train loss: 1.3046410083770752, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 758 -- Train loss: 1.3059642314910889, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 759 -- Train loss: 1.3038640022277832, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 760 -- Train loss: 1.3053693771362305, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 761 -- Train loss: 1.3055418729782104, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 762 -- Train loss: 1.3060007095336914, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 763 -- Train loss: 1.3050918579101562, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 764 -- Train loss: 1.3052663803100586, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 765 -- Train loss: 1.305189609527588, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 766 -- Train loss: 1.3053700923919678, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 767 -- Train loss: 1.3049006462097168, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 768 -- Train loss: 1.3044301271438599, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 769 -- Train loss: 1.305403709411621, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 770 -- Train loss: 1.306474208831787, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 771 -- Train loss: 1.304828405380249, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 772 -- Train loss: 1.304500937461853, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 773 -- Train loss: 1.3045485019683838, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 774 -- Train loss: 1.3051519393920898, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 775 -- Train loss: 1.3048405647277832, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 776 -- Train loss: 1.3053261041641235, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 777 -- Train loss: 1.305395245552063, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 778 -- Train loss: 1.3040099143981934, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 779 -- Train loss: 1.305733323097229, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 780 -- Train loss: 1.3049650192260742, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 781 -- Train loss: 1.3042532205581665, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 782 -- Train loss: 1.3048429489135742, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 783 -- Train loss: 1.3046809434890747, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 784 -- Train loss: 1.3048323392868042, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 785 -- Train loss: 1.3045459985733032, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 786 -- Train loss: 1.3041635751724243, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 787 -- Train loss: 1.304895281791687, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 788 -- Train loss: 1.3049203157424927, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 789 -- Train loss: 1.305146336555481, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 790 -- Train loss: 1.3044346570968628, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 791 -- Train loss: 1.3040940761566162, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 792 -- Train loss: 1.304848313331604, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 793 -- Train loss: 1.3037829399108887, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 794 -- Train loss: 1.3040974140167236, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 795 -- Train loss: 1.3048616647720337, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 796 -- Train loss: 1.3053152561187744, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 797 -- Train loss: 1.3040961027145386, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 798 -- Train loss: 1.304540991783142, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 799 -- Train loss: 1.304347038269043, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 800 -- Train loss: 1.3045004606246948, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 801 -- Train loss: 1.3043614625930786, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 802 -- Train loss: 1.3040978908538818, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 803 -- Train loss: 1.3049646615982056, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 804 -- Train loss: 1.304028868675232, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 805 -- Train loss: 1.3041502237319946, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 806 -- Train loss: 1.304042100906372, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 807 -- Train loss: 1.3047653436660767, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 808 -- Train loss: 1.3051114082336426, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 809 -- Train loss: 1.3045892715454102, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 810 -- Train loss: 1.3038933277130127, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 811 -- Train loss: 1.3042471408843994, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 812 -- Train loss: 1.3039891719818115, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 813 -- Train loss: 1.303327202796936, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 814 -- Train loss: 1.3035207986831665, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 815 -- Train loss: 1.304262638092041, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 816 -- Train loss: 1.304791808128357, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 817 -- Train loss: 1.3046213388442993, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 818 -- Train loss: 1.30476975440979, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 819 -- Train loss: 1.304502248764038, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 820 -- Train loss: 1.3023308515548706, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 821 -- Train loss: 1.3048977851867676, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 822 -- Train loss: 1.3046150207519531, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 823 -- Train loss: 1.3049119710922241, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 824 -- Train loss: 1.3045921325683594, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 825 -- Train loss: 1.3046177625656128, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 826 -- Train loss: 1.304048776626587, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 827 -- Train loss: 1.3034546375274658, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 828 -- Train loss: 1.304302453994751, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 829 -- Train loss: 1.3039149045944214, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 830 -- Train loss: 1.3041081428527832, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 831 -- Train loss: 1.3042597770690918, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 832 -- Train loss: 1.3036413192749023, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 833 -- Train loss: 1.304248332977295, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 834 -- Train loss: 1.3037811517715454, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 835 -- Train loss: 1.3038511276245117, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 836 -- Train loss: 1.3036178350448608, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 837 -- Train loss: 1.3038973808288574, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 838 -- Train loss: 1.3045778274536133, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 839 -- Train loss: 1.3032997846603394, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 840 -- Train loss: 1.3044414520263672, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 841 -- Train loss: 1.3038544654846191, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 842 -- Train loss: 1.3039523363113403, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 843 -- Train loss: 1.3031326532363892, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 844 -- Train loss: 1.3032944202423096, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 845 -- Train loss: 1.3035088777542114, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 846 -- Train loss: 1.3037197589874268, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 847 -- Train loss: 1.3042888641357422, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 848 -- Train loss: 1.3035142421722412, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 849 -- Train loss: 1.3037388324737549, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 850 -- Train loss: 1.3029842376708984, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 851 -- Train loss: 1.3033108711242676, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 852 -- Train loss: 1.3029929399490356, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 853 -- Train loss: 1.3036417961120605, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 854 -- Train loss: 1.3035045862197876, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 855 -- Train loss: 1.3029446601867676, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 856 -- Train loss: 1.302240252494812, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 857 -- Train loss: 1.303356647491455, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 858 -- Train loss: 1.30335533618927, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 859 -- Train loss: 1.3037041425704956, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 860 -- Train loss: 1.303348422050476, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 861 -- Train loss: 1.303969144821167, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 862 -- Train loss: 1.3032761812210083, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 863 -- Train loss: 1.3036880493164062, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 864 -- Train loss: 1.3033370971679688, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 865 -- Train loss: 1.303576111793518, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 866 -- Train loss: 1.3034968376159668, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 867 -- Train loss: 1.3035824298858643, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 868 -- Train loss: 1.3035123348236084, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 869 -- Train loss: 1.3027966022491455, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 870 -- Train loss: 1.303350567817688, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 871 -- Train loss: 1.3037915229797363, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 872 -- Train loss: 1.3030565977096558, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 873 -- Train loss: 1.3032703399658203, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 874 -- Train loss: 1.3031679391860962, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 875 -- Train loss: 1.3037266731262207, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 876 -- Train loss: 1.3031941652297974, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 877 -- Train loss: 1.303654670715332, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 878 -- Train loss: 1.3036069869995117, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 879 -- Train loss: 1.3035210371017456, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 880 -- Train loss: 1.3028275966644287, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 881 -- Train loss: 1.303218960762024, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 882 -- Train loss: 1.3040614128112793, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 883 -- Train loss: 1.3030390739440918, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 884 -- Train loss: 1.3035345077514648, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 885 -- Train loss: 1.3037333488464355, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 886 -- Train loss: 1.3028481006622314, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 887 -- Train loss: 1.3042502403259277, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 888 -- Train loss: 1.303267240524292, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 889 -- Train loss: 1.3035441637039185, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 890 -- Train loss: 1.3043829202651978, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 891 -- Train loss: 1.3035298585891724, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 892 -- Train loss: 1.3029193878173828, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 893 -- Train loss: 1.3035551309585571, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 894 -- Train loss: 1.303086519241333, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 895 -- Train loss: 1.302925944328308, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 896 -- Train loss: 1.3031624555587769, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 897 -- Train loss: 1.3030494451522827, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 898 -- Train loss: 1.3035838603973389, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 899 -- Train loss: 1.3035916090011597, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 900 -- Train loss: 1.302647590637207, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 901 -- Train loss: 1.3036588430404663, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 902 -- Train loss: 1.3031476736068726, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 903 -- Train loss: 1.3040261268615723, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 904 -- Train loss: 1.3031983375549316, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 905 -- Train loss: 1.3030465841293335, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 906 -- Train loss: 1.303235650062561, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 907 -- Train loss: 1.3032158613204956, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 908 -- Train loss: 1.3031920194625854, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 909 -- Train loss: 1.3035495281219482, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 910 -- Train loss: 1.3038229942321777, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 911 -- Train loss: 1.3026663064956665, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 912 -- Train loss: 1.3027344942092896, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 913 -- Train loss: 1.3030625581741333, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 914 -- Train loss: 1.3032176494598389, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 915 -- Train loss: 1.303001880645752, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 916 -- Train loss: 1.3019979000091553, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 917 -- Train loss: 1.3033663034439087, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 918 -- Train loss: 1.3025263547897339, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 919 -- Train loss: 1.3031893968582153, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 920 -- Train loss: 1.3036595582962036, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 921 -- Train loss: 1.302927017211914, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 922 -- Train loss: 1.3025057315826416, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 923 -- Train loss: 1.3031160831451416, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 924 -- Train loss: 1.3034096956253052, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 925 -- Train loss: 1.3027094602584839, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 926 -- Train loss: 1.3032934665679932, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 927 -- Train loss: 1.3026349544525146, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 928 -- Train loss: 1.3029531240463257, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 929 -- Train loss: 1.3034693002700806, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 930 -- Train loss: 1.303670048713684, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 931 -- Train loss: 1.3027448654174805, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 932 -- Train loss: 1.3029577732086182, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 933 -- Train loss: 1.3031678199768066, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 934 -- Train loss: 1.3029003143310547, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 935 -- Train loss: 1.3021714687347412, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 936 -- Train loss: 1.3029325008392334, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 937 -- Train loss: 1.3035532236099243, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 938 -- Train loss: 1.3026859760284424, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 939 -- Train loss: 1.3030730485916138, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 940 -- Train loss: 1.302826166152954, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 941 -- Train loss: 1.3023487329483032, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 942 -- Train loss: 1.3024197816848755, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 943 -- Train loss: 1.302366852760315, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 944 -- Train loss: 1.3025150299072266, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 945 -- Train loss: 1.302818775177002, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 946 -- Train loss: 1.3028850555419922, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 947 -- Train loss: 1.3034249544143677, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 948 -- Train loss: 1.3023799657821655, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 949 -- Train loss: 1.3034695386886597, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 950 -- Train loss: 1.3019822835922241, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 951 -- Train loss: 1.3024334907531738, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 952 -- Train loss: 1.302542805671692, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 953 -- Train loss: 1.3030275106430054, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 954 -- Train loss: 1.3025356531143188, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 955 -- Train loss: 1.302695631980896, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 956 -- Train loss: 1.3025919198989868, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 957 -- Train loss: 1.3024786710739136, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 958 -- Train loss: 1.3026018142700195, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 959 -- Train loss: 1.3030625581741333, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 960 -- Train loss: 1.3030328750610352, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 961 -- Train loss: 1.3025169372558594, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 962 -- Train loss: 1.3026912212371826, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 963 -- Train loss: 1.3030544519424438, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 964 -- Train loss: 1.3024998903274536, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 965 -- Train loss: 1.3024660348892212, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 966 -- Train loss: 1.302300214767456, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 967 -- Train loss: 1.303134560585022, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 968 -- Train loss: 1.302012324333191, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 969 -- Train loss: 1.3027129173278809, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 970 -- Train loss: 1.3026670217514038, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 971 -- Train loss: 1.3025014400482178, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 972 -- Train loss: 1.3022223711013794, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 973 -- Train loss: 1.3022265434265137, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 974 -- Train loss: 1.3021063804626465, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 975 -- Train loss: 1.3023734092712402, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 976 -- Train loss: 1.3032807111740112, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 977 -- Train loss: 1.3025835752487183, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 978 -- Train loss: 1.3026163578033447, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 979 -- Train loss: 1.3024520874023438, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 980 -- Train loss: 1.3023096323013306, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 981 -- Train loss: 1.3017299175262451, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 982 -- Train loss: 1.3012620210647583, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 983 -- Train loss: 1.302517294883728, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 984 -- Train loss: 1.303037405014038, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 985 -- Train loss: 1.301936388015747, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 986 -- Train loss: 1.3032150268554688, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 987 -- Train loss: 1.3026323318481445, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 988 -- Train loss: 1.3019731044769287, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 989 -- Train loss: 1.3020873069763184, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 990 -- Train loss: 1.3021459579467773, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 991 -- Train loss: 1.30194091796875, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 992 -- Train loss: 1.302886724472046, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 993 -- Train loss: 1.3021845817565918, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 994 -- Train loss: 1.3030961751937866, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 995 -- Train loss: 1.3026312589645386, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 996 -- Train loss: 1.302067518234253, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 997 -- Train loss: 1.3025678396224976, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 998 -- Train loss: 1.3024958372116089, Train Acc: 1.0 Test Acc: 1.0\n",
      "Step 999 -- Train loss: 1.3019914627075195, Train Acc: 1.0 Test Acc: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>att_prog_measure</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂▂▂▂▂▅████████████████</td></tr><tr><td>data_repeat_frac</td><td>▄▅▅▅▃▆▁▆▃▄▅▄▃▅▄▄▅▃▃▆▅▄▅▅▅▄▃▃▄▅▄▅▆▆▂▃█▆▂▇</td></tr><tr><td>idx0_check</td><td>▁▁▁▁▁▁▂▄████████████████████████████████</td></tr><tr><td>idx10_check</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂████████████████</td></tr><tr><td>idx11_check</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█████████████████</td></tr><tr><td>idx12_check</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▄████████████████</td></tr><tr><td>idx13_check</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▆████████████████</td></tr><tr><td>idx14_check</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▂████████████████</td></tr><tr><td>idx15_check</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂████████████████</td></tr><tr><td>idx1_check</td><td>▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂▂▂▃▄▇████████████████████</td></tr><tr><td>idx2_check</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▇████████████████</td></tr><tr><td>idx3_check</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█████████████████</td></tr><tr><td>idx4_check</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█████████████████</td></tr><tr><td>idx5_check</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█████████████████</td></tr><tr><td>idx6_check</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█████████████████</td></tr><tr><td>idx7_check</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█████████████████</td></tr><tr><td>idx8_check</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁█████████████████</td></tr><tr><td>idx9_check</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▇████████████████</td></tr><tr><td>mean_cosine_sim</td><td>▄▇▇█████████████▇▇▇▇▇▇▇▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>mean_cosine_sim_0</td><td>▅███▇▇▆▆▃▁▂▂▃▃▂▂▂▂▂▃▂▂▂▂▂▂▂▁▁▁▁▁▁▂▂▁▁▁▂▁</td></tr><tr><td>mean_cosine_sim_1</td><td>▄▇▇█████████▆▅▄▄▃▂▁▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>mean_cosine_sim_10</td><td>▄▇▇▇███████████████████▄▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>mean_cosine_sim_11</td><td>▄▇▇▇███████████████████▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>mean_cosine_sim_12</td><td>▄▇▇▇███████████████████▄▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>mean_cosine_sim_13</td><td>▄▇▇▇███████████████████▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>mean_cosine_sim_14</td><td>▄▇▇████████████████████▇▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>mean_cosine_sim_2</td><td>▄▇▇▇▇██████████████████▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>mean_cosine_sim_3</td><td>▄▇▇▇▇██████████████████▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>mean_cosine_sim_4</td><td>▄▇▇▇███████████████████▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>mean_cosine_sim_5</td><td>▄▇▇▇███████████████████▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>mean_cosine_sim_6</td><td>▄▇▇▇███████████████████▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>mean_cosine_sim_7</td><td>▄▇▇▇███████████████████▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>mean_cosine_sim_8</td><td>▄▇▇████████████████████▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>mean_cosine_sim_9</td><td>▄▇▇▇███████████████████▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>model_repeat_frac</td><td>▃▂▇▃▂▂▃▇▆▆▁▇▂▃▁▁█▂▄▇▂▃▆▂▂▂▁▁▂▂▂▂▂▂▁▁▂▂▁▂</td></tr><tr><td>test_acc</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▂▂▂▂▂▂▂▆████████████████</td></tr><tr><td>train_acc</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂▂▂▂▂▂▂▆████████████████</td></tr><tr><td>train_loss</td><td>████████▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▄▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>att_prog_measure</td><td>0.86604</td></tr><tr><td>data_repeat_frac</td><td>0.05937</td></tr><tr><td>idx0_check</td><td>1.0</td></tr><tr><td>idx10_check</td><td>1.0</td></tr><tr><td>idx11_check</td><td>1.0</td></tr><tr><td>idx12_check</td><td>1.0</td></tr><tr><td>idx13_check</td><td>1.0</td></tr><tr><td>idx14_check</td><td>1.0</td></tr><tr><td>idx15_check</td><td>1.0</td></tr><tr><td>idx1_check</td><td>1.0</td></tr><tr><td>idx2_check</td><td>1.0</td></tr><tr><td>idx3_check</td><td>1.0</td></tr><tr><td>idx4_check</td><td>1.0</td></tr><tr><td>idx5_check</td><td>1.0</td></tr><tr><td>idx6_check</td><td>1.0</td></tr><tr><td>idx7_check</td><td>1.0</td></tr><tr><td>idx8_check</td><td>1.0</td></tr><tr><td>idx9_check</td><td>1.0</td></tr><tr><td>mean_cosine_sim</td><td>0.00647</td></tr><tr><td>mean_cosine_sim_0</td><td>-0.00024</td></tr><tr><td>mean_cosine_sim_1</td><td>0.00978</td></tr><tr><td>mean_cosine_sim_10</td><td>0.00697</td></tr><tr><td>mean_cosine_sim_11</td><td>0.02301</td></tr><tr><td>mean_cosine_sim_12</td><td>0.02431</td></tr><tr><td>mean_cosine_sim_13</td><td>0.02298</td></tr><tr><td>mean_cosine_sim_14</td><td>0.02012</td></tr><tr><td>mean_cosine_sim_2</td><td>0.00228</td></tr><tr><td>mean_cosine_sim_3</td><td>0.00121</td></tr><tr><td>mean_cosine_sim_4</td><td>0.00722</td></tr><tr><td>mean_cosine_sim_5</td><td>0.00714</td></tr><tr><td>mean_cosine_sim_6</td><td>-0.00198</td></tr><tr><td>mean_cosine_sim_7</td><td>0.00337</td></tr><tr><td>mean_cosine_sim_8</td><td>0.0054</td></tr><tr><td>mean_cosine_sim_9</td><td>0.00582</td></tr><tr><td>model_repeat_frac</td><td>0.05937</td></tr><tr><td>test_acc</td><td>1.0</td></tr><tr><td>train_acc</td><td>1.0</td></tr><tr><td>train_loss</td><td>1.30199</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">mws_softmax_attention_1000_steps</strong> at: <a href='https://wandb.ai/wth_ucsd/loss_plateau_tf/runs/11gdga8l' target=\"_blank\">https://wandb.ai/wth_ucsd/loss_plateau_tf/runs/11gdga8l</a><br/>Synced 6 W&B file(s), 1000 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251010_003520-11gdga8l/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "config = DotMap(config)\n",
    "\n",
    "config.model.vocab_size = max(config.data.p, config.data.max_num) + 1\n",
    "config.model.block_size = 2 * config.data.num_tokens + 1\n",
    "\n",
    "data_sampler = MovingWindowSum(\n",
    "    min_num=config.data.min_num,\n",
    "    max_num=config.data.max_num,\n",
    "    k=config.data.k,\n",
    "    p=config.data.p,\n",
    ")\n",
    "\n",
    "model = GPTSoftmax(config.model, return_att=True).to(device)\n",
    "optim = Adam(model.parameters(), lr=config.train.lr)\n",
    "\n",
    "if config.train.wandb:\n",
    "    wandb_run_name = 'mws_softmax_attention_1000_steps'\n",
    "    wandb.login(key=\"\")\n",
    "    wandb.init(project=\"loss_plateau_tf\", name=wandb_run_name, config=config)\n",
    "    wandb.watch(model)\n",
    "\n",
    "for step in range(config.train.num_steps):\n",
    "    train_step(\n",
    "        model=model,\n",
    "        optim=optim,\n",
    "        data_sampler=data_sampler,\n",
    "        step=step,\n",
    "        config=config,\n",
    "    )\n",
    "\n",
    "if config.train.wandb:\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba5e0f9-ca5f-449a-b912-ab65685ae6e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (emerge)",
   "language": "python",
   "name": "emerge"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
